[
    {
        "paper": "Tri-Level Navigator LLM-Empowered Tri-Level Learning for Time Series OOD Generalization.pdf",
        "target": "TTSO is a general framework for learning invariant representations across diverse domain distri-\nbutions, currently discussed only for time series classification. This framework could be further\nenhanced by extending it to more time series OOD tasks, such as time series forecasting and anomaly\ndetection. Additionally, distribution shifts occur not only in time series but also in other machine\nlearning domains like images [Deecke et al., 2021] and text [Tan et al., 2022]. Applying our approach\nto these domains could further improve performance.",
        "target_bullets": "- The framework of TTSO focuses on learning invariant representations but is currently limited to time series classification.\n- The potential of TTSO could be expanded by including other time series out-of-distribution tasks such as time series forecasting and anomaly detection.\n- The concept of distribution shifts is not exclusive to time series data and also occurs in other machine learning domains such as images and text.\n- Extending the application of TTSO to these other domains could likely lead to performance improvements."
    },
    {
        "paper": "FinBen A Holistic Financial Benchmark for Large Language Models.pdf",
        "target": "Despite the novel efforts to benchmark LLMs in the financial domain through FinBen, we acknowl-\nedge several inherent limitations that could impact the benchmark\u2019s effectiveness and applicability:\nDataset Size Limitations: The restricted size of available datasets, a common issue in open-source\nfinancial data, may affect the models\u2019 financial understanding and generalization across various\ncontexts. Model Size Limitations: Due to computational constraints, our evaluation was limited to\nthe LLaMA 70B model, potentially overlooking the capabilities of larger or differently architected\nmodels. Generalizability: The tasks, particularly trading and forecasting, are based on American\nmarket data and English texts, possibly limiting the benchmark\u2019s applicability to global financial mar-\nkets. Potential Negative Impacts: While FinBen aims to advance financial language understanding,\nit is crucial to consider potential misuse, such as propagating financial misinformation or exerting\nunethical influence on markets. Responsible usage and further safeguards are essential13.",
        "target_bullets": "- Dataset Size Limitations: The restricted size of available datasets in open-source financial data may impact the model's generalization across various contexts.\n- Model Size Limitations: Due to computational constraints, the evaluation was limited to a specific model, potentially overlooking the capabilities of larger or differently architected models.\n- Generalizability Concerns: Tasks are based on American market data and English texts, potentially limiting the benchmark's applicability to global financial markets.\n- Potential Negative Impacts: There is a risk of FinBen being misused to propagate financial misinformation or exert unethical influence on markets, emphasizing the need for responsible usage and safeguards."
    },
    {
        "paper": "Magnet We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function.pdf",
        "target": "Although Magnet provides an efficient and effective way to address the attribute binding problem, we\nacknowledge our technique is subject to a few limitations.\nFig. 21 displays the failure cases of Magnet. First, the neglect of the object (columns 1-2), may\nbe attributed to the model\u2019s limited ability to foreground limited subjects. Second, the excessive\nmanipulation of the object embedding leads to out-of-distribution (columns 3-4). An interesting\nobservation is that Magnet sometimes generates images with correct concepts, but incorrect positional\nrelations (columns 5-6). We suspect that the color layout has been determined in the early stage. In\nthis case, Magnet maps the object to the position of the attribute in the image, rather than blending\nthe attribute with the object. Magnet inherits the well-known issue of T2I models, presenting merged\nobjects (columns 7-8). Finally, it is still challenging to generate an unnatural concept when the object\nhas a strong attribute bias (columns 9-10).\n22",
        "target_bullets": "- Magnet has limitations in addressing the attribute binding problem, as evidenced by failure cases shown in Figure 21.\n- Limitations noted include the model's limited ability to foreground certain subjects and the potential for excessive manipulation of object embedding leading to out-of-distribution issues.\n- Magnet may generate images with correct concepts but incorrect positional relations, potentially due to color layout determinations in early stages.\n- The model inherits issues of T2I models, presenting merged objects in some cases.\n- Generating unnatural concepts is still challenging, particularly when objects have a strong attribute bias."
    },
    {
        "paper": "Deep Graph Mating.pdf",
        "target": "In this paper, we explore a novel GRAMA task for learning-free GNN reuse. The child model from\nGRAMA is expected to functionally merge knowledge from pre-trained parent models. Uniquely,\nGRAMA establishes the \ufb01rst paradigm in GNN reuse that operates entirely without re-training or \ufb01ne-\ntuning, while also eliminating the need for ground-truth labels. To this end, we start by developing two\nvanilla GRAMA approaches, which reveal speci\ufb01c challenges inherent to GRAMA. These challenges\nmotivate us to develop a DuMCC framework for topology-aware model reuse, leveraging a parent\nmessage coordination scheme followed by child message calibration. Experiments on node- and\ngraph-level tasks across various domains demonstrate the effectiveness of the proposed approach for\nannotation-free knowledge transfer without additional learning.\nDespite its strengths, the proposed DuMCC is primarily designed for homogeneous GRAMA, as\ndiscussed in Sect. 3. Currently, the framework does not support cross-architecture heterogeneous\nGRAMA, where parent models have different architectures, such as a combination of GCN and\nGraphSAGE. Additionally, it does not handle scenarios where parent models address tasks at differ-\nent levels, such as node-level versus graph-level tasks\u2014another aspect of heterogeneous GRAMA.\nThese limitations primarily arise from the absence of direct correspondence between the differing\narchitectural layers of the parent models, an issue we plan to explore in our future work. We will\nalso explore the possibility of a fully data-independent GRAMA scheme and investigate broader\napplications beyond training-free model reuse, such as its use as a pre-processing step to facilitate\ngraph-based knowledge amalgamation. Further discussions on limitations and potential solutions are\nprovided in Sect. H of the appendix.\n10",
        "target_bullets": "- The DuMCC framework for topology-aware model reuse is limited to homogeneous GRAMA and does not support cross-architecture heterogeneous GRAMA. \n- The framework currently cannot handle scenarios where parent models have different architectures, like a combination of GCN and GraphSAGE, or when parent models address tasks at different levels, such as node-level versus graph-level tasks. \n- The limitations arise from the absence of direct correspondence between the differing architectural layers of the parent models, which the authors plan to explore in future work. \n- Future research will investigate the possibility of a fully data-independent GRAMA scheme and explore broader applications beyond training-free model reuse, such as graph-based knowledge amalgamation. \n- Additional discussions on limitations and potential solutions are provided in section H of the appendix."
    },
    {
        "paper": "Foundation Inference Models for Markov Jump Processes.pdf",
        "target": "The main limitations of our methodology clearly involve our synthetic distribution. Evaluating FIM\non empirical datasets whose distribution significantly deviates from our synthetic distribution will,\ninevitably, yield poor estimates. Consider Figure 4 (right), for example. The performance of FIM\nquickly deteriorates for V \u22653, for which the ratio between the largest and smallest rates gets larger\nthan about three orders of magnitude. These cases are unlikely under our prior Beta distributions, and\nhence effectively lie outside of our synthetic distribution.\nMore generally, the MJP dynamics underlying phenomena that feature long-lived, metastable states,\nultimately depends on the shape of the energy landscape characterizing the set X, inasmuch as the\ntransition rates between metastable states i and j (fij in our notation) are characterized by the depth\nof the energy traps (that is, the height of the barrier between them).",
        "target_bullets": "- Evaluation of the methodology on empirical datasets with distributions deviating significantly from the synthetic distribution may lead to poor estimates.\n- The performance of the methodology deteriorates for some cases where the ratio between the largest and smallest rates exceeds about three orders of magnitude, which are unlikely under the prior Beta distributions used in the study.\n- The methodology may not effectively capture long-lived, metastable states in phenomena with complex energy landscapes, particularly with high barriers between metastable states."
    }
]