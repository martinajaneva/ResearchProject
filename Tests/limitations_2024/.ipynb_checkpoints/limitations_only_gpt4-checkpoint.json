[
    {
        "paper": "Tri-Level Navigator LLM-Empowered Tri-Level Learning for Time Series OOD Generalization.pdf",
        "target": "TTSO is a general framework for learning invariant representations across diverse domain distri-\nbutions, currently discussed only for time series classification. This framework could be further\nenhanced by extending it to more time series OOD tasks, such as time series forecasting and anomaly\ndetection. Additionally, distribution shifts occur not only in time series but also in other machine\nlearning domains like images [Deecke et al., 2021] and text [Tan et al., 2022]. Applying our approach\nto these domains could further improve performance.",
        "target_bullets": "- TTSO is currently evaluated only on time series classification tasks.\n- The framework could be extended to other time series OOD (out-of-distribution) tasks, such as forecasting and anomaly detection.\n- Distribution shifts also occur in other domains, including images and text.\n- Applying TTSO to domains beyond time series may further improve its performance."
    },
    {
        "paper": "FinBen A Holistic Financial Benchmark for Large Language Models.pdf",
        "target": "Despite the novel efforts to benchmark LLMs in the financial domain through FinBen, we acknowl-\nedge several inherent limitations that could impact the benchmark\u2019s effectiveness and applicability:\nDataset Size Limitations: The restricted size of available datasets, a common issue in open-source\nfinancial data, may affect the models\u2019 financial understanding and generalization across various\ncontexts. Model Size Limitations: Due to computational constraints, our evaluation was limited to\nthe LLaMA 70B model, potentially overlooking the capabilities of larger or differently architected\nmodels. Generalizability: The tasks, particularly trading and forecasting, are based on American\nmarket data and English texts, possibly limiting the benchmark\u2019s applicability to global financial mar-\nkets. Potential Negative Impacts: While FinBen aims to advance financial language understanding,\nit is crucial to consider potential misuse, such as propagating financial misinformation or exerting\nunethical influence on markets. Responsible usage and further safeguards are essential13.",
        "target_bullets": "- Limited dataset size: The small and restricted nature of available open-source financial datasets may affect the models\u2019 financial understanding and ability to generalize across different contexts.  \n- Model constraints: Evaluation was only conducted on the LLaMA 70B model due to computational limitations, potentially overlooking the performance of larger or differently designed models.  \n- Lack of global generalizability: Benchmark tasks rely primarily on American market data and English texts, which may limit applicability to global financial markets.  \n- Risk of negative outcomes: There is potential for misuse of FinBen, such as spreading financial misinformation or exerting unethical market influence, highlighting the need for responsible usage and additional safeguards."
    },
    {
        "paper": "Magnet We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function.pdf",
        "target": "Although Magnet provides an efficient and effective way to address the attribute binding problem, we\nacknowledge our technique is subject to a few limitations.\nFig. 21 displays the failure cases of Magnet. First, the neglect of the object (columns 1-2), may\nbe attributed to the model\u2019s limited ability to foreground limited subjects. Second, the excessive\nmanipulation of the object embedding leads to out-of-distribution (columns 3-4). An interesting\nobservation is that Magnet sometimes generates images with correct concepts, but incorrect positional\nrelations (columns 5-6). We suspect that the color layout has been determined in the early stage. In\nthis case, Magnet maps the object to the position of the attribute in the image, rather than blending\nthe attribute with the object. Magnet inherits the well-known issue of T2I models, presenting merged\nobjects (columns 7-8). Finally, it is still challenging to generate an unnatural concept when the object\nhas a strong attribute bias (columns 9-10).\n22",
        "target_bullets": "- Magnet may neglect objects or fail to foreground limited subjects, resulting in missing objects in the generated images.\n- Over-manipulation of the object embedding can produce out-of-distribution results.\n- The model sometimes generates images with correct concepts but incorrect positional relationships, possibly because the color layout is determined early in the process.\n- Magnet may map objects to the position of attributes rather than blending them, leading to positional mismatches between objects and attributes.\n- The approach inherits the common issue of text-to-image (T2I) models, such as merging or blending multiple objects unintentionally.\n- It remains challenging for Magnet to generate unnatural concepts when an object has a strong attribute bias."
    },
    {
        "paper": "Deep Graph Mating.pdf",
        "target": "In this paper, we explore a novel GRAMA task for learning-free GNN reuse. The child model from\nGRAMA is expected to functionally merge knowledge from pre-trained parent models. Uniquely,\nGRAMA establishes the \ufb01rst paradigm in GNN reuse that operates entirely without re-training or \ufb01ne-\ntuning, while also eliminating the need for ground-truth labels. To this end, we start by developing two\nvanilla GRAMA approaches, which reveal speci\ufb01c challenges inherent to GRAMA. These challenges\nmotivate us to develop a DuMCC framework for topology-aware model reuse, leveraging a parent\nmessage coordination scheme followed by child message calibration. Experiments on node- and\ngraph-level tasks across various domains demonstrate the effectiveness of the proposed approach for\nannotation-free knowledge transfer without additional learning.\nDespite its strengths, the proposed DuMCC is primarily designed for homogeneous GRAMA, as\ndiscussed in Sect. 3. Currently, the framework does not support cross-architecture heterogeneous\nGRAMA, where parent models have different architectures, such as a combination of GCN and\nGraphSAGE. Additionally, it does not handle scenarios where parent models address tasks at differ-\nent levels, such as node-level versus graph-level tasks\u2014another aspect of heterogeneous GRAMA.\nThese limitations primarily arise from the absence of direct correspondence between the differing\narchitectural layers of the parent models, an issue we plan to explore in our future work. We will\nalso explore the possibility of a fully data-independent GRAMA scheme and investigate broader\napplications beyond training-free model reuse, such as its use as a pre-processing step to facilitate\ngraph-based knowledge amalgamation. Further discussions on limitations and potential solutions are\nprovided in Sect. H of the appendix.\n10",
        "target_bullets": "- The current DuMCC framework is designed only for homogeneous GRAMA, where parent models share the same architecture.\n- It does not support cross-architecture heterogeneous GRAMA, such as combining models like GCN and GraphSAGE.\n- The framework is unable to handle parent models addressing tasks at different levels (e.g., merging node-level and graph-level tasks).\n- These limitations are mainly due to the lack of correspondence between layers in different parent model architectures.\n- The authors plan to explore support for heterogeneous architectures, fully data-independent GRAMA schemes, and broader applications (e.g., using GRAMA as a pre-processing step) in future work."
    },
    {
        "paper": "Foundation Inference Models for Markov Jump Processes.pdf",
        "target": "The main limitations of our methodology clearly involve our synthetic distribution. Evaluating FIM\non empirical datasets whose distribution significantly deviates from our synthetic distribution will,\ninevitably, yield poor estimates. Consider Figure 4 (right), for example. The performance of FIM\nquickly deteriorates for V \u22653, for which the ratio between the largest and smallest rates gets larger\nthan about three orders of magnitude. These cases are unlikely under our prior Beta distributions, and\nhence effectively lie outside of our synthetic distribution.\nMore generally, the MJP dynamics underlying phenomena that feature long-lived, metastable states,\nultimately depends on the shape of the energy landscape characterizing the set X, inasmuch as the\ntransition rates between metastable states i and j (fij in our notation) are characterized by the depth\nof the energy traps (that is, the height of the barrier between them).",
        "target_bullets": "- The methodology relies on a synthetic data distribution, which limits its generalizability to empirical datasets with significantly different distributions.\n- The FIM (Fisher Information Matrix) yields poor estimates when applied to data where the largest to smallest rate ratios exceed about three orders of magnitude\u2014scenarios unlikely under the assumed prior Beta distributions.\n- The method performs inadequately for cases outside the synthetic distribution, as demonstrated in Figure 4 (right) for V \u2265 3.\n- The accuracy of analysis is also influenced by the underlying Markov Jump Process (MJP) dynamics, specifically by the energy landscape and transition rates between metastable states, which may differ in real-world phenomena."
    }
]