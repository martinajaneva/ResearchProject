[
    {
        "paper": "Tri-Level Navigator LLM-Empowered Tri-Level Learning for Time Series OOD Generalization.pdf",
        "target": "TTSO is a general framework for learning invariant representations across diverse domain distri-\nbutions, currently discussed only for time series classification. This framework could be further\nenhanced by extending it to more time series OOD tasks, such as time series forecasting and anomaly\ndetection. Additionally, distribution shifts occur not only in time series but also in other machine\nlearning domains like images [Deecke et al., 2021] and text [Tan et al., 2022]. Applying our approach\nto these domains could further improve performance.",
        "target_bullets": "- The current TTSO framework is limited to time series classification and has not yet been extended to other time series out-of-distribution tasks such as forecasting and anomaly detection.  \n- The framework's applicability has been primarily demonstrated in the context of time series data, with potential for broader domain applications.  \n- Distribution shifts are a common challenge across various machine learning domains, including images and text, not just time series data.  \n- The effectiveness of applying the TTSO approach to domains like images and text remains to be explored and validated in future research.  \n- Extending and testing the framework in these additional tasks and domains could lead to further performance improvements."
    },
    {
        "paper": "FinBen A Holistic Financial Benchmark for Large Language Models.pdf",
        "target": "Despite the novel efforts to benchmark LLMs in the financial domain through FinBen, we acknowl-\nedge several inherent limitations that could impact the benchmark\u2019s effectiveness and applicability:\nDataset Size Limitations: The restricted size of available datasets, a common issue in open-source\nfinancial data, may affect the models\u2019 financial understanding and generalization across various\ncontexts. Model Size Limitations: Due to computational constraints, our evaluation was limited to\nthe LLaMA 70B model, potentially overlooking the capabilities of larger or differently architected\nmodels. Generalizability: The tasks, particularly trading and forecasting, are based on American\nmarket data and English texts, possibly limiting the benchmark\u2019s applicability to global financial mar-\nkets. Potential Negative Impacts: While FinBen aims to advance financial language understanding,\nit is crucial to consider potential misuse, such as propagating financial misinformation or exerting\nunethical influence on markets. Responsible usage and further safeguards are essential13.",
        "target_bullets": "- Limited dataset sizes in open-source financial data may hinder the models' ability to understand and generalize across diverse financial contexts.  \n- Evaluation was confined to the LLaMA 70B model due to computational constraints, which may exclude insights from larger or different model architectures.  \n- The benchmark primarily focuses on American market data and English texts, potentially restricting its relevance to other global financial markets.  \n- There is a risk of misuse of the benchmark, including the propagation of financial misinformation or unethical influence on markets, underscoring the need for responsible usage and safeguards."
    },
    {
        "paper": "Magnet We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function.pdf",
        "target": "Although Magnet provides an efficient and effective way to address the attribute binding problem, we\nacknowledge our technique is subject to a few limitations.\nFig. 21 displays the failure cases of Magnet. First, the neglect of the object (columns 1-2), may\nbe attributed to the model\u2019s limited ability to foreground limited subjects. Second, the excessive\nmanipulation of the object embedding leads to out-of-distribution (columns 3-4). An interesting\nobservation is that Magnet sometimes generates images with correct concepts, but incorrect positional\nrelations (columns 5-6). We suspect that the color layout has been determined in the early stage. In\nthis case, Magnet maps the object to the position of the attribute in the image, rather than blending\nthe attribute with the object. Magnet inherits the well-known issue of T2I models, presenting merged\nobjects (columns 7-8). Finally, it is still challenging to generate an unnatural concept when the object\nhas a strong attribute bias (columns 9-10).\n22",
        "target_bullets": "- Magnet may fail to foreground objects effectively, leading to neglect of the object in some cases.  \n- Excessive manipulation of object embeddings can result in out-of-distribution outputs.  \n- The model sometimes produces images with correct concepts but incorrect positional relationships.  \n- Magnet tends to map attributes to object positions rather than blending attributes with objects, especially affecting color layout.  \n- The technique inherits common issues from T2I models, such as merging objects incorrectly.  \n- Generating unnatural concepts remains challenging when objects have strong attribute biases."
    },
    {
        "paper": "Deep Graph Mating.pdf",
        "target": "In this paper, we explore a novel GRAMA task for learning-free GNN reuse. The child model from\nGRAMA is expected to functionally merge knowledge from pre-trained parent models. Uniquely,\nGRAMA establishes the \ufb01rst paradigm in GNN reuse that operates entirely without re-training or \ufb01ne-\ntuning, while also eliminating the need for ground-truth labels. To this end, we start by developing two\nvanilla GRAMA approaches, which reveal speci\ufb01c challenges inherent to GRAMA. These challenges\nmotivate us to develop a DuMCC framework for topology-aware model reuse, leveraging a parent\nmessage coordination scheme followed by child message calibration. Experiments on node- and\ngraph-level tasks across various domains demonstrate the effectiveness of the proposed approach for\nannotation-free knowledge transfer without additional learning.\nDespite its strengths, the proposed DuMCC is primarily designed for homogeneous GRAMA, as\ndiscussed in Sect. 3. Currently, the framework does not support cross-architecture heterogeneous\nGRAMA, where parent models have different architectures, such as a combination of GCN and\nGraphSAGE. Additionally, it does not handle scenarios where parent models address tasks at differ-\nent levels, such as node-level versus graph-level tasks\u2014another aspect of heterogeneous GRAMA.\nThese limitations primarily arise from the absence of direct correspondence between the differing\narchitectural layers of the parent models, an issue we plan to explore in our future work. We will\nalso explore the possibility of a fully data-independent GRAMA scheme and investigate broader\napplications beyond training-free model reuse, such as its use as a pre-processing step to facilitate\ngraph-based knowledge amalgamation. Further discussions on limitations and potential solutions are\nprovided in Sect. H of the appendix.\n10",
        "target_bullets": "- The proposed DuMCC framework is primarily designed for homogeneous GRAMA and does not currently support cross-architecture heterogeneous scenarios, such as combining different GNN architectures like GCN and GraphSAGE.  \n- It also cannot handle multi-task scenarios where parent models operate at different levels (e.g., node-level vs. graph-level tasks).  \n- The limitations mainly stem from the lack of direct correspondence between varying architectural layers of parent models, which the authors plan to address in future work.  \n- Future research directions include developing a fully data-independent GRAMA scheme and exploring broader applications beyond training-free reuse, such as graph-based knowledge amalgamation.  \n- Additional discussions on limitations and potential solutions are available in Section H of the appendix."
    },
    {
        "paper": "Foundation Inference Models for Markov Jump Processes.pdf",
        "target": "The main limitations of our methodology clearly involve our synthetic distribution. Evaluating FIM\non empirical datasets whose distribution significantly deviates from our synthetic distribution will,\ninevitably, yield poor estimates. Consider Figure 4 (right), for example. The performance of FIM\nquickly deteriorates for V \u22653, for which the ratio between the largest and smallest rates gets larger\nthan about three orders of magnitude. These cases are unlikely under our prior Beta distributions, and\nhence effectively lie outside of our synthetic distribution.\nMore generally, the MJP dynamics underlying phenomena that feature long-lived, metastable states,\nultimately depends on the shape of the energy landscape characterizing the set X, inasmuch as the\ntransition rates between metastable states i and j (fij in our notation) are characterized by the depth\nof the energy traps (that is, the height of the barrier between them).",
        "target_bullets": "- The methodology's main limitation is its reliance on synthetic distributions, which may not accurately represent empirical datasets with significantly different distributions.\n- Performance of FIM deteriorates quickly when the parameter V exceeds 3, especially when the ratio of the largest to smallest rates becomes very large, indicating poor estimates outside the synthetic distribution range.\n- The synthetic data, based on prior Beta distributions, does not capture scenarios with large disparities in transition rates, which are more representative of real-world data.\n- The underlying Markov jump process (MJP) dynamics are influenced by the energy landscape of the state space, affecting transition rates between metastable states.\n- The approach may not perform well for phenomena with long-lived, metastable states, as these are heavily dependent on the specific shape and barriers within the energy landscape."
    }
]