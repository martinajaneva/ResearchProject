{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3305c5fc-4a7e-47ba-a2d4-3d97539dc6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "!pip install --upgrade PyMuPDF\n",
    "from API_KEY import API_KEY\n",
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import json\n",
    "\n",
    "output_folder = \"limitations_2024\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "inline_limitations = re.compile(\n",
    "    r\"\\b(Limitations?|Challenges?)[:\\s]+(.{100,1500}?)\",\n",
    "    flags=re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "\n",
    "find_limitations = re.compile(\n",
    "    r\"(?:^|\\n)(?:\\d{0,2}[\\.]\\s*)?\"\n",
    "    r\"(limitations|limitation|conclusions and limitations|future work|conclusion (?:and|&) future work|limitations (?:and|&) future work|conclusion|conclusions|discussion|conclusion (?:and|&) discussion|conclusions|research limitations|study limitations|challenges)\"\n",
    "    r\"(?::)?\\s*\\n+(.*?)(?=\\n\\s*(?:\\d{1,2}[\\.]+\\s*)?[A-Z][A-Za-z0-9, \\-]{3,60}\\n|\\Z)\",\n",
    "    flags=re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "\n",
    "def text_from_pdf(path):\n",
    "    text = \"\"\n",
    "    with fitz.open(path) as doc:\n",
    "        for page in doc:\n",
    "            text += \" \".join([block[4] for block in page.get_text(\"blocks\")]) + \"\\n\"\n",
    "    return text\n",
    "\n",
    "keywords = [\"limitations\", \"conclusions and limitations\",\"future work\", \"challenges\", \"limitation\", \"study limitations\", \"research limitations\", \"limitations and future work\", \"conclusion and future work\", \"conclusion & future work\"]\n",
    "def find_section(txt):\n",
    "    paper_sections = {}\n",
    "    for f in find_limitations.finditer(txt):\n",
    "        title = f.group(1).strip().lower()\n",
    "        text = f.group(2).strip()\n",
    "        if not text.lower().startswith(\"question: does the paper discuss the limitations\"):\n",
    "            paper_sections[title] = text\n",
    "    has_limitations = any(k in paper_sections for k in keywords)\n",
    "    if not has_limitations:\n",
    "        for section_title in [\"conclusion\", \"conclusions and limitations\" \"conclusions\", \"discussion\", \"conclusion and discussion\", \"conclusion & discussion\"]:\n",
    "            section_text = paper_sections.get(section_title)\n",
    "            if section_text:\n",
    "                match = inline_limitations.search(section_text)\n",
    "                if match:\n",
    "                    paper_sections[\"inline\"] = match.group(1).strip()\n",
    "                break\n",
    "    return paper_sections\n",
    "    \n",
    "def extract_limitations(path):\n",
    "    doc = fitz.open(path)\n",
    "    txt = \"\"\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        txt += page.get_text(\"text\") + \"\\n\"\n",
    "    title_paper = os.path.basename(path)\n",
    "    paper_sections = find_section(txt)\n",
    "    return title_paper, paper_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7582d74d-444f-44d3-9a83-d863666671ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unidecode\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import os\n",
    "import json\n",
    "import unidecode\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from difflib import get_close_matches \n",
    "\n",
    "pdf_folder = \"Papers\"\n",
    "abstracts_file = \"../retrieved_abstracts/data_2024/papers_data.jsonl\"\n",
    "output_file = \"test_inputs/abstract_conclusion_test.json\"\n",
    "os.makedirs(\"test_inputs\", exist_ok=True)\n",
    "\n",
    "def clean_title(title):\n",
    "    title = unidecode.unidecode(title)\n",
    "    title = title.lower()\n",
    "    title = re.sub(r\"[^a-z0-9 ]\", \" \", title)\n",
    "    title = re.sub(r\"\\s+\", \" \", title).strip()\n",
    "    return title\n",
    "\n",
    "abstract_map = {}\n",
    "with open(abstracts_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "            if \"title\" in paper and \"abstract\" in paper:\n",
    "                abstract_map[clean_title(paper[\"title\"])] = paper[\"abstract\"]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for file in tqdm(os.listdir(pdf_folder)):\n",
    "    if not file.endswith(\".pdf\"):\n",
    "        continue\n",
    "\n",
    "    raw_title = file.replace(\".pdf\", \"\")\n",
    "    cleaned = clean_title(raw_title)\n",
    "\n",
    "    abstract = abstract_map.get(cleaned)\n",
    "    if not abstract:\n",
    "        matches = get_close_matches(cleaned, list(abstract_map.keys()), n=1, cutoff=0.7)\n",
    "        if matches:\n",
    "            abstract = abstract_map[matches[0]]\n",
    "        else:\n",
    "            continue\n",
    "    try:\n",
    "        _, sections = extract_limitations(os.path.join(pdf_folder, file))\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "    conclusion = (\n",
    "        sections.get(\"conclusion\")\n",
    "        or sections.get(\"discussion\")\n",
    "        or sections.get(\"conclusions\")\n",
    "        or sections.get(\"conclusion and discussion\")\n",
    "        or sections.get(\"conclusion & discussion\")\n",
    "        or sections.get(\"conclusions and limitations\")\n",
    "    )\n",
    "\n",
    "    if not conclusion:\n",
    "        continue\n",
    "\n",
    "    combined_input = abstract.strip() + \"\\n\\n\" + conclusion.strip()\n",
    "\n",
    "    dataset.append({\n",
    "        \"paper\": raw_title,\n",
    "        \"input\": combined_input\n",
    "    })\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46599f7e-34da-4f45-b396-63c8da95b2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import unidecode\n",
    "import re\n",
    "from difflib import get_close_matches\n",
    "from tqdm import tqdm\n",
    "\n",
    "pdf_folder = \"Papers\"\n",
    "tokenized_file = \"../tokenized_data/tokenized_2024.json\"\n",
    "output_file = \"test_inputs/tokenized_test.json\"\n",
    "os.makedirs(\"test_inputs\", exist_ok=True)\n",
    "\n",
    "def clean_title(title):\n",
    "    title = unidecode.unidecode(title)\n",
    "    title = title.lower()\n",
    "    title = re.sub(r\"[^a-z0-9 ]\", \" \", title)\n",
    "    title = re.sub(r\"\\s+\", \" \", title).strip()\n",
    "    return title\n",
    "\n",
    "with open(tokenized_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    tokenized_data = json.load(f)\n",
    "\n",
    "tokenized_map = {\n",
    "    clean_title(entry[\"title\"]): entry[\"tokens\"]\n",
    "    for entry in tokenized_data.values()\n",
    "    if isinstance(entry, dict) and \"title\" in entry and \"tokens\" in entry\n",
    "}\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for file in tqdm(os.listdir(pdf_folder)):\n",
    "    if not file.endswith(\".pdf\"):\n",
    "        continue\n",
    "\n",
    "    raw_title = file.replace(\".pdf\", \"\")\n",
    "    cleaned = clean_title(raw_title)\n",
    "\n",
    "    tokens = tokenized_map.get(cleaned)\n",
    "    if not tokens:\n",
    "        matches = get_close_matches(cleaned, list(tokenized_map.keys()), n=1, cutoff=0.7)\n",
    "        if matches:\n",
    "            tokens = tokenized_map[matches[0]]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    dataset.append({\n",
    "        \"paper\": raw_title,\n",
    "        \"input\": \" \".join(tokens)\n",
    "    })\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset, f, indent=4)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
