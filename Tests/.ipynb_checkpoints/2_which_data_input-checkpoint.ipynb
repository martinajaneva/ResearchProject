{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6e26f5-5dcd-449f-8b55-262a12b11453",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.31.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.31.2-py3-none-any.whl (484 kB)\n",
      "Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.31.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.51.3\n"
     ]
    }
   ],
   "source": [
    "# import models\n",
    "import sys\n",
    "import os\n",
    "!pip install transformers\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(),\"..\", \"helper\")))\n",
    "from bart_limitations import generate_lims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f7e77a-fb3b-47df-a75c-296f9f22974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "generate_lims(\n",
    "    model_path=\"../Generate_limitations/training_model/model_output_abstract/final\",\n",
    "    input_path = \"test_inputs/abstract_conclusion_test.json\",\n",
    "    output_path=\"test_outputs/generated_limitations_abstract.jsonl\"\n",
    ")\n",
    "generate_lims(\n",
    "    model_path=\"../Generate_limitations/training_model/model_output_tokenized/final\",\n",
    "    input_path = \"test_inputs/tokenized_test.json\",\n",
    "    output_path=\"test_outputs/generated_limitations_tokenized.jsonl\"\n",
    ")\n",
    "generate_lims(\n",
    "    model_path=\"../Generate_limitations/training_model/model_output_full/final\",\n",
    "    input_path = \"test_inputs/full_text_test.json\",\n",
    "    output_path=\"test_outputs/generated_limitations_full.jsonl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89def036-1127-44b2-8979-d0721d2cf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "smooth_function = SmoothingFunction().method4\n",
    "\n",
    "def clean_bullet_points(bullets):\n",
    "    if isinstance(bullets, str):\n",
    "        return [b.strip() for b in bullets.split(\" - \") if b.strip()]\n",
    "    elif isinstance(text, list):\n",
    "        return text\n",
    "    return []\n",
    "\n",
    "def flat_bullets(text):\n",
    "    if isinstance(text, list):\n",
    "        return \" \".join(text)\n",
    "    elif isinstance(text, str):\n",
    "        return text\n",
    "    return \"\"\n",
    "    \n",
    "ground_truth_file = \"limitations_2024/limitations_only_gpt4_nano.json\"\n",
    "with open(ground_truth_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    ground_truth = {\n",
    "        i[\"paper\"].replace(\".pdf\", \"\"): i[\"target_bullets\"] \n",
    "        for i in json.load(f) \n",
    "        if \"paper\" in i and \"target_bullets\" in i}\n",
    "    \n",
    "generated_files = {\n",
    "    \"abstract\": \"test_outputs/generated_limitations_abstract.jsonl\",\n",
    "    \"tokenized\": \"test_outputs/generated_limitations_tokenized.jsonl\",\n",
    "    \"full\": \"test_outputs/generated_limitations_full.jsonl\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "beabaac8-77ee-41a6-85a9-7e0404776524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score for abstract: 0.025879 (from 5 papers)\n",
      "Bleu score for tokenized: 0.027108 (from 5 papers)\n",
      "Bleu score for full: 0.033661 (from 5 papers)\n"
     ]
    }
   ],
   "source": [
    "def bleu_score(ground, text):\n",
    "    ref_bullets = clean_bullet_points(ground)\n",
    "    hyp_bullets = clean_bullet_points(text)\n",
    "\n",
    "    ref = [\" \".join(ref_bullets).split()]\n",
    "    hyp = \"  \".join(hyp_bullets).split()\n",
    "    if not ref or not hyp:\n",
    "        return 0\n",
    "    return sentence_bleu(ref, hyp, smoothing_function=smooth_function)\n",
    "\n",
    "for model, path in generated_files.items():\n",
    "    score = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for l in f:\n",
    "            item = json.loads(l)\n",
    "            paper = item[\"paper\"]\n",
    "            ground = ground_truth.get(paper)\n",
    "            generated = item[\"generated\"]\n",
    "            bleu = bleu_score(ground, generated)\n",
    "            score.append(bleu)\n",
    "    mean = sum(score) / len(score) if score else 0\n",
    "    print(f\"Bleu score for {model}: {mean:4f} (from {len(score)} papers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80d4f93b-3a71-45f2-9283-b52a9cb8b6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge-score in /opt/conda/lib/python3.11/site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from rouge-score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (from rouge-score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from rouge-score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.11/site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk->rouge-score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk->rouge-score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk->rouge-score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk->rouge-score) (4.66.4)\n",
      "Rouge score for abstract:\n",
      "Rouge-1 F1: 0.2996\n",
      "Rouge-L F1: 0.1671\n",
      "Rouge score for tokenized:\n",
      "Rouge-1 F1: 0.2999\n",
      "Rouge-L F1: 0.1749\n",
      "Rouge score for full:\n",
      "Rouge-1 F1: 0.3232\n",
      "Rouge-L F1: 0.1820\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "for model, path in generated_files.items():\n",
    "    r1 = []\n",
    "    rl = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for l in f:\n",
    "            item = json.loads(l)\n",
    "            paper = item[\"paper\"]\n",
    "            ground = ground_truth.get(paper)\n",
    "            generated = flat_bullets(item[\"generated\"])\n",
    "            \n",
    "            score = scorer.score(ground, generated)\n",
    "            r1.append(score[\"rouge1\"].fmeasure)\n",
    "            rl.append(score[\"rougeL\"].fmeasure)\n",
    "    mean_r1 = sum(r1) / len(r1) if r1 else 0\n",
    "    mean_rl = sum(rl) / len(rl) if rl else 0\n",
    "    print(f\"Rouge score for {model}:\")\n",
    "    print(f\"Rouge-1 F1: {mean_r1:.4f}\")\n",
    "    print(f\"Rouge-L F1: {mean_rl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2b0db594-fcaf-4344-8d80-a18d0c32afea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8764, 0.8754, 0.8717, 0.8693, 0.8624])\n",
      "BERT score for ABSTRACT:\n",
      " F1: 0.8710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8757, 0.8713, 0.8660, 0.8669, 0.8427])\n",
      "BERT score for TOKENIZED:\n",
      " F1: 0.8645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8792, 0.8639, 0.8584, 0.8678, 0.8764])\n",
      "BERT score for FULL:\n",
      " F1: 0.8691\n"
     ]
    }
   ],
   "source": [
    "# !pip install bert-score\n",
    "from bert_score import score\n",
    "import json\n",
    "\n",
    "for model, path in generated_files.items():\n",
    "    refs = []\n",
    "    hyps = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for l in f:\n",
    "            item = json.loads(l)\n",
    "            paper = item[\"paper\"].replace(\".pdf\", \"\")\n",
    "            ground = flat_bullets(ground_truth.get(paper))\n",
    "            generated = flat_bullets(item[\"generated\"])\n",
    "            \n",
    "            refs.append(ground)\n",
    "            hyps.append(generated)\n",
    "    P, R, F1 = score(hyps, refs, lang=\"en\",  device=\"cpu\", verbose=False)\n",
    "    print(F1)\n",
    "    mean_f1 = F1.mean().item()\n",
    "    print(f\"BERT score for {model.upper()}:\")\n",
    "    print(f\" F1: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8dc8c-d2a3-4288-8d90-63120a319b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
