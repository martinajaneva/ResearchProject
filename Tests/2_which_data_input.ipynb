{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b6e26f5-5dcd-449f-8b55-262a12b11453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "!pip install transformers --quiet\n",
    "!pip install openai --quiet\n",
    "!pip install more_itertools --quiet\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(),\"..\", \"helper\")))\n",
    "from bart_limitations import generate_lims\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4f7e77a-fb3b-47df-a75c-296f9f22974d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:08<00:00,  8.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop Finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "generate_lims(\n",
    "    model_path=\"../Generate_limitations/training_model/model_output_abstract/final\",\n",
    "    input_path = \"test_inputs/abstract_conclusion_test.json\",\n",
    "    output_path=\"test_outputs/generated_limitations_abstract.jsonl\",\n",
    "    check_limitations=False\n",
    ")\n",
    "generate_lims(\n",
    "    model_path=\"../Generate_limitations/training_model/model_output_tokenized/final\",\n",
    "    input_path = \"test_inputs/tokenized_test.json\",\n",
    "    output_path=\"test_outputs/generated_limitations_tokenized.jsonl\",\n",
    "    check_limitations=False\n",
    ")\n",
    "generate_lims(\n",
    "    model_path=\"../Generate_limitations/training_model/model_output_full/final\",\n",
    "    input_path = \"test_inputs/full_text_test.json\",\n",
    "    output_path=\"test_outputs/generated_limitations_full.jsonl\",\n",
    "    check_limitations=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89def036-1127-44b2-8979-d0721d2cf0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk --quiet\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm \n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "generated_files = {}\n",
    "smooth_function = SmoothingFunction().method4\n",
    "\n",
    "def clean_bullet_points(bullets):\n",
    "    if isinstance(bullets, str):\n",
    "        return [b.strip() for b in bullets.split(\" - \") if b.strip()]\n",
    "    elif isinstance(bullets, list):\n",
    "        return bullets\n",
    "    return []\n",
    "\n",
    "def flat_bullets(text):\n",
    "    if isinstance(text, list):\n",
    "        return \" \".join(text)\n",
    "    elif isinstance(text, str):\n",
    "        return text\n",
    "    return \"\"\n",
    "    \n",
    "ground_truth_file = \"limitations_2024/limitations_only_gpt4_nano.json\"\n",
    "with open(ground_truth_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    ground_truth = {\n",
    "        i[\"paper\"].replace(\".pdf\", \"\"): i[\"target_bullets\"] \n",
    "        for i in json.load(f) \n",
    "        if \"paper\" in i and \"target_bullets\" in i}\n",
    "    \n",
    "generated_files = {\n",
    "    \"abstract\": \"test_outputs/generated_limitations_abstract.jsonl\",\n",
    "    \"tokenized\": \"test_outputs/generated_limitations_tokenized.jsonl\",\n",
    "    \"full\": \"test_outputs/generated_limitations_full.jsonl\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "beabaac8-77ee-41a6-85a9-7e0404776524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu score for abstract: 0.015465 (from 5 papers)\n",
      "Bleu score for tokenized: 0.017546 (from 5 papers)\n",
      "Bleu score for full: 0.017826 (from 5 papers)\n"
     ]
    }
   ],
   "source": [
    "def bleu_score(ground, text):\n",
    "    ref_bullets = clean_bullet_points(ground)\n",
    "    hyp_bullets = clean_bullet_points(text)\n",
    "\n",
    "    ref = [\" \".join(ref_bullets).split()]\n",
    "    hyp = \"  \".join(hyp_bullets).split()\n",
    "    if not ref or not hyp:\n",
    "        return 0\n",
    "    return sentence_bleu(ref, hyp, smoothing_function=smooth_function)\n",
    "\n",
    "for model, path in generated_files.items():\n",
    "    score = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for l in f:\n",
    "            item = json.loads(l)\n",
    "            paper = item[\"paper\"]\n",
    "            ground = ground_truth.get(paper)\n",
    "            generated = item[\"generated\"]\n",
    "            bleu = bleu_score(ground, generated)\n",
    "            score.append(bleu)\n",
    "    mean = sum(score) / len(score) if score else 0\n",
    "    print(f\"Bleu score for {model}: {mean:4f} (from {len(score)} papers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80d4f93b-3a71-45f2-9283-b52a9cb8b6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge score for abstract:\n",
      "Rouge-1 F1: 0.2909\n",
      "Rouge-L F1: 0.1600\n",
      "Rouge score for tokenized:\n",
      "Rouge-1 F1: 0.2789\n",
      "Rouge-L F1: 0.1581\n",
      "Rouge score for full:\n",
      "Rouge-1 F1: 0.3011\n",
      "Rouge-L F1: 0.1800\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge-score --quiet\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "for model, path in generated_files.items():\n",
    "    r1 = []\n",
    "    rl = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for l in f:\n",
    "            item = json.loads(l)\n",
    "            paper = item[\"paper\"]\n",
    "            ground = ground_truth.get(paper)\n",
    "            generated = flat_bullets(item[\"generated\"])\n",
    "            \n",
    "            score = scorer.score(ground, generated)\n",
    "            r1.append(score[\"rouge1\"].fmeasure)\n",
    "            rl.append(score[\"rougeL\"].fmeasure)\n",
    "    mean_r1 = sum(r1) / len(r1) if r1 else 0\n",
    "    mean_rl = sum(rl) / len(rl) if rl else 0\n",
    "    print(f\"Rouge score for {model}:\")\n",
    "    print(f\"Rouge-1 F1: {mean_r1:.4f}\")\n",
    "    print(f\"Rouge-L F1: {mean_rl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b0db594-fcaf-4344-8d80-a18d0c32afea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from bert-score) (2.4.0+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from bert-score) (2.2.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from bert-score) (4.51.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /opt/conda/lib/python3.11/site-packages (from bert-score) (4.66.4)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (from bert-score) (3.9.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from bert-score) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.0.1->bert-score) (2024.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (1.13.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.0.0->bert-score) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.0.0->bert-score) (12.5.82)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.31.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from transformers>=3.0.0->bert-score) (0.5.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert-score) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert-score) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert-score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert-score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib->bert-score) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->bert-score) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->bert-score) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->bert-score) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->bert-score) (2024.7.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: bert-score\n",
      "Successfully installed bert-score-0.3.13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d721120f2ac8423e88532dc8e8ecbcdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfff932351a4eaf9bdf108862072a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80318412790e46d4b7f1604bfd71ed4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d072021640ee462f9572e37bc2593a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13e68e92eb6145cfb696025e465b6690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0e509e3c06483d9d29792328a4ad9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8660, 0.8717, 0.8609, 0.8737, 0.8512])\n",
      "BERT score for ABSTRACT:\n",
      " F1: 0.8647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8816, 0.8740, 0.8584, 0.8530, 0.8496])\n",
      "BERT score for TOKENIZED:\n",
      " F1: 0.8633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8712, 0.8643, 0.8645, 0.8741, 0.8546])\n",
      "BERT score for FULL:\n",
      " F1: 0.8657\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score\n",
    "from bert_score import score\n",
    "import json\n",
    "\n",
    "for model, path in generated_files.items():\n",
    "    refs = []\n",
    "    hyps = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for l in f:\n",
    "            item = json.loads(l)\n",
    "            paper = item[\"paper\"].replace(\".pdf\", \"\")\n",
    "            ground = flat_bullets(ground_truth.get(paper))\n",
    "            generated = flat_bullets(item[\"generated\"])\n",
    "            \n",
    "            refs.append(ground)\n",
    "            hyps.append(generated)\n",
    "    P, R, F1 = score(hyps, refs, lang=\"en\",  device=\"cpu\", verbose=False)\n",
    "    print(F1)\n",
    "    mean_f1 = F1.mean().item()\n",
    "    print(f\"BERT score for {model.upper()}:\")\n",
    "    print(f\" F1: {mean_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8dc8c-d2a3-4288-8d90-63120a319b33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
