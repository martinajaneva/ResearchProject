[
    {
        "paper": "Tri-Level Navigator LLM-Empowered Tri-Level Learning for Time Series OOD Generalization",
        "input": "Tri-Level Navigator: LLM-Empowered Tri-Level\nLearning for Time Series OOD Generalization\nChengtao Jian\nTongji University, Shanghai, China\njct@tongji.edu.cn\nKai Yang\u2217\nTongji University, Shanghai, China\nkaiyang@tongji.edu.cn\nYang Jiao\nTongji University, Shanghai, China\nyangjiao@tongji.edu.cn\nAbstract\nOut-of-Distribution (OOD) generalization in machine learning is a burgeoning\narea of study. Its primary goal is to enhance the adaptability and resilience of\nmachine learning models when faced with new, unseen, and potentially adversarial\ndata that significantly diverges from their original training datasets. In this paper,\nwe investigate time series OOD generalization via pre-trained Large Language\nModels (LLMs). We first propose a novel Tri-level learning framework for Time\nSeries OOD generalization, termed TTSO, which considers both sample-level and\ngroup-level uncertainties. This formula offers a fresh theoretic perspective for\nformulating and analyzing OOD generalization problem. In addition, we provide\na theoretical analysis to justify this method is well motivated. We then develop\na stratified localization algorithm tailored for this tri-level optimization problem,\ntheoretically demonstrating the guaranteed convergence of the proposed algorithm.\nOur analysis also reveals that the iteration complexity to obtain an \u03f5-stationary\npoint is bounded by O( 1\n\u03f52 ). Extensive experiments on real-world datasets have\nbeen conducted to elucidate the effectiveness of the proposed method.\n1\nIntroduction\nIn machine learning, a common challenge arises when the distributions of training and test sets\ndiffer significantly [Qui\u00f1onero-Candela et al., 2009]. This mismatch demands that models, trained\non specific distribution data, should generalize well on unseen distribution data, known as OOD\ngeneralization [Shen et al., 2021, Zhou et al., 2022]. Despite a vast amount of research on the OOD\ngeneralization [Zhang et al., 2017, Sagawa et al., 2019, Huang et al., 2020, Arjovsky et al., 2019],\nthe field of OOD generalization in time series is relatively limited and presents more significant\nchallenges. This is primarily due to the inherent temporal dependencies and dynamic changes\ncharacteristic of time series data [Hamilton, 2020]. Therefore, a critical aspect of improving time\nseries OOD generalization is to learn robust representations that remain stable despite shifts in\ndistributions.\nRecently, the field of machine learning has witnessed remarkable advancements in pre-trained\nfoundation models, with notable examples including Large Language Models (LLMs) such as GPT\n[Radford et al., 2018], LLaMA [Touvron et al., 2023] and CLIP [Radford et al., 2021]. These models\nhave been instrumental in capturing and leveraging complex patterns across various domains. In\naddition, using foundation models, especially LLMs, in processing non-linguistic data, e.g., time\nseries is increasingly drawing attention. By fine-tuning only a few handful of parameters, these\n\u2217Corresponding author.\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\nmodels show remarkable versatility in diverse data formats ranging from audio [Ghosal et al., 2023],\nimage [Lu et al., 2021] and time series [Chang et al., 2023, Jin et al., 2023]. Studies indicate that\nLLMs, as part of the broader foundation model spectrum, demonstrate sophisticated reasoning and\nstrong pattern recognition capabilities [Wang et al., 2023, Chu et al., 2023], fundamentally acting as\npattern machines [Mirchandani et al., 2023]. Moreover, LLMs have been shown to be effective in\ntransfer learning across various modalities, due to their data-independent self-attention mechanism\n[Zhou et al., 2023].\nAdditionally, recent advancements in vision-language foundation models have shown promising\ndevelopments in OOD generalization [Zheng et al., 2022], yet the exploration in time series remains\nunderdeveloped. The potential of using foundational models is highlighted by the study in Liu et al.\n[2023a], Hendrycks et al. [2020], which suggests that pre-trained transformers can improve OOD\nrobustness. Despite existing efforts, the limited exploration of foundational model applications in\ntime series OOD generalization suggests an emerging field.\nIn this paper, we propose a Tri-level learning framework for Time Series OOD generalization, named\nTTSO. Unlike conventional OOD generalization methods that focus solely on group-level [Jiao\net al., 2022a, Huang et al., 2020] or sample-level uncertainties [Zhang et al., 2017, Zhou et al., 2021,\nHan et al., 2024], our framework uniquely addresses both by combing a minimization problem for\noptimal model parameter learning, a maximization problem for dynamically data re-grouping, and\nanother maximization problem for data augmentation under a tri-level framework. To tackle this\ntri-level problem, we propose a stratified localization algorithm via cutting planes. Leveraging the\nadvanced representation learning capabilities of LLMs, we adapt this tri-level learning framework for\nfine-tuning LLMs.\nOur contributions can be summarized as follows:\n\u2022 Tri-level Learning Framework. In contrast to most existing works in OOD generalization,\nwhich primarily focus on either group-level or sample-level uncertainties, TTSO uniquely\nintegrates both aspects under a tri-level learning framework. Specially, this comprehensive\nframework emphasizes the interdependent relationship between problems of each level,\nadvancing beyond the typical single or bi-level methodologies in OOD generalization.\nMoreover, a theoretical framework based on Vapnik-Chervonenkis dimension has been\ndeveloped to rigorously analyze and elucidate the generalization properties of TTSO. We\nthen leverage this tri-level framework to fine-tune LLMs, achieving an maximum 4.9%\nimprovement in performance on time series classification in OOD scenarios.\n\u2022 Stratified Localization Algorithm. To tackle the aforementioned tri-level optimization\nproblem, we develop a stratified localization method using cutting planes. Unlike traditional\ngradient-based methods, TTSO removes the necessity of computing the hypergradient\nfor the outer optimization problem. This computation is typically very challenging and\ncomputationally intensive due to the nested structure of the tri-level optimization problem.\nFurthermore, the decomposable nature of cutting planes offers a promising avenue for\nenabling distributed implementations of TTSO, thereby potentially enhancing scalability\nand computational efficiency.\n\u2022 Iteration Complexity Analysis. To validate the effectiveness of our method, we conducted\na thorough theoretical analysis of the algorithm. We theoretically derive that the iteration\ncomplexity of the proposed algorithm for achieving an \u03f5-stationary point is bounded by\nO\n\u0000 1\n\u03f52\n\u0001\n.\n2\nRelated Work\nIn this section, we provide an overview of the foundational concepts and methodologies related to\nour research, including OOD Generalization and the LLM in time series.\nOOD Generalization. OOD Generalization research focuses on improving the model\u2019s ability to\ngeneralize when there is a difference in distribution between the training and test data, and has been\nwidely studied in the fields of Computer Vision (CV) [Recht et al., 2019, Salman et al., 2021] and\nNatural Language Processing (NLP) [Tu et al., 2020, Schneider et al., 2020]. Existing works for\nout-of-distribution (OOD) generalization are diverse and can generally be categorized into approaches\nthat consider sample-level [Zhang et al., 2017, Zhou et al., 2021] or group-level [Sagawa et al., 2019,\n2\nHuang et al., 2020] uncertainty. However, the exploration of OOD generalization specially for time\nseries remains relatively underdeveloped. A recent study [Lu et al., 2023] introduced \u2019Diversify\u2019,\nan innovative approach that models time series data from the perspective of distribution and obtain\nsuperior performance. In our work, we consider both sample-level and group-level uncertainties and\nformulate them as a tri-level optimization problem.\nLLM in Time Series. The integration of LLMs in time series analysis is a rapidly evolving field,\ndrawing significant interest due to their superior pattern recognition and reasoning abilities [Wang\net al., 2023, Chu et al., 2023]. A recent example is Time-LLM [Jin et al., 2023], which introduces an\ninnovative method by reprogramming time series and incorporating linguistic prompts, effectively\nactivating the extensive capabilities of LLM. In addition, the OFA framework [Zhou et al., 2023],\nutilizing the frozen pretrained transformer framework, validates the ersatility and effectiveness of\npre-trained models in time series analysis. Another innovative approach is PromptCast [Xue and\nSalim, 2023], which employs a prompt-based learning method, transforming numerical input and\noutput data into prompts for effective forecasting in zero-shot settings. The TEMPO [Cao et al., 2023]\nadapts to changes in time series distribution by decomposing time series and adding different prompts\nfor each component and obtain competitive performance in time series forecasting. In specialized\ndomains like traffic [Xue et al., 2022], finance [Zhang et al., 2023] and healthcare [Liu et al., 2023b],\nLLMs have also shown unique advantages. In this work, we aim to enhance OOD robusness for time\nseries by fine-tuning LLMs with TTSO.\n3\nProblem Formulation and Algorithm\nNotations. X and Y represent the input and target spaces of samples, respectively. The predictor\nf\u03c6 = h\u03c9 \u25e6r\u03b8 consists of the representation function r\u03b8(\u00b7) with parameter \u03b8 and the classifier h\u03c9 with\nparameter \u03c9. The function f\u03c6 : X \u2192Y maps time series X \u2208X to Y \u2208Y, where X \u2208RT \u00d7F\nand Y \u2208R+, with T as the time series length and F as the feature dimensions. The multivariate\ntime series X, composed of F univariate time series each with T observations, is sampled i.i.d.\nfrom distribution P and represented as X = [x1, x2, . . . , xF ], where xi = [xv1, xv2, . . . , xT ] for\ni = 1, . . . , F. Assume source domain distributions are PSi for i \u2208{1, 2, . . . , K} and the target\ndomain distribution is PT. The source domain data DSi is sampled i.i.d. from PSi, and the target\ndomain data DT is sampled i.i.d. from PT.\n3.1\nPreliminary\nGiven a training dataset Dtrain = {(Xi, Yi)}N\ni=1 sampled from the distribution Ptrain(X, Y ). In\nsupervised learning, the goal is to learn an optimal predictor f\u03c6\u2217on Dtrain such that f\u03c6\u2217gener-\nalizes well on a test dataset Dtest sampled from the distribution Ptest(X, Y ). In self-supervised\ncontrastive learning, for a given time series X, we generate two augmented views, Xa1 and Xa2,\nusing augmentation methods a1, a2 \u2208A. These augmentations produce the time series represen-\ntations R1 = r\u03b8 (a1(X)) \u2208RT \u00d7M and R2 = r\u03b8 (a2(X)) \u2208RT \u00d7M, through the representation\nfunction r\u03b8. The objective of contrastive learning is to minimize the distance between positive pairs\n(R1, R2) while maximizing the distance between positive and negative pairs. The general formula\nfor contrastive loss, as detailed in Zhao et al. [2022], is formulated as follows\n\u2113con = \u2113align (r\u03b8; P, \u03c0) + \u03bb\u2113reg (r\u03b8; P, \u03c0).\n(1)\nThe first term aims to minimize the distance between positive pairs in the latent space, while the\nsecond term served as a regularizer prevents representation collapse. To evaluate the performance of\nthe model, a classifier h\u03c9 is trained using the representation functionr\u03b8\u2217\nh\u03c9\u2217= arg minh\u03c9 E(X,Y )\u223cPtrain\u2113sup (h\u03c9 \u25e6r\u03b8\u2217(X), Y ) ,\n(2)\nwhere the representation function r\u03b8\u2217(\u00b7) is optimized via the supervised loss in Eq. (2). The\nclassification is performed using f\u03c6\u2217= h\u03c9\u2217\u25e6r\u03b8\u2217. However, the discrepancy between the training\ndistribution Ptrain(X, Y ) and the test distribution Ptest(X, Y ) poses a challenge for generalizing f\u03c6\u2217\nto test data. Directly optimizing \u2113sup (f\u03c6(X), Y ) may lead to overfitting, compromising performance\non unseen data. To mitigate this issue, invariant representation learning [Arjovsky et al., 2019] is\nemployed to handle distribution shifts by learning robust invariant representations across diverse\ndistributions. To achieve this, we begin with the following assumption.\n3\nAssumption 1 (Invariant Assumption [Zhao et al., 2022]). Considering K different environments\n(domains) E, there exists a random variable \u03c8(X) such that for any e, e\u2032 \u2208supp(E), it holds that\nP (Y | \u03c8 (Xe)) = P (Y | \u03c8 (Xe\u2032)).\nThis assumption implies that for time series X observed in different environments, invariant rationales\nexist and their relationship with the corresponding labels remains stable. This stability ensures that\npredictions remain consistent across various environments, relying on these rationales. Assuming\n\u03c8(\u00b7) represents the representation function r\u03b8(\u00b7) parameterized by \u03b8, then it follows that\nr\u03b8\u2217(Xe) = r\u03b8\u2217(Xe\u2032) = \u03c8(X).\n(3)\nIn contrastive representation learning, where labels are not available, the theoretical analysis of the\ndownstream performance is challenging. To address this, research [Zhao et al., 2022] bridges this gap\nby connects contrastive loss to downstream risks,\nR(h\u03c9\u25e6r\u03b8;P\u03c0)\u2264c\u2225h\u03c9\u2225\n\u221a\nK\u03c3(\u2113align(r\u03b8;P,\u03c0))\n1\n4 +\u2225h\u03c9\u2225\u03c4(\u03c3,\u03b4)+P\nkP\u03c0(Ck)\u2225ek\u2212h\u03c9\u25e6\u00b5k(r\u03b8;P\u03c0)\u2225\n(4)\nwhere c is a positive constant, \u03c4(\u03c3, \u03b4) refers to a set of constants determined by the (\u03c3, \u03b4)-\naugmentation, and Ck corresponds to the sample set for class k. The first term is optimized during\ncontrastive pre-training. The second term depends on data augmentations (\u03c3, \u03b4). The third term,\nrelated to the linear layer h\u03c9, is optimized in downstream tasks. As shown by Eq. (4), contrastive\nlearning on distribution P with augmentation function \u03c0 essentially optimizes the upper bound of the\nsupervised risk.\nEach environment E corresponds to a domain distribution PSi. To learn an invariant representation\nover the domain set P, we first provide a mathematical definition of invariant risk minimization.\nDefinition 1 (Invariant Risk Minimization [Arjovsky et al., 2019]). If there exists a classifier h0 that is\noptimal for all domains in P, i.e., h0 \u2208argminh R (h \u25e6r\u03b8; PSi) , \u2200PSi \u2208P, then the representation\nfunction r\u03b8 elicits an invariant predictor h0 \u25e6r\u03b8 across the domain set P.\nThis definition is equivalent to learning features that have a stable association with the target variable,\nwhich has been theoretically and empirically proven to improve the transferability of supervised\nlearning across different distributions [Arjovsky et al., 2019, Zhao et al., 2022].\n3.2\nA Tri-level Learning Framework\nTo address OOD challenges, GroupDRO [Sagawa et al., 2019] propose a mimax formulation to\nminimizes the maximum domain supervised loss to enhance robustness against unseen data. Accord-\ning to Eq. (4), contrastive learning optimizes the upper bound of supervised risk. Thus, we extend\nGroupDRO by replacing the supervised loss with a self-supervised contrastive loss, aiming to learn\ninvariant representations. We further impose constraints on the group distribution q to mitigate the\nrisk of overfitting to specific domains. This results in a bi-level optimization problem\nmin\n\u03b8,q\nPK\ni=1 qi\u2113con(r\u03b8; DSi, \u03c0)\ns.t.\nq = arg max\nq\u2032\u2208\u2206K\nPK\ni=1 q\u2032\ni\u2113con(r\u03b8; DSi, \u03c0)\ns.t. d(p, q\u2032) \u2264\u03c4,\n(5)\nwhere d (\u00b7, \u00b7) denotes a distribution distance metric, such as KL divergence, Wasserstein distance,\nor Euclidean distance, \u2206K is a probability simplex, and \u03c4 is a constant. Following previous work\n[Qian et al., 2019], we adopt the Euclidean distance due to its strong convexity, which reults in\nfaster convergence [Rakhlin et al., 2012]. The outer optimization seeks the best parameters across all\ndomains to optimize overall performance, while the inner optimization, representing the group-level\nuncertainty, optimizes the worst-case distribution to enhance representation robustness.\nDefinition 2 (Augmentation Robust Alignment Loss [Zhao et al., 2022]). For any two augmentation\nmethods a, a\u2032 \u2208A, the robust alignment loss is defined as follows\n\u2113ar(r\u03b8; P) := EX\u223cP sup(a,a\u2032)\u2208A \u2225r\u03b8(a(X)) \u2212r\u03b8 (a\u2032(X))\u22252 .\n(6)\nTheorem 1 (Upper Bound of Risk Gap Between Augmented Domains [Shen et al., 2021]). For any\ntwo augmentation methods a, a\u2032 \u2208A, representation function r\u03b8 and classifier h\u03c9, we have\nsupa,a\u2032\u2208A \u2225R (h\u03c9 \u25e6r\u03b8; Pa) \u2212R (h\u03c9 \u25e6r\u03b8; Pa\u2032)\u2225\u2264c\u2225h\u03c9\u2225\u2113ar (r\u03b8; Ptrain ) .\n(7)\n4\n(1) sample-level\n(2) group-level\n(3) both\ntrain data (class 0)\nclassifier\nsample distribution\ntrain data (class 1)\nFigure 1: The depiction of sample-level, group-level, and combined uncertainties.\nFix r\u03b8, let ha = arg minh\u03c9R (h\u03c9 \u25e6r; Ptrain ), we have\n\u2225R (ha \u25e6r\u03b8; Pa\u2032) \u2212R (ha\u2032 \u25e6r\u03b8; Pa\u2032)\u2225\u22642c (\u2225ha\u2225+ \u2225ha\u2032\u2225) \u2113ar (r\u03b8; Ptrain ) .\n(8)\nTheorem 1 states that minimizing \u2113ar (r\u03b8; Ptrain) makes the optimal predictor more consistent across\ndifferent augmentation domains, i.e., minimize \u2113ar(r\u03b8; Ptrain) can enhance the invariance of the\nlearned representation. Nonetheless, evaluating \u2113ar (r; Ptrain) involves a supremum operator, and the\nlarge set A makes accurate computation infeasible. Therefore, we propose an approximation for\n\u2113ar (r\u03b8; Ptrain). We start with the following reasonable assumption.\nAssumption 2. For any pair of augmentation methods a, a\u2032 \u2208A, they can be viewed as introducing\nspecific perturbations \u03b4 to the sample X, i.e., a(X) = X + \u03b4a, a\u2032(X) = X + \u03b4a\u2032.\nSuppose \u03b4a and \u03b4a\u2032 are sampled from Pperb, representing the distribution of perturbations induced\nby augmentation techniques. We adopt a Gaussian Mixture Model (GMM) [Jiao et al., 2022b] to\naccurately characterize the uncertain perturbation distribution. Thus, the distribution of \u03b4 is given by\np(\u03b4; \u03c0, \u00b5, \u03c3) = PM\nm=1 \u03c0mN\n\u0000\u03b4; \u00b5m, \u03c32\nm\n\u0001\n,\n(9)\nwhere \u03c0m represents the weight of the mth component in the mixture, and PM\nm=1 \u03c0m = 1. The ex-\npression for \u2113ar (r\u03b8; Ptrain) can be written as sup\u03b4\u223cp(\u03b4;\u03c0,\u00b5,\u03c3)\nPK\ni=1 qi\u2113align(\u03b8, \u03b4; DSi). Consequently,\nwe can further extend problem (5) to the following tri-level optimization problem.\nmin\n\u03b8,q,\u03b4\nPK\ni=1 qi\u2113con (\u03b8, \u03b4; DSi)\ns.t.\nq = arg max\nq\u2032\u2208\u2206K\nPK\ni=1 q\u2032\ni\u2113con (\u03b8, \u03b4; DSi)\ns.t. d(p, q\u2032) \u2264\u03c4\n\u03b4 =\narg max\n\u03b4\u2032\u223cp(\u03b4\u2032;\u03c0,\u00b5,\u03c3)\nPK\ni=1 q\u2032\ni\u2113align(\u03b8, \u03b4\u2032; DSi)\ns.t. \u2225\u00b5\u2225\u2264C1, \u2225\u03c3\u2225\u2264C2, PM\nm=1 \u03c0m = 1, \u03c0m \u22650,\n(10)\nwhere C1 and C2 are constants. The third-level optimization addresses sample-level uncertainties by\nmaximizing the alignment loss under the worst-case perturbation distribution. Figure 1 illustrates\nthese concepts, showing how group-level and sample-level optimizations interact within the tri-level\nframework. To theoretically justify our approach in Eq. (10), we present the following theorem.\nTheorem 2 (Upper Bound on Target Error). Given the previous setup, let H be a hypothe-\nsis space of Vapnik-Chervonenkis (VC) Dimension d and h\u2217\nT = minh\u2208H \u03f5T (h).\nLet P\u03b1 =\n{P\u03b1 | P\u03b1 = P\ni \u03b1iPSi, P\ni \u03b1i = 1, \u03b1i \u22650 }. If \u02c6h \u2208H is the empirical minimizer on P\u03b1, then\nfor any \u03b4 and PC \u2208P\u03b1, with probability at least 1 \u2212\u03b4,\n\u03f5T (\u02c6h) \u22643\u03f5T (h\u2217\nT ) + \u03bb + dH\u2206H (PC, PT) + maxi,j dH\u2206H(PSi, PSj) + C(\u03b4, m, d),\n(11)\nwhere \u03bb = 2 PK\ni=1 \u03b1i\u03f5Si(h\u2217) and C(\u03b4, m, d) is a statistical term. dH\u2206H(\u00b7, \u00b7) is a metric function\nwhich measures differences in distribution [Ben-David et al., 2010]. \u03f5Si(h) and \u03f5T (h) is the source\nerror and the target error.\nDiscussion: Theorem 2 provides a theoretical framework for estimating performance on a new target\ndistribution. The TTSO framework in Eq. (10) focuses on minimizing the terms dH\u2206H (PC, PT) and\nmaxi,j dH\u2206H(PSi, PSj), thereby giving a tighter bound of target error to improve generalization\nability. Proof of Theorem 2 and further discussion of our motivation are in Appendix A.2.\n5\nAlgorithm 1 SLA: Stratified Localization Algorithm\nInput: Training datasets {DSi}, learning rates \u03b7\u03b8, \u03b7q, \u03b7\u03b4, number of iterations T.\nOutput: Optimized parameters \u03b8\u2217\n1: Initialize parameters \u03b8, q, \u03b4 and initial set of cutting planes S0.\n2: for t = 0 to T \u22121 do\n3:\nUpdate variable \u03b8(t+1), q(t+1) and \u03b4(t+1) according to Eq. (19), (20) and (21)\n4:\nif t mod k == 0 then\n5:\nif h(\u03b8(t+1), q(t+1), \u03b4(t+1)) > \u03b5 then\n6:\nAdd new cutting planes to set St+1 according to Eq. (22)\n7:\nend if\n8:\nend if\n9: end for\n10: return \u03b8(T )\nHowever, solving the constrained tri-level optimization is extremely challenging. In the next subsec-\ntion, we introduce a stratified localization algorithm to address this problem effectively.\n3.3\nStratified Localization Algorithm\nDue to the hierarchical structure of the tri-level problem, we develop a stratified version of the\nlocalization method [Boyd and Vandenberghe, 2007, Jiao et al., 2023] to tackle the problem presented\nin Eq. (10). First, we use exterior penalty method to reformulate the third level problem, the resulting\nproblem is\nmin\n\u03b8,q,\u03b4\nPK\ni=1 qi\u2113con (\u03b8, \u03b4; DSi)\ns.t.\nq = arg max\nq\u2032\u2208\u2206K\nPK\ni=1 q\u2032\ni\u2113con (\u03b8, \u03b4; DSi)\ns.t. d (p, q\u2032) \u2264\u03c4\n\u03b4 =\narg max\n\u03b4\u2032\u223cp(\u03b4\u2032;\u03c0,\u00b5,\u03c3)\nPK\ni=1 q\u2032\ni\u2113align\n\u0000\u03b8, \u03b4\u2032; DSi\n\u0001\n\u2212P3,\n(12)\nwhere P3 is a penalty term defined as P3 = \u03c11(max(0, \u2225\u00b5\u2225\u2212C1))2 + \u03c12(max(0, \u2225\u03c3\u2225\u2212C2))2 +\n\u03c13(PM\nm=1 \u03c0m \u22121)2 + \u03c14(max(0, \u2212\u03c0m))2, and \u03c1i are penalty coefficients.\nGiven that the third-level optimization is a constraint for the second-level optimization, we em-\nploy T3 steps of gradient ascent to approximate the third-level problem. This technique is com-\nmonly used in previous bi-level optimization studies [Ji et al., 2021]. By defining f3(\u03b8, q\u2032, \u03b4\u2032) =\nPK\ni=1 q\u2032\ni(\u2113align (\u03b8, \u03b4\u2032; DSi) \u2212P3 and using the exterior penalty method, the resulting optimization\nproblem can be expressed as\nmin\n\u03b8,q,\u03b4\nPK\ni=1 qi\u2113con (\u03b8, \u03b4; DSi)\ns.t.\nq = arg max\nq\u2032\u2208\u2206K\nPK\ni=1 q\u2032\ni\u2113con (\u03b8, \u03b4; DSi) \u2212P2,\n(13)\nwhere P2 = \u03bb1(PK\ni=1 qi\u22121)2+PK\ni=1 \u03bb2 max(0, \u2212qi)+\u03bb3\u2225\u03b4\u2212\u03b4(0)\u2212PT3\u22121\ni=0\n\u03b7\u03b4\u2207\u03b4\u2032f3(\u03b8, q\u2032, \u03b4\u2032)\u22252.\nLikewise, we perform T2 steps of gradient ascent to replace the second level optimization problem.\nWith the definition of f2(\u03b8, q\u2032, \u03b4) = PK\ni=1 q\u2032\ni\u2113con\n\u0000\u03b8, \u03b4; Dtr\nSi\n\u0001\n\u2212P2, \u03c6(\u03b8, \u03b4) = arg maxq\u2032f2(\u03b8, q\u2032, \u03b4)\nand h(\u03b8, q, \u03b4) = \u2225q \u2212\u03c6(\u03b8, \u03b4)\u2225, Eq. (13) can be reformulated as follows\nmin\n\u03b8,q,\u03b4\nPK\ni=1 qi\u2113con (\u03b8, \u03b4; DSi)\ns.t.\nh(\u03b8, q, \u03b4) = 0.\n(14)\nLet f1 (\u03b8, q, \u03b4) = PK\ni=1 qi\u2113align (\u03b8, \u03b4; DSi). Considering the approximations of q and \u03b4, the above\nproblem can be relaxed as\nmin\n\u03b8,q,\u03b4\nf1 (\u03b8, q, \u03b4)\ns.t.\nh(\u03b8, q, \u03b4) \u2264\u03b5,\n(15)\nwhere \u03b5 > 0 is a constant. Inspired by the polyhedral approximation method [B\u00fcrger et al., 2013],\nwe utilize cutting planes to approximate the feasible region with respect to h(\u03b8, q, \u03b4) \u2264\u03b5. In the\n6\n(t + 1)th iteration, the set of cutting planes, denoted as St, is defined as follows\nSt = {a\u22a4\ni \u03b8 + b\u22a4\ni q + c\u22a4\ni \u03b4 + di \u22640, i = 1, \u00b7 \u00b7 \u00b7 , |St|},\n(16)\nwhere ai \u2208RN, bi \u2208RM, ci \u2208RH, di \u2208R1, and |St| represents the number of cutting planes in\nSt. Then Eq. (15) can be expressed as the following approximation problem\nmin\n\u03b8,q,\u03b4\nf1 (\u03b8, q, \u03b4)\ns.t.\na\u22a4\ni \u03b8 + b\u22a4\ni q + c\u22a4\ni \u03b4 + di \u22640,\ni = 1, \u00b7 \u00b7 \u00b7 , |St|.\n(17)\nThe penalty function with respect to Eq. (17) can be described as\nF (\u03b8, q, \u03b4) = f1 (\u03b8, q, \u03b4) + P\ni \u03bbi max(0, a\u22a4\ni \u03b8 + b\u22a4\ni q + c\u22a4\ni \u03b4 + di)2.\n(18)\nIn (t + 1)th iteration, the variables are updated as follows\n\u03b8t+1 = \u03b8t \u2212\u03b7\u03b8\u2207\u03b8F(\u03b8t, qt, \u03b4t),\n(19)\nqt+1 = qt \u2212\u03b7q\u2207qF(\u03b8t, qt, \u03b4t),\n(20)\n\u03b4t+1 = \u03b4t \u2212\u03b7\u03b4\u2207\u03b4F(\u03b8t, qt, \u03b4t).\n(21)\nThroughout the iteration process, the set of cutting planes St is updated every k iterations for a\ntighter and more accurate polyhedral approximation. Before adding new cutting planes, we first\ncheck whether (\u03b8t+1, qt+1, \u03b4t+1) is a solution for Eq. (15). If it is not a feasible solution to Eq.\n(15), i.e., h(\u03b8, q, \u03b4) > \u03b5, new cutting planes are added to St based on Theorem 3 and Proposition 22.\nAlgorithm 1 provides details of the proposed method.\nTheorem 3. Let T2 = 1. If a first-order Taylor expansion is applied to the function f2(\u03b8, q, \u03b4) at the\npoint (\u03b8, \u03b4), it follows that the function h (\u03b8, q, \u03b4) is convex with respect to (\u03b8, q, \u03b4). The detailed\nproof can be found in Appendix A.3.\nProposition 1. Given the convexity of the function h(\u03b8, q, \u03b4), a new cutting plane is generated when\nthe condition h(\u03b8, q, \u03b4) > \u03b5 is not met. This cutting plane is formally expressed as\nh(\u03b8t+1, qt+1, \u03b4t+1) +\n\uf8ee\n\uf8f0\n\u2207\u03b8h(\u03b8t+1, qt+1, \u03b4t+1)\n\u2207qh(\u03b8t+1, qt+1, \u03b4t+1)\n\u2207\u03b4h(\u03b8t+1, qt+1, \u03b4t+1)\n\uf8f9\n\uf8fb\n\u22a4\uf8ee\n\uf8f0\n\u03b8 \u2212\u03b8t+1\nq \u2212qt+1\n\u03b4 \u2212\u03b4t+1\n\uf8f9\n\uf8fb\u2264\u03b5.\n(22)\nFor the detailed derivation and proof of Proposition 22, please see Appendix A.1.\n3.4\nTTSO for Fine-tuning LLMs\nLLMs have garnered considerable attention in time series applications [Jin et al., 2023, Zhou et al.,\n2023]. The emergent abilities of LLMs, especially in OOD scenarios, largely depends on the\nrobustness of their representations[Wang et al., 2023, Chu et al., 2023]. This section connects the\nestablished theoretical foundation with the practical application of fine-tuning LLMs for time series\nOOD generalization. We adapt TTSO framework for fine-tuning LLMs to enhance the performance\nin time series OOD generalization. Our proposed method involves a dual-stage fine-tuning method\ntailored for time series. The main process of fine-tuning are described below.\nTime Series Pre-processing. Preprocessing starts with an input projection layer to bridge the gap\nin dimensions between raw time series data and the LLM\u2019s native embedding dimension. This step\nis crucial for the LLM\u2019s effective integration of time series. Following this, positional encoding is\napplied to preserve the sequential integrity of the time series.\nDual-stage Fine-tuning Method. In the first stage, we employ TTSO framework to fine-tune LLMs,\nin line with the previously mentioned tri-level optimization framework as illustrate in Eq. (10). We\nadopt the contrastive loss function designed for time series from Yue et al. [2022]. In the second stage,\nthe learned weights of the LLM, including the projection layer, are transferred to the downstream\nfine-tuning stage for time series classification. To retain the knowledge learned by the LLM from the\ncorpus, we follow Chang et al. [2023], Zhou et al. [2023] by fixing the weights of the fully connected\nand attention layers, using Layer Normalization Tuning [Lu et al., 2022a] to adjust only the layer\nnormalization parameters, making the affine transformation trainable.\nConstrained Optimization for Fine-Tuning. Research [Wortsman et al., 2022] indicates that\nadopting radical strategies for fine-tuning models, such as larger learning rates, can reduce out-of-\n7\ndistribution robustness. Unconstrained optimization of model parameters during fine-tuning can\nlead to knowledge forgetting issues and decrease the model\u2019s generalization ability, as mentioned in\nXuhong et al. [2018]. Therefore, during fine-tuning for downstream tasks, we impose constraints on\nthe parameters, following Xuhong et al. [2018], resulting in the following optimization problem\nmin\n\u03b8\n\u2113cls (r\u03b8 \u25e6h\u03c9; D)\ns.t.\n\u2225\u03b8 \u2212\u03b80\u2225\u2264\u03b3,\n(23)\nwhere \u03b80 and \u03b8 respectively denote the weights from the first and second fine-tuning phases of the\nLLMs. More details of fine-tuning LLMs can be found in appendix D.\n4\nConvergence Analysis\nAssumption 3 (Lipschitz Continuity of Gradient). Assume that the gradient of the function F is\nL-Lipschitz continuous gradient, i.e., for any x, y, there exists L > 0 such that:\n\u2225\u2207F(x) \u2212\u2207F(y)\u2225\u2264L \u2225x \u2212y\u2225.\n(24)\nAssumption 4 (Unbiasedness and Variance Bound of Stochastic Gradients). Assume for the\nstochastic gradients g\u03b8, gq, g\u03b4, the following conditions are satisfied\nE\u03b6t\nj[g\u03b8(\u03b8t, qt, \u03b4t; \u03b6t\nj) \u2212\u2207\u03b8F(\u03b8t, qt, \u03b4t)] = 0,\nE\u03b6t\nj[gq(\u03b8t, qt, \u03b4t; \u03b6t\nj) \u2212\u2207qF(\u03b8t, qt, \u03b4t)] = 0,\nE\u03b6t\nj[g\u03b4(\u03b8t, qt, \u03b4t; \u03b6t\nj) \u2212\u2207\u03b4F(\u03b8t, qt, \u03b4t)] = 0,\nE\u03b6t\nj[\u2225g\u03b8(\u03b8t, qt, \u03b4t; \u03b6t\nj) \u2212\u2207\u03b8F(\u03b8t, qt, \u03b4t)\u22252] \u2264\u03c32\n1,\nE\u03b6t\nj[\u2225gq(\u03b8t, qt, \u03b4t; \u03b6t\nj) \u2212\u2207qF(\u03b8t, qt, \u03b4t)\u22252] \u2264\u03c32\n2,\nE\u03b6t\nj[\u2225g\u03b4(\u03b8t, qt, \u03b4t; \u03b6t\nj) \u2212\u2207\u03b4F(\u03b8t, qt, \u03b4t)\u22252] \u2264\u03c32\n3,\n(25)\nwhere E\u03b6t\nj[\u00b7] denotes the expectation over the \u03b6t\nj.\nAssumption 5 (Bounded Gradient). Assume that the gradient of the function F is bounded, i.e.,\n\u2200t, \u2225\u2207\u03b8F(\u03b8t, qt, \u03b4t)\u22252 \u2264\u03b12\n1, \u2225\u2207qF(\u03b8t, qt, \u03b4t)\u22252 \u2264\u03b12\n2, \u2225\u2207\u03b4F(\u03b8t, qt, \u03b4t)\u22252 \u2264\u03b12\n3.\nDefinition 3 (\u03f5-Stationary Point). Following Xu et al. [2023], Jiao et al. [2024], a point (\u03b8, q, \u03b4) is\nconsidered an \u03f5-stationary point (where \u03f5 > 0) of a differentiable function F if the sum of squares of\nits gradients on these variables satisfies \u2225\u2207Gt\u2225\u2264\u03f5. Let T(\u03f5) be the index of the first iteration that\nsatisfies \u2225\u2207Gt\u2225\u2264\u03f5, i.e., T(\u03f5) = min{t | \u2225\u2207Gt\u2225\u2264\u03f5, t > t1}.\nTheorem 4 (Convergence Guarantee). With the continuous addition of cutting planes, the optimal\nobjective value of the approximated problem, delineated in Eq. (17), is guaranteed to converge\nmonotonically. For further details, see the proof of Theorem 4 in appendix A.4.\nTheorem 5 (Convergence Rate). Under the assumptions 3, 4, and 5, by setting the step-sizes as\n\u03b7\u03b8 = \u03b7q = \u03b7\u03b4 =\n1\n\u221aT1\u2212t1 and the batch size as B, for a given \u03f5, it follows that\nT(\u03f5) \u223cO\n\u0012\nt1 + L2(m(\u03b12\n1 + \u03b12\n2 + \u03b12\n3) + \u03c32\n1 + \u03c32\n2 + \u03c32\n3)2\n4m2(\u03f5 \u2212F(\u03b8T1, qT1, \u03b4T1) + F \u2217)2\n\u0013\n,\n(26)\nwhere F \u2217represents the lower bound of F. The proof of Theorem 5 is detailed in appendix A.5.\n5\nExperiment\nTo evaluate the proposed TTSO framework, we conduct experiments on 6 real-world time se-\nries datasets using the leave-one-domain-out setting, including HHAR [Blunck et al., 2015],\nPAMAP [Reiss, 2012], WESAD [Philip Schmidt et al., 2018], SWELL [Koldijk et al., 2014],\nUSC-HAD[Zhang and Sawchuk, 2012] and DSADS [Barshan and Altun, 2013]. We compare with\nbaseline method ERM [Vapnik, 1991] and 8 general OOD generalization methods: IRM [Arjovsky\net al., 2019], GroupDRO [Sagawa et al., 2019], ANDMask [Parascandolo et al., 2020], RSC [Huang\net al., 2020], Mixup [Zhang et al., 2017], VERx [Krueger et al., 2021], DIFEX[Lu et al., 2022b].\nAnd we further compare with 2 recent strong approach in time series: AdaRNN[Du et al., 2021]\nand GILE [Qian et al., 2021]. We also include DIVERSIFY[Lu et al., 2023], DFDG[Zhang et al.,\n2021], and CCDG[Ragab et al., 2022], three methods specifically designed for time series OOD\n8\ngeneralization. To guarantee a fair comparison, we implement all methods using the same backbone\narchitecture (except AdaRNN and GILE), TCN [Bai et al., 2018], a model widely used in time series\nanalysis. In addition, we fine-tune the pre-trained Large Language Model, GPT2[Radford et al.,\n2018], within our TTSO framework to harness its sophisticated representation learning capabilities.\nDetailed information regarding datasets, domain setting, data pre-processing, network architecture\nand hyperparameters are provided in Appendix B.1, B.2, B.3 and C.\n5.1\nMain Results\nWe report the average results over 3 runs for each dataset, along with the standard deviation. The\nresults for the HHAR, PAMAP, and WESAD datasets are shown in Tables 1, where our method\noutperforms the second-best baseline by 2.8%, 4.8%, and 4.9% respectively. Additional results are\nprovided in Appendix E.1 (Table 4). These results demonstrate the superiority and effectiveness\nof the TTSO framework, as it accounts for both sample-level and group-level uncertainties, which\noptimizes the upper bound in Theorem 2.\nCompared to traditional methods like ERM, IRM, and GroupDRO, both TTSO and TTSO\u2217show more\nconsistent and generally superior performance in OOD generalization, highlighting the advantages of\na tri-level learning framework. The TTSO\u2217method, which incorporates LLM fine-tuning, consistently\noutperforms other approaches, demonstrating the effectiveness of LLM with TTSO fine-tuning in\nenhancing OOD generalization for time series. Concurrently, the TTSO method, even without LLM\nfine-tuning, shows strong generalization performance, especially on the HHAR dataset where it\nclosely matches TTSO\u2217results. This indicates that the TTSO framework is highly effective in\ngeneralizing across different scenarios, even in the absence of LLM.\nTable 1: Classification accuracy (%) on HHAR, PAMAP, and WESAD datasets. Bold indicates the\nbest, underline the second-best performance. Standard deviation is shown in the lower right corner.\nHHAR\nPAMAP\nWESAD\nALL\nMethod\nA\nB\nC\nD\nAVG\nA\nB\nC\nD\nAVG\nA\nB\nC\nD\nAVG\nAVG\nADARNN\n73.00.01\n65.00.04\n76.80.02\n67.00.00\n70.5\n71.80.01\n72.40.01\n53.80.01\n74.20.04\n68.0\n40.80.02\n72.20.00\n63.40.02\n47.80.03\n56.0\n64.8\nGILE\n65.30.01\n61.50.01\n79.20.00\n58.40.03\n66.1\n70.10.02\n74.50.00\n45.60.02\n66.00.01\n64.1\n42.20.01\n72.70.02\n70.50.00\n46.90.02\n58.1\n62.8\nERM\n71.60.01\n66.40.01\n78.30.02\n68.60.01\n71.2\n72.10.01\n81.80.00\n58.90.00\n68.70.01\n70.4\n44.50.02\n71.40.01\n65.80.02\n48.00.01\n57.5\n66.4\nIRM\n72.80.05\n63.90.01\n79.20.01\n68.10.00\n71.0\n71.40.02\n83.10.02\n58.80.04\n71.10.02\n71.1\n45.10.01\n71.80.01\n67.00.02\n45.50.01\n57.4\n66.5\nGroupDRO\n71.10.01\n66.10.01\n75.20.01\n67.50.02\n70.0\n71.30.01\n80.70.00\n57.90.01\n70.30.02\n70.0\n50.70.01\n70.20.02\n62.30.00\n53.70.02\n59.2\n66.4\nANDMask\n73.00.03\n62.30.01\n81.40.03\n68.90.00\n71.4\n74.00.01\n81.00.03\n55.30.02\n72.30.01\n70.6\n44.20.02\n70.00.03\n61.70.00\n47.60.02\n55.9\n66.0\nRSC\n76.30.05\n63.90.01\n78.40.05\n64.30.01\n70.7\n74.60.02\n84.30.01\n58.90.01\n72.50.01\n72.6\n56.90.02\n70.80.03\n67.20.01\n57.40.02\n63.1\n68.8\nMixup\n73.10.02\n65.90.01\n79.10.02\n69.50.00\n71.9\n73.60.01\n87.30.00\n59.30.00\n69.90.01\n72.5\n52.90.01\n70.00.02\n73.60.02\n64.40.01\n65.2\n69.9\nVERx\n67.50.01\n63.20.02\n81.20.03\n68.00.00\n70.0\n74.20.00\n85.50.00\n59.50.00\n70.00.00\n72.3\n58.30.02\n72.00.00\n66.30.02\n50.50.02\n61.8\n68.0\nDIFEX\n71.50.02\n62.00.01\n81.50.02\n65.50.01\n70.1\n73.60.01\n84.10.01\n59.20.01\n71.40.01\n72.1\n48.20.01\n71.70.02\n63.20.01\n51.30.01\n58.6\n66.9\nDFDG\n71.20.01\n65.80.00\n74.10.02\n70.40.00\n70.3\n73.10.01\n80.50.03\n59.20.02\n70.20.01\n70.8\n49.80.03\n71.60.01\n71.10.00\n50.70.02\n60.8\n67.3\nCCDG\n73.00.02\n63.20.00\n77.30.00\n72.40.00\n71.5\n72.30.01\n84.80.00\n56.60.01\n72.10.00\n71.5\n54.50.03\n70.50.01\n69.80.03\n54.10.02\n62.2\n68.4\nDIVERSIFY\n73.70.01\n64.20.01\n78.90.01\n71.20.01\n71.8\n74.00.02\n84.00.03\n56.50.00\n72.90.03\n72.0\n57.60.02\n73.00.00\n72.60.01\n57.10.02\n64.6\n69.3\nTTSO\n76.60.00\n67.50.00\n80.20.01\n68.10.01\n73.1\n75.30.01\n86.10.01\n60.50.01\n72.50.01\n73.6\n59.90.01\n71.30.01\n76.20.00\n63.00.01\n67.6\n71.4\nTTSO\u2217\n77.60.02\n67.30.01\n80.60.00\n69.90.01\n73.9\n78.50.02\n89.60.00\n61.40.01\n75.00.01\n76.1\n59.50.00\n71.90.03\n77.30.01\n65.00.00\n68.4\n72.8\n5.2\nAblation Study\nHHAR\nPAMAP\nWESAD\n62\n64\n66\n68\n70\n72\n74\n76\n78\nAVG. Accuracy (%)\nTTSO+ +\nTTSO+\nTTSO\n+\nTTSO\nFigure 2: Ablation study of\nTTSO\u2217\nThis ablation study is conducted to further understand the impact\nof our TTSO framework\u2019s fine-tuning on model performance. We\ncompare four distinct variants: a pretrained GPT2 fine-tuned with\nTTSO (TTSO++), a pretrained GPT2 without TTSO fine-tuning\n(TTSO+\u2212), a randomly initialized GPT2 fine-tuned with TTSO\n(TTSO\u2212+), and a randomly initialized GPT2 without TTSO fine-\ntuning (TTSO\u2212\u2212). This comparison helps in quantifying the effec-\ntiveness of the TTSO fine-tuning strategy in enhancing the model\u2019s\nOOD generalization capabilities.\nThe ablation results are presented in Figure 2. From this results, we\ncan see that: (a) TTSO++ demonstrates the best performance in all\nscenarios, further validating that the combination of a pre-trained GPT2 model with TTSO fine-tuning\ncan significantly improve the model\u2019s OOD generalization capabilities. (b) Although TTSO+\u2212does\nnot employ TTSO fine-tuning, it still exhibits relatively good performance. This suggests that the\npre-trained GPT2 model has an intrinsic capacity for OOD generalization, consistent with previous\nempirical studies [Zheng et al., 2022, Hendrycks et al., 2020]. (c) Compared to TTSO\u2212\u2212, TTSO\u2212+\napplies TTSO to fine-tune on a randomly initialized GPT2, TTSO++ achieves improved performance.\n9\nThis demonstrates that even in the absence of a pre-trained model, TTSO fine-tuning can effectively\nenhance the model\u2019s OOD generalization capabilities, though not as significantly as that with a\npre-trained GPT2.\n6\nConclusion\nExisting OOD generalization methods mainly focus on sample-level uncertainties or group-level\nuncertainties, often overlooking the interplay between these two aspects. In light of this, we propose\nthe TTSO framework to integrate both sample-level and group-level uncertainties within a unified\ntri-level learning approach, thereby enhancing the model\u2019s robustness and adaptability in facing\ndiverse and unforeseen distribution shifts. In addition, this innovative framework introduces a\nfresh perspective for the development and analysis of the Out-of-Distribution (OOD) generalization\nproblem. Based on this formulation, we develop a stratified localization algorithm for the tri-level\noptimization problem and provide theoretical analysis regarding the iteration complexity of the\nproposed algorithm. Comprehensive studies have been carried out to assess the performance of\nthe proposed algorithm and substantiate the theoretic claims. It is seen that TTSO with LLM can\nconsiderably improves the performance of time series OOD generalization.\n7\nAcknowledgements\nThis work was supported in part by the National Natural Science Foundation of China under Grant\n12371519 and 61771013; in part by Asiainfo Technologies; in part by the Fundamental Research\nFunds for the Central Universities of China; and in part by the Fundamental Research Funds of\nShanghai Jiading District.\nReferences\nMartin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.\narXiv preprint arXiv:1907.02893, 2019.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and\nrecurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018.\nBillur Barshan and Kerem Altun. Daily and Sports Activities. UCI Machine Learning Repository,\n2013. DOI: https://doi.org/10.24432/C5C59F.\nShai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman\nVaughan. A theory of learning from different domains. Machine learning, 79:151\u2013175, 2010.\nHenrik Blunck, Sourav Bhattacharya, Thor Prentow, Mikkel Kjrgaard, and Anind Dey.\nHeterogeneity activity recognition.\nUCI Machine Learning Repository, 2015.\nDOI:\nhttps://doi.org/10.24432/C5689X.\nStephen Boyd and Lieven Vandenberghe. Localization and cutting-plane methods. From Stanford EE\n364b lecture notes, 386, 2007.\nStephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\nMathias B\u00fcrger, Giuseppe Notarstefano, and Frank Allg\u00f6wer. A polyhedral approximation framework\nfor convex and robust distributed optimization. IEEE Transactions on Automatic Control, 59(2):\n384\u2013395, 2013.\nDefu Cao, Furong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo:\nPrompt-based generative pre-trained transformer for time series forecasting.\narXiv preprint\narXiv:2310.04948, 2023.\nChing Chang, Wen-Chih Peng, and Tien-Fu Chen. llm4ts: Two-stage fine-tuning for time-series\nforecasting with pre-trained llms. arXiv preprint arXiv:2308.08469, 2023.\n10\nZhixuan Chu, Hongyan Hao, Xin Ouyang, Simeng Wang, Yan Wang, Yue Shen, Jinjie Gu, Qing Cui,\nLongfei Li, Siqiao Xue, et al. Leveraging large language models for pre-trained recommender\nsystems. arXiv preprint arXiv:2308.10837, 2023.\nLucas Deecke, Timothy Hospedales, and Hakan Bilen. Visual representation learning over latent\ndomains. In International Conference on Learning Representations, 2021.\nYuntao Du, Jindong Wang, Wenjie Feng, Sinno Pan, Tao Qin, Renjun Xu, and Chongjun Wang.\nAdarnn: Adaptive learning and forecasting of time series. In Proceedings of the 30th ACM\ninternational conference on information & knowledge management, pages 402\u2013411, 2021.\nDeepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio gener-\nation using instruction-tuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731,\n2023.\nJames D Hamilton. Time series analysis. Princeton university press, 2020.\nPengchao Han, Xingyan Shi, and Jianwei Huang. Fedal: Black-box federated knowledge distillation\nenabled by adversarial learning. IEEE Journal on Selected Areas in Communications, 2024.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song.\nPretrained transformers improve out-of-distribution robustness. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pages 2744\u20132751, 2020.\nZeyi Huang, Haohan Wang, Eric P Xing, and Dong Huang. Self-challenging improves cross-domain\ngeneralization. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August\n23\u201328, 2020, Proceedings, Part II 16, pages 124\u2013140. Springer, 2020.\nKaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced\ndesign. In International conference on machine learning, pages 4882\u20134892. PMLR, 2021.\nYang Jiao, Kai Yang, and Dongjin Song. Distributed distributionally robust optimization with\nnon-convex objectives. Advances in neural information processing systems, 35:7987\u20137999, 2022a.\nYang Jiao, Kai Yang, Dongjing Song, and Dacheng Tao. Timeautoad: Autonomous anomaly detection\nwith self-supervised contrastive loss for multivariate time series. IEEE Transactions on Network\nScience and Engineering, 9(3):1604\u20131619, 2022b.\nYang Jiao, Kai Yang, Tiancheng Wu, Dongjin Song, and Chengtao Jian. Asynchronous distributed\nbilevel optimization. In The Eleventh International Conference on Learning Representations, 2023.\nYang Jiao, Kai Yang, Tiancheng Wu, Chengtao Jian, and Jianwei Huang. Provably convergent\nfederated trilevel learning. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 38, pages 12928\u201312937, 2024.\nMing Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yux-\nuan Liang, Yuan-Fang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming\nlarge language models. arXiv preprint arXiv:2310.01728, 2023.\nSaskia Koldijk, Maya Sappelli, Suzan Verberne, Mark A Neerincx, and Wessel Kraaij. The swell\nknowledge work dataset for stress and user modeling research.\nIn Proceedings of the 16th\ninternational conference on multimodal interaction, pages 291\u2013298, 2014.\nDavid Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Dinghuai\nZhang, Remi Le Priol, and Aaron Courville. Out-of-distribution generalization via risk extrap-\nolation (rex). In International Conference on Machine Learning, pages 5815\u20135826. PMLR,\n2021.\nBo Liu, Liming Zhan, Zexin Lu, Yujie Feng, Lei Xue, and Xiao-Ming Wu. How good are large\nlanguage models at out-of-distribution detection? arXiv preprint arXiv:2308.10261, 2023a.\nXin Liu, Daniel McDuff, Geza Kovacs, Isaac Galatzer-Levy, Jacob Sunshine, Jiening Zhan, Ming-\nZher Poh, Shun Liao, Paolo Di Achille, and Shwetak Patel. Large language models are few-shot\nhealth learners. arXiv preprint arXiv:2305.15525, 2023b.\n11\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Pretrained transformers as universal\ncomputation engines. arXiv preprint arXiv:2103.05247, 1, 2021.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Frozen pretrained transformers as\nuniversal computation engines. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 36, pages 7628\u20137636, 2022a.\nWang Lu, Jindong Wang, Haoliang Li, Yiqiang Chen, and Xing Xie. Domain-invariant feature\nexploration for domain generalization. arXiv preprint arXiv:2207.12020, 2022b.\nWang Lu, Jindong Wang, Xinwei Sun, Yiqiang Chen, and Xing Xie. Out-of-distribution representation\nlearning for time series classification. In International Conference on Learning Representations,\n2023.\nSuvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas,\nKanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines.\narXiv preprint arXiv:2307.04721, 2023.\nGiambattista Parascandolo, Alexander Neitz, ANTONIO ORVIETO, Luigi Gresele, and Bernhard\nSch\u00f6lkopf. Learning explanations that are hard to vary. In International Conference on Learning\nRepresentations, 2020.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in neural information processing systems, 32,\n2019.\nA Philip Schmidt, R Duerichen Reiss, and Introducing WESAD Kristof Van Laerhoven. A multimodal\ndataset for wearable stress and affect detection. In Proceedings of the International Conference on\nMultimodal Interaction, 2018.\nHangwei Qian, Sinno Jialin Pan, and Chunyan Miao. Latent independent excitation for generalizable\nsensor-based cross-person activity recognition. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 11921\u201311929, 2021.\nQi Qian, Shenghuo Zhu, Jiasheng Tang, Rong Jin, Baigui Sun, and Hao Li. Robust optimization\nover multiple domains. In Proceedings of the AAAI Conference on Artificial Intelligence, pages\n4739\u20134746, 2019.\nJoaquin Qui\u00f1onero-Candela, Masashi Sugiyama, Neil D. Lawrence, and Anton Schwaighofer. Dataset\nshift in machine learning. MIT Press, 2009.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. OpenAI Blog, 2018.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021.\nMohamed Ragab, Zhenghua Chen, Wenyu Zhang, Emadeldeen Eldele, Min Wu, Chee-Keong Kwoh,\nand Xiaoli Li. Conditional contrastive domain generalization for fault diagnosis. IEEE Transactions\non Instrumentation and Measurement, 71:1\u201312, 2022. doi: 10.1109/TIM.2022.3154000.\nAlexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for\nstrongly convex stochastic optimization. In International Conference on Machine Learning, pages\n1571\u20131578, 2012.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers\ngeneralize to imagenet? In International conference on machine learning, pages 5389\u20135400.\nPMLR, 2019.\nAttila Reiss. PAMAP2 physical activity monitoring. UCI Machine Learning Repository, 2012. DOI:\nhttps://doi.org/10.24432/C5NW2H.\n12\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust\nneural networks. In International Conference on Learning Representations, 2019.\nHadi Salman, Andrew Ilyas, Logan Engstrom, Sai Vemprala, Aleksander Madry, and Ashish Kapoor.\nUnadversarial examples: Designing objects for robust vision. Advances in Neural Information\nProcessing Systems, 34:15270\u201315284, 2021.\nSteffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias\nBethge. Improving robustness against common corruptions by covariate shift adaptation. Advances\nin Neural Information Processing Systems, 33:11539\u201311551, 2020.\nZheyan Shen, Jiashuo Liu, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. Towards\nout-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624, 2021.\nQingyu Tan, Ruidan He, Lidong Bing, and Hwee Tou Ng. Domain generalization for text classification\nwith memory-based supervised contrastive learning. In Proceedings of the 29th International\nConference on Computational Linguistics, pages 6916\u20136926, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nLifu Tu, Garima Lalwani, Spandana Gella, and He He. An empirical study on robustness to spurious\ncorrelations using pre-trained language models. Transactions of the Association for Computational\nLinguistics, 8:621\u2013633, 2020.\nVladimir Vapnik. Principles of risk minimization for learning theory. Advances in Neural Information\nProcessing Systems, 4, 1991.\nYan Wang, Zhixuan Chu, Xin Ouyang, Simeng Wang, Hongyan Hao, Yue Shen, Jinjie Gu, Siqiao\nXue, James Y Zhang, Qing Cui, et al. Enhancing recommender systems with large language model\nreasoning graphs. arXiv preprint arXiv:2308.10835, 2023.\nMitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,\nRaphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust\nfine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 7959\u20137971, 2022.\nZi Xu, Huiling Zhang, Yang Xu, and Guanghui Lan. A unified single-loop alternating gradient projec-\ntion algorithm for nonconvex\u2013concave and convex\u2013nonconcave minimax problems. Mathematical\nProgramming, 201(1):635\u2013706, 2023.\nHao Xue and Flora D Salim. Promptcast: A new prompt-based learning paradigm for time series\nforecasting. IEEE Transactions on Knowledge and Data Engineering, 2023.\nHao Xue, Bhanu Prakash Voutharoja, and Flora D Salim. Leveraging language foundation models\nfor human mobility forecasting. In Proceedings of the 30th International Conference on Advances\nin Geographic Information Systems, pages 1\u20139, 2022.\nLI Xuhong, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with\nconvolutional networks. In International Conference on Machine Learning, pages 2825\u20132834.\nPMLR, 2018.\nZhihan Yue, Yujing Wang, Juanyong Duan, Tianmeng Yang, Congrui Huang, Yunhai Tong, and\nBixiong Xu. Ts2vec: Towards universal representation of time series. In Proceedings of the AAAI\nConference on Artificial Intelligence, pages 8980\u20138987, 2022.\nBoyu Zhang, Hongyang Yang, and Xiao-Yang Liu. Instruct-fingpt: Financial sentiment analysis by\ninstruction tuning of general-purpose large language models. arXiv preprint arXiv:2306.12659,\n2023.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\nrisk minimization. arXiv preprint arXiv:1710.09412, 2017.\n13\nMi Zhang and Alexander A. Sawchuk. Usc-had: A daily activity dataset for ubiquitous activity\nrecognition using wearable sensors. In ACM International Conference on Ubiquitous Comput-\ning (Ubicomp) Workshop on Situation, Activity and Goal Awareness (SAGAware), Pittsburgh,\nPennsylvania, USA, September 2012.\nWenyu Zhang, Mohamed Ragab, and Ramon Sagarna. Robust domain-free domain generalization\nwith class-aware alignment. In ICASSP 2021 - 2021 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 2870\u20132874, 2021. doi: 10.1109/ICASSP39728.\n2021.9413872.\nXuyang Zhao, Tianqi Du, Yisen Wang, Jun Yao, and Weiran Huang. Arcl: Enhancing contrastive\nlearning with augmentation-robust representations. In International Conference on Learning\nRepresentations, 2022.\nZangwei Zheng, Xiangyu Yue, Kai Wang, and Yang You. Prompt vision transformer for domain\ngeneralization. arXiv preprint arXiv:2208.08914, 2022.\nKaiyang Zhou, Yongxin Yang, Yu Qiao, and Tao Xiang. Domain generalization with mixstyle. In\nInternational Conference on Learning Representations, 2021.\nKaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, and Chen Change Loy. Domain generalization: A\nsurvey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\nTian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Rong Jin. One fits all: Power general time series\nanalysis by pretrained lm. arXiv preprint arXiv:2302.11939, 2023.\n14\nAppendix\nA\nTheoretical Proofs and Discussion\nA.1\nProof of Proposition 22\nSince h(\u03b8, q, \u03b4) is a convex function, we have that\nh(\u03b8, q, \u03b4) \u2265h(\u03b8t+1, qt+1, \u03b4t+1) +\n\uf8ee\n\uf8f0\n\u2207\u03b8h(\u03b8t+1, qt+1, \u03b4t+1)\n\u2207qh(\u03b8t+1, qt+1, \u03b4t+1)\n\u2207\u03b4h(\u03b8t+1, qt+1, \u03b4t+1)\n\uf8f9\n\uf8fb\n\u22a4\uf8ee\n\uf8f0\n\u03b8 \u2212\u03b8t+1\nq \u2212qt+1\n\u03b4 \u2212\u03b4t+1\n\uf8f9\n\uf8fb.\n(27)\nAccording to theorem 3, combine with Eq. (15) and Eq. (27), the new cutting plane will be generated\nas\nh(\u03b8t+1, qt+1, \u03b4t+1) +\n\uf8ee\n\uf8f0\n\u2207\u03b8h(\u03b8t+1, qt+1, \u03b4t+1)\n\u2207qh(\u03b8t+1, qt+1, \u03b4t+1)\n\u2207\u03b4h(\u03b8t+1, qt+1, \u03b4t+1)\n\uf8f9\n\uf8fb\n\u22a4\uf8ee\n\uf8f0\n\u03b8 \u2212\u03b8t+1\nq \u2212qt+1\n\u03b4 \u2212\u03b4t+1\n\uf8f9\n\uf8fb\u2264\u03b5.\n(28)\nFrom the inequalities, we can derive the coefficients ai, bi, ci and di as follows\nai = \u2207\u03b8h(\u03b8t+1, qt+1, \u03b4t+1),\nbi = \u2207qh(\u03b8t+1, qt+1, \u03b4t+1),\nci = \u2207\u03b4h(\u03b8t+1, qt+1, \u03b4t+1),\ndi = h(\u03b8t+1, qt+1, \u03b4t+1) \u2212\u2207\u03b8h(\u03b8t+1, qt+1, \u03b4t+1)\u22a4\u03b8t+1\n\u2212\u2207qh(\u03b8t+1, qt+1, \u03b4t+1)\u22a4qt+1 \u2212\u2207\u03b4h(\u03b8t+1, qt+1, \u03b4t+1)\u22a4\u03b4t+1 \u2212\u03b5,\n(29)\nwhich concludes the proof.\nA.2\nProof of Theorem 2\nA.2.1\nBackground\nFor a sample distribution PS on inputs space X with a binary labeling function f and a hypothesis h,\nthe error (or risk) is defined as follows\n\u03f5S(h, f) = Ex\u223cPS[|h(x) \u2212f(x)|].\n(30)\nFor simplicity, we use the shorthand \u03f5S(h) = \u03f5S(h, fS) for source risk and \u03f5T (h) = \u03f5T (h, fD) for\ntarget risk, where fS and fD represent the labeling function of source and target domain, respectively.\nTo bound the target error, following Ben-David et al. [2010], we define the H-divergence. Given\nsource distribution PS and target distribution PT over input space X, let H be a hypothesis class on\nX. The H-divergence between PS and PT is\ndH (PS, PT ) = 2 sup\nh\u2208H\n|PrPS[I(h)] \u2212PrPT [I(h)]| ,\n(31)\nwhere Ih = {x \u2208X : h(x) = 1, h \u2208H}. In addition, for a hypothesis space H, the symmetric\ndifference hypothesis space dH\u2206H is the set of hypotheses. And the dH\u2206H in [Ben-David et al.,\n2010] is defined as\ndH\u2206H (PS, PT ) = 2 sup\nh,h\u2032\u2208H\n|Prx\u223cPS [h(x) \u0338= h\u2032(x)] \u2212Prx\u223cPT [h(x) \u0338= h\u2032(x)]| .\n(32)\nTheorem 6. (Modified from Theorem 4 in [Ben-David et al., 2010]) Let H be a hypothesis space\nof VC dimension d. For each i \u2208{1, . . . , K}, let DSi be a labeled sample of size \u03b2im drawn from\nPSi and labeled according to function fSi. If \u02c6h \u2208H is the empirical minimizer of \u02c6\u03f5\u03b1(h) for a fixed\nweight vector \u03b1 on these samples and h\u2217\nT = minh\u2208H \u03f5T (h) is the target error minimizer, then for\nany \u03b4 \u2208(0, 1), with probability at least 1 \u2212\u03b4,\n\u03f5T (\u02c6h) \u2264\u03f5T (h\u2217\nT ) + PN\ni=1 \u03b1i (2\u03bbi + dH\u2206H (PSi, PT )) + 2\nr\u0010PN\ni=1\n\u03b12\ni\n\u03b2i\n\u0011 \u0010\nd log(2m)\u2212log(\u03b4)\n2m\n\u0011\n,\n(33)\nwhere \u03bbi = minh\u2208H {\u03f5T (h) + \u03f5Si(h)}.\n15\nA.2.2\nProof\nAccording to the definition of dH\u2206H in Eq. (32), we have\ndH\u2206H (PSi, PT ) = 2 sup\nh,h\u2032\u2208H\n\f\fPrx\u223cPSi [h(x) \u0338= h\u2032(x)] \u2212Prx\u223cPT [h(x) \u0338= h\u2032(x)]\n\f\f\n= 2 sup\nh,h\u2032\u2208H\n\f\f\u0000Prx\u223cPSi [h(x) \u0338= h\u2032(x)] \u2212Prx\u223cPC [h(x) \u0338= h\u2032(x)]\n\u0001\n+ (Prx\u223cPC [h(x) \u0338= h\u2032(x)] \u2212Prx\u223cPT [h(x) \u0338= h\u2032(x)])|\n\u22642 sup\nh,h\u2032\u2208H\n\f\fPrx\u223cPSi [h(x) \u0338= h\u2032(x)] \u2212Prx\u223cPC [h(x) \u0338= h\u2032(x)]\n\f\f\n+ 2 sup\nh,h\u2032\u2208H\n|Prx\u223cPC [h(x) \u0338= h\u2032(x)] \u2212Prx\u223cPT [h(x) \u0338= h\u2032(x)]|\n= dH\u2206H (PSi, PC) + dH\u2206H (PC, PT )\n(34)\nSince P\u03b1 = {P\u03b1 | P\u03b1 = P\ni \u03b1iPSi, P\ni \u03b1i = 1, \u03b1i \u22650 \u2200i} and PC \u2208P\u03b1, we can obtain that\ndH\u2206H (PSi, PC) = dH\u2206H\n\u0010\nPSi,\nX\nj \u03b1jPSj\n\u0011\n= dH\u2206H\n\u0010X\nj \u03b1jPSi,\nX\nj \u03b1jPSj\n\u0011\n\u2264\nX\nj \u03b1jdH\u2206H\n\u0000PSi, PSj\n\u0001\n= dH\u2206H\n\u0000PSi, PSj\n\u0001\n\u2264max\ni,j dH\u2206H\n\u0000PSi, PSj\n\u0001\n(35)\nCombine with Eq. (34) and 35, we have\ndH\u2206H (PSi, PT ) \u2264max\ni,j dH\u2206H\n\u0000PSi, PSj\n\u0001\n+ dH\u2206H (PC, PT )\n(36)\nSubstitute Eq. (36) into Eq. (33), we obtain that\n\u03f5T (\u02c6h) \u2264\u03f5T (h\u2217\nT ) +\nXN\ni=1 \u03b1j\n\u0012\n2\u03bbi + max\ni,j dH\u2206H\n\u0000PSi, PSj\n\u0001\n+ dH\u2206H (PC, PT )\n\u0013\n+ 2\ns\u0012XN\ni=1\n\u03b12\ni\n\u03b2i\n\u0013 \u0012d log(2m) \u2212log(\u03b4)\n2m\n\u0013\n.\n(37)\nSince \u03bbi = minh\u2208H {\u03f5T (h) + \u03f5Si(h)} = \u03f5T (h\u2217) + \u03f5Si(h\u2217), by setting \u03bb = 2 PN\ni=1 \u03b1i\u03f5Si(h\u2217) and\nC(\u03b4, m, d) = 2\nr\u0010PN\ni=1\n\u03b12\ni\n\u03b2i\n\u0011 \u0010\nd log(2m)\u2212log(\u03b4)\n2m\n\u0011\nyields the proof.\nA.2.3\nDiscussion\nTheorem 2 provides a theoretical framework for estimating performance on a new target distribution.\nThe first term, \u03f5T (h\u2217\nT ), represents the target error under the ideal hypothesis h\u2217\nT . The second term,\n\u03bb = 2 PN\nj=1 \u03b1i\u03f5Si(h), aggregates the combined error over all source distributions weighted by \u03b1i\nand can be minimized via supervised loss with labels. The third term, dH\u2206H (PC, PT), measures the\ndistributional discrepancy between a composite source distribution and the target distribution. The\nfourth term, maxi,j dH\u2206H(PSi, PSj), quantifies the maximum discrepancy between any two source\ndistributions. The last term, C(\u03b4, m, d), is a statistical term which depends on the confidence level \u03b4,\nsample size m, and VC dimension d.\nThe tri-level learning framework proposed in Eq.\n(10) aims to minimize the third term,\ndH\u2206H (PC, PT), and the fourth term, maxi,j dH\u2206H(PSi, PSj). These two terms correspond to\nthe group-level and sample-level uncertainties, respectively. Below, we discuss how these two terms\nalign with the motivation for our tri-level optimization.\nFor the term dH\u2206H (PC, PT), the goal of time series OOD generalization is to learn a model that\ngeneralizes well to unseen domain distributions, which makes direct optimization infeasible due\nto the unavailability of target dataset. To minimize this discrepancy, we can only enlarge the set\nPC. Specifically, we manipulate \u03b4-perturbations applied to individual samples in the third level of\nour tri-level learning framework. By optimizing these perturbations, we explore a broader range\n16\nof variations within each source domain, which potentially minimizes the term dH\u2206H (PC, PT),\nenhancing the robustness of learned representations.\nFor the term maxi,j dH\u2206H(PSi, PSj), the second-level optimization in our tri-level framework adjusts\nthe weights \u03b1i that define the mixture of source distributions PC. By dynamically modifying these\nweights based on the \u2018worst-case\u2019 distribution, we minimize the term maxi,j dH\u2206H(PSi, PSj).\nOur approach not only enhances representation invariance across diverse domains but also improves\nthe model\u2019s resilience against variations within individual samples. The effectiveness of this tri-level\nframework is rooted in the interdependence between the problems at each level; adjustments in one\nlevel influence the conditions and outcomes of the others. This demonstrates the necessity of our\ntri-level learning optimization, as it requires a coordinated strategy that simultaneously considers\nsample-level, group-level, and parameter-level dynamics.\nA.3\nProof of Theorem 3\nFirst, the first-order Taylor expansion of f2(\u03b8, q, \u03b4) at the point (\u03b8, \u03b4) is obtained as follows\n\u02dcf2(\u03b8, q\u2032, \u03b4) = f2(\u03b8, q\u2032, \u03b4) + \u2207\u03b8f2(\u03b8, q\u2032, \u03b4)T (\u03b8 \u2212\u03b8) + \u2207\u03b4f2(\u03b4, q\u2032, \u03b4)T (\u03b4 \u2212\u03b4).\n(38)\nSince T2 = 1, therefore, we have:\n\u03c6(\u03b8, \u03b4) = q\u2032\n0 \u2212\u03b7q\u2207q\u2032 \u02dcf2 (\u03b8, q\u2032, \u03b4) .\n(39)\nCombine with Eq. (38) and (39), we can obtain that\n\u03c6(\u03b8, \u03b4) = q\u2032\n0 \u2212\u03b7q\u2207q\u2032\n\u0010\nf2(\u03b8, q\u2032, \u03b4) + \u2207\u03b8f2(\u03b8, q\u2032, \u03b4)(\u03b8 \u2212\u03b8) + \u2207\u03b4f2(\u03b8, q\u2032, \u03b4)(\u03b4 \u2212\u03b4)\n\u0011\n= q\u2032\n0 \u2212\u03b7q\u2207q\u2032f2(\u03b8, q\u2032, \u03b4) \u2212\u03b7q\u2207q\u2032\u2207\u03b8f2(\u03b8, q\u2032, \u03b4)\u03b8 + \u2207q\u2032\u2207\u03b8f2(\u03b8, q\u2032, \u03b4)\u03b8\n\u2212\u2207q\u2032\u2207\u03b4f2(\u03b8, q\u2032, \u03b4)\u03b4 + \u2207q\u2032\u2207\u03b4f2(\u03b8, q\u2032, \u03b4)\u03b4\n= \u2212\u03b7q\u2207q\u2032\u2207\u03b8f2(\u03b8, q\u2032, \u03b4)\u03b8 \u2212\u2207q\u2032\u2207\u03b4f2(\u03b8, q\u2032, \u03b4)\u03b4 + C,\n(40)\nwhere C = q\u2032\n0\u2212\u03b7q\u2207q\u2032f2(\u03b8, q\u2032, \u03b4)+\u2207q\u2032\u2207\u03b8f2(\u03b8, q\u2032, \u03b4)\u03b8+\u2207q\u2032\u2207\u03b4f2(\u03b8, q\u2032, \u03b4)\u03b4 is an affine function.\nTherefore, \u03c6(\u03b8, \u03b4) is a convex function. According to preserve convexity[Boyd and Vandenberghe,\n2004], h(\u03b8, q, \u03b4) = \u2225q \u2212\u03d5(\u03b8, \u03b4)\u2225is convexity, which concludes the proof of theorem 3.\nA.4\nProof of Theorem 4\nAssume that in the tth iteration, a new cutting plane is added, and the selected point (\u03b8t+1, qt+1, \u03b4t+1)\nalways lies within the region Rc\nt formed by the cutting plane set Ct, we have\nRc\n0 \u2287Rc\n1 \u2287\u00b7 \u00b7 \u00b7 \u2287Rc\nt.\n(41)\nLet Ht denote the feasible region of problem in Eq. (17) at the tth iteration, and R represent the\nfeasible region of problem in Eq. (15), then it follows that\nH0 \u2287\u00b7 \u00b7 \u00b7 \u2287Ht \u2287H.\n(42)\nLet F(\u03b8t, qt, \u03b4t) denote the optimal value of problem in Eq. (17) at the tth iteration, and f \u2217\n1 represent\nthe optimal value of the problem Eq. (15). Based on equation (42), we can obtain\nF\n\u0000\u03b80, q0, \u03b40\u0001\n\u2264F\n\u0000\u03b8t, qt, \u03b4t\u0001\n\u2264\u00b7 \u00b7 \u00b7 \u2264F \u2217.\n(43)\nIt\u2019s seen that the sequence\n\b\nF(\u03b8t, qt, \u03b4t)\n\t\nis monotonically increasing. As T1 \u2192\u221e, f1 monotoni-\ncally converges to a certain fixed value. It is worth mentioning that h(\u03b8, q, \u03b4) = \u2225q \u2212\u03d5(\u03b8, \u03b4)\u2225is a\nconvex function. Since the sublevel set of a convex function is convex, the feasible region of problem\nin Eq. (17) is convexity. This implies that the iterative procedure, by continuously adding a cutting\nplane, is progressively converging to the optimal value f \u2217\n1 of the problem as referenced in Eq. (15).\nA.5\nProof of Theorem 5\nTo begin with, we introduce a fundamental lemma that is pivotal for the subsequent analysis.\n17\nLemma 1. For\n1\nm\nPm\nj=1 g\u03b8\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\n, 1\nm\nPm\nj=1 gq(\u03b8t, qt, \u03b4t; \u03b6t\nj), 1\nm\nPm\nj=1 g\u03b4\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\n,\nthey are unbiased and bounded, that is,\nE{\u03b6t\nj}\nh\n1\nm\nPm\nj=1 g\u03b8\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001i\n= \u2207\u03b8f1\n\u0000\u03b8t, qt, \u03b4t\u0001\n,\nE{\u03b6t\nj}\n\u0014\r\r\r 1\nm\nPm\nj=1 g\u03b8\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\r\r\r\n2\u0015\n\u2264\u03b12\n1 + \u03c32\n1\nm ,\n(44)\nE{\u03b6t\nj}\nh\n1\nm\nPm\nj=1 gq\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001i\n= \u2207qf1\n\u0000\u03b8t, qt, \u03b4t\u0001\n,\nE{\u03b6t\nj}\n\u0014\r\r\r 1\nm\nPm\nj=1 gq\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\r\r\r\n2\u0015\n\u2264\u03b12\n2 + \u03c32\n2\nm ,\n(45)\nE{\u03b6t\nj}\nh\n1\nm\nPm\nj=1 g\u03b4\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001i\n= \u2207\u03b4f1\n\u0000\u03b8t, qt, \u03b4t\u0001\n,\nE{\u03b6t\nj}\n\u0014\r\r\r 1\nm\nPm\nj=1 g\u03b4\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\r\r\r\n2\u0015\n\u2264\u03b12\n3 + \u03c32\n3\nm .\n(46)\nHere, E{\u03b6t\nj}[\u00b7] denotes the expectation with respect to a set of variables {\u03b6t\n1, \u00b7 \u00b7 \u00b7 , \u03b6t\nm}.\nA.5.1\nProof of Lemma 1\nTaking the variable \u03b8 as an example, according to Assumption 4, for all i = 1, \u00b7 \u00b7 \u00b7 , n, we have\nE{\u03b6t\nj}\nh\n1\nm\nPm\nj=1 g\u03b8\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001i\n= 1\nm\nPm\nj=1 E\u03b6t\nj\n\u0002\ng\u03b8\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\u0003\n= \u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\n.\n(47)\nBased on Assumption 5, the variance of \u2207\u03b8F(\u03b8t, qt, \u03b4t) is bounded, from which we can deduce\nE{\u03b6t\nj}\nh\n1\nm\nPm\nj=1 g\u03b8\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001i\n= E{\u03b6j}\n\u0014\r\r\r 1\nm\nPm\nj=1 g\u03b8\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\n\u2212\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r\r\n2\u0015\n+\n\r\r\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2\n=\nPm\nj=1 E\u03b6j[\u2225g\u03b8(\u03b8,q,\u03b4;\u03b6j)\u2212\u2207\u03b8F (\u03b8,q,\u03b4)\u22252]\nm2\n+ \u03b12\n1\n\u2264\u03c32\n1\nm + \u03b12\n1.\n(48)\nThe proofs of Eq. (45) and Eq. (46) follow a similar logic. Thus, we complete the proof of Lemma 1.\nCombine with lemma 1, we now proceed to derive Theorem 5. Under Assumption 3 that the gradient\nof F is Lipschitz continuous, for t > t1, it follows that\nF\n\u0000\u03b8t+1, qt+1, \u03b4t+1\u0001\n\u2264F\n\u0000\u03b8t, qt, \u03b4t\u0001\n+\n\n\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\n, \u03b8t+1 \u2212\u03b8t\u000b\n+\n\n\u2207qF\n\u0000\u03b8t, qt, \u03b4t\u0001\n, qt+1 \u2212qt\u000b\n+\n\n\u2207\u03b4F\n\u0000\u03b8t, qt, \u03b4t\u0001\n, \u03b4t+1 \u2212\u03b4t\u000b\n+ L\n2\n\u0010\r\r\u03b8t+1 \u2212\u03b8t\r\r2 +\n\r\rqt+1 \u2212qt\r\r2 +\n\r\r\u03b4t+1 \u2212\u03b4t\r\r2\u0011\n= F\n\u0000\u03b8t, qt, \u03b4t\u0001\n\u2212\u03b7\u03b8\nD\n\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\n, 1\nm\nPm\nj=1 g\u03b8\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001E\n\u2212\u03b7q\nD\n\u2207qF\n\u0000\u03b8t, qt, \u03b4t\u0001\n, 1\nm\nPm\nj=1 gq\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001E\n\u2212\u03b7\u03b4\nD\n\u2207\u03b4F\n\u0000\u03b8t, qt, \u03b4t\u0001\n, 1\nm\nPm\nj=1 g\u03b4\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001E\n+ L(\u03b7\u03b8)2\n2\n\r\r\r 1\nm\nPm\nj=1 g\u03b8\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\r\r\r\n2\n+ L(\u03b7q)2\n2\n\r\r\r 1\nm\nPm\nj=1 gq\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\r\r\r\n2\n+ L(\u03b7\u03b4)2\n2\n\r\r\r 1\nm\nPm\nj=1 g\u03b4\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\r\r\r\n2\n.\n(49)\n18\nTaking the expectation of both sides of Equation (49) with respect to {\u03b6t\n1, \u00b7 \u00b7 \u00b7 , \u03b6t\nm}, we can obtain\nE{\u03b6t\nj}\n\u0002\nF\n\u0000\u03b8t+1, qt+1, \u03b4t+1\u0001\u0003\n\u2264F\n\u0000\u03b8t, qt, \u03b4t\u0001\n\u2212\u03b7\u03b8E{\u03b6t\nj}\n\u001c\n\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\n, 1\nm\nXm\nj=1 g\u03b8\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\u001d\n\u2212\u03b7qE{\u03b6t\nj}\n\u001c\n\u2207qF\n\u0000\u03b8t, qt, \u03b4t\u0001\n, 1\nm\nXm\nj=1 gq\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\u001d\n\u2212\u03b7\u03b4E{\u03b6t\nj}\n\u001c\n\u2207\u03b4F\n\u0000\u03b8t, qt, \u03b4t\u0001\n, 1\nm\nXm\nj=1 g\u03b4\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\u001d\n+ L (\u03b7\u03b8)2\n2\nE{\u03b6t\nj}\n\r\r\r\r\n1\nm\nXm\nj=1 g\u03b8\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\r\r\r\r\n2\n+ L (\u03b7q)2\n2\nE{\u03b6t\nj}\n\r\r\r\r\n1\nm\nXm\nj=1 gq\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\r\r\r\r\n2\n+ L (\u03b7\u03b4)2\n2\nE{\u03b6\u2248\n} \u05d2\n\r\r\r\r\n1\nm\nXm\nj=1 g\u03b4\n\u0000\u03b8t, qt, \u03b4t; \u03b6t\nj\n\u0001\r\r\r\r\n2\n(i)\n\u2264F\n\u0000\u03b8t, qt, \u03b4t\u0001\n\u2212\n\r\r\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2 \u03b7\u03b8 \u2212\n\r\r\u2207qF\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2 \u03b7q \u2212\n\r\r\u2207\u03b4F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2 \u03b7\u03b4\n+ L (\u03b7\u03b8)2\n2\n\u0012\n\u03b12\n1 + \u03c32\n1\nm\n\u0013\n+ L (\u03b7q)2\n2\n\u0012\n\u03b12\n2 + \u03c32\n2\nm\n\u0013\n+ L (\u03b7\u03b4)2\n2\n\u0012\n\u03b12\n3 + \u03c32\n3\nm\n\u0013\n.\n(50)\nThe inequality (i) is based on lemma 1. Taking the total expectation of both sides of Eq. (50), we have\nE\n\u0002\nF\n\u0000\u03b8t+1, qt+1, \u03b4t+1\u0001\u0003\n\u2264E\n\u0002\nF\n\u0000\u03b8t, qt, \u03b4t\u0001\u0003\n\u2212\u03b7\u03b8E\nh\r\r\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n\u2212\u03b7qE\nh\r\r\u2207qF\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n\u2212\u03b7\u03b4E\nh\r\r\u2207\u03b4F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n+ L (\u03b7\u03b8)2\n2\n\u0012\n\u03b12\n1 + \u03c32\nm\n\u0013\n+ L (\u03b7q)2\n2\n\u0012\n\u03b12\n2 + \u03c32\nm\n\u0013\n+ L (\u03b7\u03b4)2\n2\n\u0012\n\u03b12\n3 + \u03c32\nm\n\u0013\n,\n(51)\nwhere E[\u00b7] denotes the expectation over all terms. Summing Eq. (51) from t = t1 to t = T1 \u22121, we\nobtain\nE\nh\nF\n\u0010\n\u03b8T1, qT1, \u03b4T1\u0011i\n\u2264E\n\u0002\nF\n\u0000\u03b8t1, qt1, \u03b4t\u0001\u0003\n\u2212\u03b7\u03b8\nT1\u22121\nX\nt=t1\nE\nh\r\r\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n\u2212\u03b7q\nXT1\u22121\nt=T1 E\nh\r\r\u2207qF\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n\u2212\u03b7\u03b4\nXT1\u22121\nt=t1 E\nh\r\r\u2207\u03b4F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n+\nXT1\u22121\nt=t1\nL (\u03b7\u03b8)2\n2\n\u0012\n\u03b12\n1 + \u03c32\n1\nm\n\u0013\n+\nXT1\u22121\nt=t1\nL (\u03b7q)2\n2\n\u0012\n\u03b12\n2 + \u03c32\n2\nm\n\u0013\n+\nXT1\u22121\nt=t1\nL (\u03b7\u03b4)2\n2\n\u0012\n\u03b12\n3 + \u03c32\n3\nm\n\u0013\n=E\n\u0002\nF\n\u0000\u03b8t1, qt1, \u03b4t\u0001\u0003\n\u2212\u03b7\u03b8\nXT1\u22121\nt=t1 E\nh\r\r\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n\u2212\u03b7q\nXT1\u22121\nt=T1 E\nh\r\r\u2207qF\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n\u2212\u03b7\u03b4\nXT1\u22121\nt=t1 E\nh\r\r\u2207\u03b4F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n+ L\n2\nXT1\u22121\nt=t1 (\u03b72\n\u03b8\u03b12\n1 + \u03b72\nq\u03b12\n2 + \u03b72\n\u03b4\u03b12\n3)\n+ L\n2m\nXT1\u22121\nt=t1 (\u03b72\n\u03b8\u03c32\n1 + \u03b72\nq\u03c32\n2 + \u03b72\n\u03b4\u03c32\n3)\n(52)\nLet \u03b7\u03b8 = \u03b7q = \u03b7\u03b4 =\n1\n\u221aT1\u2212t1 , Combining with Eq. (52), we have that\nXT1\u22121\nt=t1 E\nh\r\r\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2\n+\n\r\r\u2207qF\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2 +\n\r\r\u2207\u03b4F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n\u2264F\n\u0010\n\u03b8T1, qT1, \u03b4T1\u0011\n\u2212F \u2217+ L\n2 (T1 \u2212t1)\u22121\n2 XT1\u22121\nt=t1\nX3\ni=1 \u03b12\ni\n+ L\n2m(T1 \u2212t1)\u22121\n2 XT1\u22121\nt=t1\nX3\ni=1 \u03c32\ni .\n(53)\n19\nCombining the definition of \u03f5-stationary point described in Definition (3) with Eq. (53), we have\nXT1\u22121\nt=t1 E\nh\r\r\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2 +\n\r\r\u2207qF\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2 +\n\r\r\u2207\u03b4F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n\u2264F\n\u0010\n\u03b8T1, qT1, \u03b4T1\u0011\n\u2212f \u2217\n1 + L\n2 (T1(\u03f5) \u2212t1)\u22121\n2 XT1\u22121\nt=t1\nX3\ni=1 \u03b12\ni\n+ L\n2m(T1(\u03f5) \u2212t1)\u22121\n2 XT1\u22121\nt=t1\nX3\ni=1 \u03c32\ni\n\u2264\u03f5,\n(54)\nthat is,\nT1(\u03f5) \u223ct1 + L2(m(\u03b12\n1 + \u03b12\n2 + \u03b12\n3) + \u03c32\n1 + \u03c32\n2 + \u03c32\n3)2\n4m2(\u03f5 \u2212F(\u03b8T1, qT1, \u03b4T1) + F \u2217)2\n.\n(55)\nAccording to Eq. (55), we can obtain\nT1(\u03f5) \u223c1\n\u03f52\n(56)\nHence, it can be concluded that there exists a T1(\u03f5) such that \u2225\u2207Gt\u2225= E\nh\r\r\u2207\u03b8F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2 +\n\r\r\u2207qF\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2 +\n\r\r\u2207\u03b4F\n\u0000\u03b8t, qt, \u03b4t\u0001\r\r2i\n\u2264\u03f5, which concludes the proof of Theorem 5.\nB\nDatasets\nB.1\nDatasets Information\nHHAR[Blunck et al., 2015] collected activity data from 9 subjects engaging in 6 activities, using\nsmartphones that capture 3D accelerometer data from various positions. PAMAP[Reiss, 2012]\nencompasses 18 physical activities data from 9 subjects, recorded using wearable sensors monitoring\nphysiological and movement metrics. WESAD[Philip Schmidt et al., 2018] focuses on stress and\naffective state detection from 15 subjects, employing wearable sensors for ECG, EMG, respiration,\nand temperature under varied conditions. SWELL[Koldijk et al., 2014] recorded stress responses\nin a work environment from 25 participants using ECG, EDA, and heart rate sensors during typical\noffice tasks under stressors. USC-HAD[Zhang and Sawchuk, 2012] comprises detailed motion and\norientation data from 14 subjects performing various activities, captured via a MotionNode device\nwith high sampling rate. DSADS[Barshan and Altun, 2013] consists of 19 activities recorded from 8\nsubjects at Bilkent University, using body-worn sensors on torso and limbs, with data segmented for\ndetailed analysis. Table 2 shows the information of the 6 datasets we used in our experiments.\nTable 2: Dataset information.\nDataset\nClasses\nDimension\nsubjects\nsamples\nHHAR\n6\n3\n4\n73420\nPAMAP\n13\n40\n9\n31984\nWESAD\n4\n8\n4\n63180\nSWELL\n2\n3\n16\n52944\nUSC-HAD\n12\n3\n4\n40244\nDSADS\n19\n45\n4\n17520\nB.2\nDomain Setting\nThe domain setting is summarized in Table 3. This setting is done to balance the number of samples\nand classed across different domains.\nB.3\nData pre-processing\nFor all datasets, we configure the window size as 128 and the step size as 64, resulting in a 50%\noverlap between two adjacent time series samples. Each sample is standardized using the formula\n\u02dcx = x\u2212\u00b5\n\u03c3 , where \u00b5 and \u03c3 represent the mean and standard deviation of the dataset, respectively. It\u2019s\n20\nTable 3: Domain setting for HHAR, PAMAP and WESAD dataset.\nDataset\nDomain A\nDomain B\nDomain C\nDomain D\nHHAR\n1\n2\n3\n4\nPAMAP\n1\n2\n3\n4\nWESAD\n1,2\n3,5\n4,6,9\n7,8\nSWELL\n1,2,4,5\n6,7,9,10\n12,13,14,16\n17,18,21,24\nUSC-HAD\n2,3,4\n4,5,6\n1,7,9,10\n11,12,13,14\nDSADS\n1,2\n3,4\n5,6\n7,8\nimportant to note that the \u00b5 and \u03c3 used here is specific to each domain, rather than the entire dataset.\nThis approach is intended to maximize the distinction between data from different domains.\nC\nNetwork Architecture and Hyperparameters\nOur baseline experiments were conducted using a network architecture consisting of 10-layers dilated\nconvolutions network. The dilation rate for each layer is set to 2k, where k is the layer number.\nWe used the same kernel size of 3 across all layers. Optimization was performed using the Adam\noptimizer with a weight decay of 3 \u00d7 10\u22124. For all baseline experiments, we set the batch size to\n256 and the learning rate to 0.002. The training was set to run for a maximum of 50 epochs. All the\nmethods are implemented with PyTorch[Paszke et al., 2019] version 1.7.1 on an NVIDIA GeForce\nRTX 4090 graphics card.\nIn the TTSO fine-tuning experiments, we employed GPT-2 as the language model. Fine-tuning was\nperformed in two stage: In the first stage, the learning rate for the large model was 1 \u00d7 10\u22124, and for\nthe input embedding, it was set at 0.001. In the second phase, we adjusted the learning rate for the\nlarge model to 5 \u00d7 10\u22125, aiming to further refine the model\u2019s performance. During the evaluation\nphase, we froze the parameters of the language model, only fine-tuning the classifier with a learning\nrate of 0.003 to adapt to the specific classification tasks. The batch size was consistently set at 16 for\nall experimental stages.\nD\nDetails of Fine-tuning\nThe structure of LLM Fine-tuning with TTSO is illustrated in figure 3a and 3b. The primary\ncomponents involved are as follows:\nLanguage Model\nInput Embedding\nMulti-Head\nAttention\nAdd & Norm\nFeed Forward\nAdd & Norm\nContrastive Loss\n(a) First Stage: Alignment Fine-tuning\nLanguage Model\nInput Embedding\nMulti-Head\nAttention\nAdd & Norm\nFeed Forward\nAdd & Norm\nClassifier\nSupervised Loss\n(b) Second Stage: Downstream Fine-tuning\nFigure 3: Structure of LLM Fine-tuning with TTSO, illustrating the two-phase approach starting\nwith alignment fine-tuning followed by downstream fine-tuning, adapted specifically for time series\nout-of-distribution generalization tasks.\nInput Embedding: The first is the input embedding, where raw time series is transformed into\nembedding space that is amenable for processing by the language model. In our experiments, the\ninput embedding is a linear layer. As shown in Figure 3, the time series (indicated by the dashed box\nbelow the waveform) is combined with positional encoding to form the input representation.\nLanguage Model: This is the core part of the model, typically comprising multiple pretrained\ntransformer encoder. This model is used for processing the input embeddings and producing advanced\n21\nfeature representations for subsequent classification tasks. Note that, to retain the intrinsic information\nof the language model, only the parameters of layer normalization can be tuned.\nClassifier: In the second stage of fine-tuning, the classifier tailors the language model to the specific\ntime series classification task. It takes the advanced feature representations from the language model\nand fine-tunes the model for downstream tasks. We use a linear layer for the classifier, with cross\nentropy as the supervised loss.\nContrastive Loss: A contrastive loss function is employed to enhance the discriminative of the\nrepresentations in the first stage of the fine-tuning process. This loss function aims to ensure that the\nrepresentations of similar time series samples are brought closer together in the representations space,\nwhile representations of dissimilar samples are pushed apart. Specifically, during this stage, the\ncontrastive loss acts as a guiding signal for the language model, encouraging it to learn representations\nthat effectively capture the underlying patterns and distinctions within the time series data, thereby\nadapting the language model to time series data more effectively.\nE\nMore Experiments\nE.1\nAdditional Datasets\nWe conducte on more datasets to demonstrate the superior performance of our framework. The results\nis summarized in Table 4.\nTable 4: Classification accuracy(%) on SWELL, USC-HAD and DSADS datasets.\nSWELL\nUSC-HAD\nDSADS\nALL\nMethod\nA\nB\nC\nD\nAVG\nA\nB\nC\nD\nAVG\nA\nB\nC\nD\nAVG\nAVG\nADARNN\n58.3\n66.2\n57.5\n50.6\n58.2\n59.5\n60.7\n60.0\n58.6\n59.7\n88.1\n77.5\n91.3\n82.9\n85.0\n67.7\nGILE\n56.7\n58.1\n54.9\n62.3\n58.0\n61.0\n64.3\n66.2\n57.2\n62.2\n84.7\n76.3\n82.6\n78.2\n80.5\n66.9\nERM\n55.6\n59.6\n53.0\n61.1\n57.3\n60.7\n62.9\n64.1\n59.3\n61.8\n86.7\n81.9\n87.3\n81.7\n84.4\n67.9\nIRM\n60.4\n58.5\n52.4\n60.3\n57.9\n60.3\n52.3\n68.2\n56.3\n59.3\n89.9\n78.4\n90.1\n83.1\n85.4\n67.5\nGroupDRO\n61.9\n60.8\n51.5\n59.1\n58.3\n62.6\n63.3\n67.6\n58.3\n63.0\n92.0\n81.6\n90.0\n82.6\n86.6\n69.3\nANDMask\n58.1\n57.9\n52.0\n63.0\n57.8\n57.8\n58.4\n66.3\n57.6\n60.0\n89.7\n79.0\n89.9\n82.5\n85.3\n67.7\nRSC\n55.6\n60.5\n65.3\n58.1\n59.9\n56.7\n56.3\n66.4\n56.6\n59.0\n85.0\n81.9\n88.1\n80.9\n84.0\n67.6\nMixup\n57.5\n61.5\n57.2\n52.6\n57.2\n64.9\n61.4\n67.9\n56.4\n62.9\n93.9\n82.3\n91.5\n84.7\n88.1\n69.4\nVERx\n57.3\n58.2\n51.7\n57.5\n56.2\n58.6\n57.3\n66.5\n59.1\n60.4\n89.8\n79.0\n95.4\n87.0\n87.9\n68.1\nDIFEX\n55.7\n61.0\n61.6\n57.2\n58.9\n59.8\n56.5\n67.2\n56.7\n60.1\n87.8\n82.8\n91.0\n83.2\n86.2\n68.4\nDIVERSIFY\n62.8\n61.9\n59.2\n63.4\n61.8\n64.7\n62.2\n60.3\n65.1\n63.1\n89.0\n86.2\n92.2\n85.7\n88.3\n71.3\nTTSO\n65.2\n65.7\n66.5\n59.1\n64.6\n63.1\n61.2\n68.0\n66.0\n64.6\n93.0\n88.9\n91.7\n87.8\n90.4\n73.2\nTTSO\u2217\n60.5\n68.7\n72.4\n63.5\n66.3\n62.1\n63.5\n65.2\n63.0\n63.3\n92.0\n89.9\n92.2\n89.5\n90.9\n73.4\nE.2\nIllustration of Sample-level and Group-level Uncertainties\n0\n20\n40\n60\n80\n100\n120\nTime Steps\n1.20\n1.25\n1.30\n1.35\n1.40\n1.45\n1.50\n1.55\n1.60\nFeatures\nFigure 4: Sample-Level Uncertainty: Each line represents a window of time series data with the same\nlabel.\nTo illustrate the concepts of sample-level and group-level uncertainties, we use the x-axis values from\naccelerometer data collected by the \u2018samsungold_1\u2019 device from four users in the HHAR dataset.\nIn Figure 4, sample-level uncertainty is shown by plotting time series data from a specific label (e.g.,\n\u2019walking\u2019), where each line represents a different time window. The variations among these lines\nillustrate the inherent noise, which represents sample-level uncertainty.\n22\n0.5\n1.0\n1.5\n2.0\n2.5\nx-axis values\n0\n2\n4\n6\n8\n10\nDensity\nGroups\nA\nB\nC\nD\nFigure 5: Group-Level Uncertainty: Histogram of \u2019x\u2019 axis values, with each color representing a\ndifferent group.\nFigure 5 demonstrates the group-level uncertainty by displaying the distribution of x-axis values from\nthe accelerometer across different groups (users). Each color represents a distinct group, and each\ngroup\u2019s unique characteristics contribute to the overall group-level uncertainty.\nE.3\nAblation Study on LLM Architectures and Parameter Configurations\nTo further investigate the impact of various architectures and parameter settings of LLMs, we\nconducted additional ablation experiments that focused on different LLM architectures and parameter\nsizes (e.g., base model and large model). These experiments included encoder-only models (e.g.,\nBERT), decoder-only models (e.g., GPT-2), and encoder-decoder models (e.g., BART) to determine\nwhich configurations yield the greatest benefits during fine-tuning. The results are summarized in the\ntable 5.\nTable 5: Performance comparison of different LLM architectures and parameter sizes across datasets.\nArchitecture\nVersion\nHHAR\nPAMAP\nWESAD\nAVG\nEncoder-Only (BERT)\nBase\n64.3\n66.9\n64.4\n64.2\nLarge\n61.7\n52.5\n62.3\n58.8\nDecoder-Only (GPT)\nBase\n72.9\n76.1\n68.4\n72.5\nLarge\n64.5\n69.4\n66.5\n66.8\nEncoder-Decoder (BART)\nBase\n57.3\n65.4\n64.2\n62.3\nLarge\n55.5\n61.2\n61.4\n59.4\nThe results indicate that decoder-only architectures, specifically the GPT-2 base model in this\nexperiments, achieve the best performance. However, increasing the number of parameters in all\nthree architectures leads to a significant drop in performance across 3 architectures for time series\nOOD generalization.\n0\n1\n2\n4\n6\n8\n10\n12\nk\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\nAverage Accuracy\nHHAR\nPAMAP\nWESAD\nFigure 6: The effect of varying the number of Transformer layers (k) on average accuracy for OOD\ngeneralization across HHAR, PAMAP, and WESAD datasets.\nTo further explore how the number of parameters affects performance, we conducted experiments\nusing GPT-2 models with varying numbers of Transformer layers on 3 datasets to evaluate their OOD\n23\ngeneralization performance. For these experiments, we utilized 20% of each dataset. As shown in\nFigure 3 (in the attached PDF), the results demonstrate that optimal OOD generalization performance\nis achieved with a configuration of 8 Transformer layers.\nBased on this findings, we incorporate this optimal layer configuration with TTSO framework,\nyielding improved results as detailed in the table 6.\nTable 6: Performance improvements on different domains for HHAR, PAMAP, and WESAD datasets.\nTarget\nA\nB\nC\nD\nAVG\nHHAR\n+1.4\n+0.1\n+0.8\n+0.9\n+0.80\nPAMAP\n+1.0\n-1.4\n+0.8\n-0.3\n+0.03\nWESAD\n+5.5\n-3.5\n+2.5\n-0.1\n+1.35\nF\nLimitation\nTTSO is a general framework for learning invariant representations across diverse domain distri-\nbutions, currently discussed only for time series classification. This framework could be further\nenhanced by extending it to more time series OOD tasks, such as time series forecasting and anomaly\ndetection. Additionally, distribution shifts occur not only in time series but also in other machine\nlearning domains like images [Deecke et al., 2021] and text [Tan et al., 2022]. Applying our approach\nto these domains could further improve performance.\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper\u2019s contributions and scope?\nAnswer: [Yes]\nJustification: In the abstract and introduction, we clearly state the main contributions of\nour work. The tri-level learning framework, stratified localization algorithm and iteration\ncomplexity analysis is in Section 3.2, 3.3 and 4.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: The limitations of this work can be found in Appendix F.\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n24\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n\u2022 The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n\u2022 While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren\u2019t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\nJustification: In this paper, all theoretical results are accompanied by a full set of assumptions\nand complete proofs. These are clearly stated and numbered within the main text and are\ncross-referenced appropriately. Detailed proofs for major theorems are provided in the\nAppendix.\nGuidelines:\n\u2022 The answer NA means that the paper does not include theoretical results.\n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.\n\u2022 The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: Our paper provides detailed descriptions of the experimental setup. Detailed\ninformation regarding datasets, domain setting, data pre-processing, network architecture\nand hyperparameters can be found in Appendix B.1, B.2, B.3 and C.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n25\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n\u2022 While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [No]\nJustification: While the data used in our study is publicly available, we are currently unable\nto provide open access to the code. However, we have included detailed descriptions of the\ndata access, preprocessing steps, model architecture, and experimental setup in the paper\nand Appendix. These details should be sufficient for others to reproduce the experimental\nresults.\nGuidelines:\n\u2022 The answer NA means that paper does not include experiments requiring code.\n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n\u2022 While we encourage the release of code and data, we understand that this might not be\npossible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n\u2022 The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\u2022 The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n\u2022 The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n\u2022 At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n\u2022 Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n26\n6. Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Our paper provides detailed descriptions of the experimental details. Detailed\ninformation regarding datasets, domain setting, data pre-processing, network architecture\nand hyperparameters can be found in Appendix B.1, B.2, B.3 and C.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n\u2022 The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment Statistical Significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: We report the mean accuracy and standard deviation across three runs with\ndifferent random seeds.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n\u2022 The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n\u2022 The assumptions made should be given (e.g., Normally distributed errors).\n\u2022 It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments Compute Resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: We provide the type of compute workers (GPU) in the Appendix C\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n27\n\u2022 The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n\u2022 The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn\u2019t make it into the paper).\n9. Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: Our research adheres to all guidelines outlined in the NeurIPS Code of Ethics.\nGuidelines:\n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\u2022 If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: There is no societal impact of the work performed.\nGuidelines:\n\u2022 The answer NA means that there is no societal impact of the work performed.\n\u2022 If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n\u2022 Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n\u2022 The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n\u2022 The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: This paper poses no such risks.\nGuidelines:\n\u2022 The answer NA means that the paper poses no such risks.\n28\n\u2022 Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n\u2022 We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [NA]\nJustification: This paper does not use existing assets.\nGuidelines:\n\u2022 The answer NA means that the paper does not use existing assets.\n\u2022 The authors should cite the original paper that produced the code package or dataset.\n\u2022 The authors should state which version of the asset is used and, if possible, include a\nURL.\n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n\u2022 If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n\u2022 For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n\u2022 If this information is not available online, the authors are encouraged to reach out to\nthe asset\u2019s creators.\n13. New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [NA]\nJustification: This paper does not release new assets.\nGuidelines:\n\u2022 The answer NA means that the paper does not release new assets.\n\u2022 Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n\u2022 The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: This paper does not involve crowdsourcing experiments or research with\nhuman subjects.\n29\nGuidelines:\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022 Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: This paper does not involve crowdsourcing experiments or research with\nhuman subjects.\nGuidelines:\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n\u2022 We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n\u2022 For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n30"
    },
    {
        "paper": "FinBen A Holistic Financial Benchmark for Large Language Models",
        "input": "FinBen: A Holistic Financial Benchmark for Large\nLanguage Models\nQianqian Xieb,a, Weiguang Hanb, Zhengyu Chenb, Ruoyu Xianga, Xiao Zhanga, Yueru Hea,\nMengxi Xiaob, Dong Lib, Yongfu Daig, Duanyu Fengg, Yijing Xua, Haoqiang Kange,\nZiyan Kuangl, Chenhan Yuanc, Kailai Yangc, Zheheng Luoc, Tianlin Zhangc,\nZhiwei Liuc, Guojun Xiongj, Zhiyang Dengi, Yuechen Jiangi, Zhiyuan Yaoi,\nHaohang Lii, Yangyang Yui,\u2217, Gang Huh, Jiajia Huangk, Xiao-Yang Liue,\u2217,\nAlejandro Lopez-Lirad,\u2217, Benyou Wangf, Yanzhao Laim, Hao Wangg, Min Pengb,\u2217,\nSophia Ananiadouc,\u2217, Jimin Huanga,\u2217\naThe Fin AI, bWuhan University, cThe University of Manchester, dUniversity of Florida,\neColumbia University, fThe Chinese University of Hong Kong, Shenzhen,\ngSichuan University, hYunnan University, iStevens Institute of Technology\njStony Brook University, kNanjing Audit University,\nlJiangxi Normal University, mSouthwest Jiaotong University\nAbstract\nLLMs have transformed NLP and shown promise in various fields, yet their poten-\ntial in finance is underexplored due to a lack of comprehensive benchmarks, the\nrapid development of LLMs, and the complexity of financial tasks. In this paper, we\nintroduce FinBen, the first extensive open-source evaluation benchmark, including\n42 datasets spanning 24 financial tasks, covering eight critical aspects: information\nextraction (IE), textual analysis, question answering (QA), text generation, risk\nmanagement, forecasting, decision-making, and bilingual (English and Spanish).\nFinBen offers several key innovations: a broader range of tasks and datasets, the\nfirst evaluation of stock trading, novel agent and Retrieval-Augmented Generation\n(RAG) evaluation, and two novel datasets for regulations and stock trading. Our\nevaluation of 21 representative LLMs, including GPT-4, ChatGPT, and the latest\nGemini, reveals several key findings: While LLMs excel in IE and textual analysis,\nthey struggle with advanced reasoning and complex tasks like text generation and\nforecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text\ngeneration and forecasting. Instruction-tuned LLMs improve textual analysis but\noffer limited benefits for complex tasks such as QA. FinBen has been used to host\nthe first financial LLMs shared task at the FinNLP-AgentScen workshop during\nIJCAI-2024, attracting 12 teams. Their novel solutions outperformed GPT-4, show-\ncasing FinBen\u2019s potential to drive innovations in financial LLMs. All datasets and\ncode are publicly available for the research community2, with results shared and\nupdated regularly on the Open Financial LLM Leaderboard3.\n\u2217Corresponding Authors\n2https://github.com/The-FinAI/PIXIU\n3Now\nunder\nthe\numbrella\nof\nFINOS\nat\nLinux\nFoundation,\nhttps://finosfoundation/\nOpen-Financial-LLM-Leaderboard\n38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.\nTable 1: Comparison of different financial bench-\nmarks based on the number of tasks and datasets\nand the task counts across aspects: information\nextraction (IE), textual analysis (TA), question\nanswering (QA), text generation (TG), risk man-\nagement (RM), forecasting (FO), decision-making\n(DM), and spanish (SP).\nBenchmark\nLanguage\nDataset Task IE TA QA TG RM FO DM SP\nCFBenchmark\nChinese\n8\n7\n1\n3\n%\n3\n%\n%\n% %\nFin-Eva\nChinese\n1\n1\n% %\n1\n%\n%\n%\n% %\nPIXIU\nEnglish\n15\n8\n1\n3\n1\n1\n1\n1\n% %\nFinanceBench\nEnglish\n1\n1\n% %\n1\n%\n%\n%\n% %\nBizBench\nEnglish\n8\n5\n2 %\n2\n1\n%\n%\n% %\nFinBen\nEnglish, Spanish\n42\n24\n6\n8\n3\n1\n4\n1\n1\n4\nTextual\nAnalysis\nRisk\nManagement\nInformatio\nn\nExtraction\nSpanish\nQuestion\nAnswering\nForecastin\ng\nText\nGeneration\nDecision\nMaking\nFOMC\nFPB\nFiQA-SA\nFinArg-ACC\nFinArg-ARC\nHeadlines\nMA\nMLESG\nMultiFin\nTSA\nAustralian\nGerman\nLendingClu\nb\nProtoSegur\no\nccf\nccfraud\npolish\ntaiwan\ntravelinsu\nrance\nCD\nFNXL\nFSRL\nFiNER-ORD\nFinRED\nNER\nSC\nEFP\nEFPA\nFNS\nFinanceES\nMultiFin-\nES\nTSA\nConvFinQA\nFinQA\nRegulation\ns\nTATQA\nACL18\nBigData22\nCIKM18\nECTSum\nEDTSum\nFintrade\n100\n4K\nFigure 1: FinBen\u2019s evaluation datasets\nwith sizes ranging from 100 to 4, 000.\n1\nIntroduction\nRecently, Large Language Models (LLMs) (Brown et al., 2020) such as ChatGPT4 and GPT-4 (Ope-\nnAI, 2023), have reshaped the field of natural language processing (NLP) and exhibited remarkable\ncapabilities in specialized domains across mathematics, coding, medicine, law, and finance (Bubeck\net al., 2023). Within the financial domain, recent several studies (Xie et al., 2023a; Lopez-Lira and\nTang, 2023; Li et al., 2023c; Xie et al., 2023b; Liu et al., 2023a; Yang et al., 2023a; Xie et al., 2024)\nhave shown the great potential of LLMs such as GPT-4 on financial text analysis and prediction tasks.\nWhile their potential is evident, a comprehensive understanding of their capabilities and limitations\nfor finance remains largely unexplored. This is due to a lack of extensive evaluation studies and\nbenchmarks, and the inherent complexities associated with the professional nature of financial tasks.\nExisting financial domain evaluation benchmarks, including PIXIU (Xie et al., 2023b), Fi-\nnanceBench (Islam et al., 2023) and BizBench (Koncel-Kedziorski et al., 2023), have limited\nevaluation tasks and primarily focus on financial NLP tasks, as shown in Table 1. Most existing\nbenchmarks cover only a small number of evaluation tasks and are centered on NLP capabilities, such\nas information extraction (IE) and question answering (QA) (Huang et al., 2024; Liu et al., 2024b;\nHu et al., 2024; Yang et al., 2024; Zhao et al., 2024a,c). While PIXIU stands out by covering the\nhighest number of tasks, it includes only one evaluation task in most categories. This narrow focus\nlimits their ability to comprehensively evaluate LLMs across the diverse and complex landscape of\nfinancial applications, such as forecasting, risk management, and decision-making. It is insufficient\nfor a thorough evaluation of LLM capabilities, especially in the financial area.\nTo bridge this gap, we propose FinBen, a novel comprehensive open-source evaluation benchmark\ndeveloped through the collaborative efforts of experts in both computer science and finance. As shown\nin Figure 1, FinBen comprises 42 datasets spanning 24 financial tasks, meticulously organized to\nassess LLMs across eight critical aspects: information extraction (IE), textual analysis (TA), question\nanswering (QA), text generation (TG), risk management (RM), forecasting (FO), decision-making\n(DM), and bilingual (English and Spanish). Each category targets specific skills of financial data\nprocessing and analysis, ensuring a thorough evaluation of LLMs and showcasing their proficiency in\nmanaging complex financial scenarios.\nFinBen introduces several innovations over existing benchmarks: 1) New tasks: FinBen introduces a\nsignificantly larger number of tasks and datasets, making it the most holistic benchmark for financial\nLLMs with the highest number of tasks and datasets. This extensive range provides a more robust\nevaluation of LLM capabilities in diverse financial contexts. 2) Broader coverage: Covering eight\naspects of the financial sector, FinBen is the first benchmark to include the evaluation of stock trading,\nwhich is the fundamental task in the financial sector, involving complex decision-making processes\nthat impact market dynamics and investment strategies. 3) New evaluation strategy: FinBen is\nthe first benchmark to include agent-based evaluation and retrieval-augmented generation (RAG)\n4https://openai.com/chatgpt\n2\nbased evaluation. These innovative strategies provide a more dynamic and realistic assessment of\nLLMs, reflecting their ability to interact with and retrieve relevant information from vast datasets.\n4) Novel datasets: FinBen proposes two novel open-source datasets of QA and stock trading tasks\nfor the research community, pushing the boundaries of what LLMs can achieve and setting a new\nstandard for dataset comprehensiveness. 5) Empowering financial LLMs research: Leveraging\nfinancial tasks in FinBen, we hosted the first shared task (see Appendix G for details) focused on\nfinancial LLMs at the FinNLP-AgentScen workshop during IJCAI-2024 5. This event attracted\n12 teams, leveraging our benchmark to develop novel LLMs-based solutions within the financial\ndomain. Remarkably, the proposed methods achieved superior performance compared to GPT-4,\ndemonstrating the benchmark\u2019s potential to foster innovations and advance the state-of-the-art (SOTA)\nin financial LLMs.\nBased on FinBen, we assess 21 representative general LLMs such as GPT-4, ChatGPT, and the\nlatest Gemini, and financial LLMs, and have the following findings: 1) Superior Capabilities\nwith Limitations: While LLMs exhibit exceptional prowess in IE and textual analysis tasks, they\nunderperform in areas necessitating advanced reasoning and complex IE, such as text generation and\nforecasting. 2) Potential in Stock Trading: SOTA LLMs have demonstrated considerable promise\nin stock trading applications. However, there remains significant room for improvement due to their\nlimitations in reasoning and comprehensive forecasting abilities. 3) Closed-Source Superiority:\nClosed-source commercial LLMs continue to lead in performance within the financial domain.\nSpecifically, GPT-4 excels in IE, text analysis, QA, and intricate stock trading tasks, while Gemini\nshows superior capabilities in text generation and forecasting. 4) Open-Source Improvements and\nLimitations: While open-source, instruction-tuned financial LLMs have shown notable enhancements\nin textual analysis and IE tasks, the advantages of instruction-tuning are less pronounced when it\ncomes to complex tasks such as QA, text generation, and forecasting.\nIn summary, the main contributions of this paper are: 1) we present FinBen, the first comprehensive\nopen-sourced evaluation benchmark for LLMs in the financial domain, 2) we utilize a novel taxonomy\ncovering eight aspects for organizing financial evaluation tasks, 3) we develop two novel evaluation\ndatasets for the research community, and 4) we conduct systematic evaluation of 21 LLMs using\nFinBen, showcasing their advantages and limitations and highlighting directions for future work.\n2\nFinBen\nIn this section, we delve into the specifics of FinBen, detailing the evaluation taxonomy, data sources,\nand evaluation tasks.\n2.1\nThe Taxonomy of Financial Evaluation Tasks\nIn the dynamic landscape of financial technology, evaluating the capabilities of LLMs necessitates a\ncomprehensive and structured approach. We propose a novel taxonomy for financial evaluation tasks,\ncategorizing and assessing LLMs across eight financial domains inspired by established taxonomies in\nfinancial tasks (Cao, 2022; Li et al., 2023b; Zhao et al., 2024b): Information Extraction (IE), Textual\nAnalysis (TA), Question Answering (QA), Text Generation (TG), Risk Management (RM),\nForecasting (FO), Decision-Making (DM), and Spanish (SP). Information Extraction focuses\non identifying key entities and relationships within financial documents, transforming unstructured\ndata into structured insights (Costantino and Coletti, 2008). Textual Analysis delves into content\nand sentiment analysis of financial texts, aiding in market trend understanding (Loughran and\nMcDonald, 2020). Question Answering evaluates the model\u2019s ability to comprehend and respond to\nfinancial queries (Maia et al., 2018). Text Generation assesses the production of coherent financial\ntext (La Quatra and Cagliero, 2020). Risk Management involves evaluating creditworthiness,\ndetecting fraud, and ensuring regulatory compliance (Aziz and Dowling, 2019). Forecasting predicts\nfuture financial trends, enabling strategic responses to market dynamics (Abu-Mostafa and Atiya,\n1996). Decision-Making assesses the model\u2019s proficiency in making informed financial decisions,\nsuch as developing trading strategies and optimizing investment portfolios (Paiva et al., 2019). Finally,\nSpanish evaluates the model\u2019s capabilities in other languages except for English, particularly in\nlow-resource languages.\n5https://sites.google.com/nlg.csie.ntu.edu.tw/finnlp-agentscen\n3\n2.2\nData Sources\nFinBen\u2019s evaluation tasks are drawn from three primary data sources: 1) open-sourced datasets from\nexisting studies originally released for non-LLM evaluation settings. Domain experts have designed\ndiverse prompts and reformulated these datasets into instruction-response pairs, making them suitable\nfor evaluating the zero-shot performance of LLMs. 2) datasets from existing evaluation benchmarks\nsuch as PIXIU. These datasets have already been transformed into the instruction tuning format,\nallowing for seamless integration and direct use in FinBen. 3) novel datasets introduced in this paper.\nThese datasets are designed to address gaps in existing benchmarks and provide unique challenges\nfor financial LLMs evaluation. Novel datasets include (As shown in Table 2):\nFinTrade. The FinTrade dataset is developed specifically for stock trading tasks, integrating historical\nstock prices, filings data, and news data for 10 stocks over a one-year period. It provides a robust\nfoundation for evaluating LLMs in agent-based financial trading scenarios. The dataset is composed\nof three main components6: (1) Stock Price Data: Historical price data for 497 trading days, obtained\nvia the yfinance API from Yahoo Finance, includes OHLCV (open, high, low, close, adjusted close\nprice, and volume) metrics. Adjusted close prices are used to maintain consistency in the return\nseries, minimizing the impact of corporate actions like dividends and stock splits. (2) Filings Data:\nSummary sections from Form 10-Q (quarterly reports) and Form 10-K (annual reports) are retrieved\nfrom the EDGAR database of the U.S. Securities and Exchange Commission (SEC). Over one year,\neach stock is linked to three quarterly reports and one annual report, providing crucial quarterly\ninsights. (3) News Data: Daily news data, compiled from multiple publicly accessible datasets,\nprovides short-term market perspectives, enabling the agent to account for market sentiment. The\ntable below summarizes the data statistics.\nRegulations. The Regulations dataset focuses on long-form question answering related to Over-the-\nCounter (OTC) derivatives and financial regulations within the European Union. Derived from the\nEuropean Securities and Markets Authority\u2019s (ESMA) comprehensive document on Regulation (EU)\nNo 648/2012 (EMIR), it maps QA pairs to relevant articles from EMIR and other directives. EMIR,\nimplemented to enhance transparency and reduce risks in derivatives trading, governs OTC derivatives,\ncentral counterparties, and trade repositories. The dataset includes 254 QA pairs, meticulously\ncurated with domain experts to ensure relevance and accuracy, addressing key regulatory issues\nsuch as reporting requirements, clearing thresholds, and obligations for financial and non-financial\ncounterparties. The QAs are updated to reflect ongoing regulatory changes, providing a dynamic\nresource for testing LLMs\u2019 understanding of complex regulatory frameworks. This dataset serves as\na critical tool for both regulatory compliance and academic research.\n2.3\nTasks\nTable 2 and Figure 1 shows all tasks, datasets, data statistics, and evaluation metrics covered by\nFinBen7.\nInformation extraction:\nIt spans seven datasets across six information extraction tasks. 1) Named\nentity recognition extracts entities like LOCATION, ORGANIZATION, and PERSON from financial\nagreements and SEC filings, using the NER (Alvarado et al., 2015) and FINER-ORD (Shah et al.,\n2023b) datasets. 2) Relation extraction identifies relationships such as \"product/material produced\"\nand \"manufacturer\" in financial news and earnings transcripts with the FINRED dataset (Sharma\net al., 2022). 3) Causal classification discerns whether sentences from financial news and SEC filings\nconvey causality using the SC dataset (Mariko et al., 2020). 4) Causal detection identifies cause\nand effect spans in financial texts with the CD dataset (Mariko et al., 2020). 5) Numeric labeling\ntags numeric spans in financial documents using the FNXL dataset (Sharma et al., 2023), focusing\non automating the assignment of labels from a large taxonomy to numeral spans in sentences. 6)\nTextual analogy parsing involves identifying common attributes and comparative elements in textual\nanalogies by extracting analogy frames, utilizing the FSRL dataset (Lamm et al., 2018), which maps\nanalogous facts to semantic role representations and identifies the analogical relations between them.\nThe evaluation of these tasks is focused on the F1 score (Goutte and Gaussier, 2005), Entity F1\nscore (Derczynski, 2016), and the Exact Match Accuracy (EM Accuracy) metric (Kim et al., 2023).\n6Please see Appendix for more details\n7For detailed instructions of each dataset, please see Appendix D\n4\nTable 2: The tasks, datasets, data statistics, and evaluation metrics included in FinBen. We use only\ntest data for evaluation. Datasets marked with an asterisk (*) are newly constructed by us, comprising\n10.32% of the total data. EM Accuracy means the exact match accuracy.\nData\nTask\nTest\nEvaluation\nLicense\nNER (Alvarado et al., 2015)\nnamed entity recognition\n980\nEntity F1\nCC BY-SA 3.0\nFiNER-ORD (Shah et al., 2023b)\nnamed entity recognition\n1,080\nEntity F1\nCC BY-NC 4.0\nFinRED (Sharma et al., 2022)\nrelation extraction\n1,068\nF1, Entity F1\nPublic\nSC (Mariko et al., 2020)\ncausal classification\n8,630\nF1, Entity F1\nCC BY 4.0\nCD (Mariko et al., 2020)\ncausal detection\n226\nF1, Entity F1\nCC BY 4.0\nFNXL (Sharma et al., 2023)\nnumeric labeling\n318\nF1, EM Accuracy\nPublic\nFSRL (Lamm et al., 2018)\ntextual analogy parsing\n97\nF1, EM Accuracy\nMIT License\nFPB (Malo et al., 2014)\nsentiment analysis\n970\nF1, Accuracy\nCC BY-SA 3.0\nFiQA-SA (Maia et al., 2018)\nsentiment analysis\n235\nF1\nPublic\nTSA (Cortis et al., 2017)\nsentiment analysis\n561\nF1, Accuracy\nCC BY-NC-SA 4.0\nHeadlines (Sinha and Khandait, 2021)\nnews headline classification\n2,283\nAvg F1\nCC BY-SA 3.0\nFOMC (Shah et al., 2023a)\nhawkish-dovish classification\n496\nF1, Accuracy\nCC BY-NC 4.0\nFinArg-ACC (Sy et al., 2023)\nargument unit classification\n969\nF1, Accuracy\nCC BY-NC-SA 4.0\nFinArg-ARC (Sy et al., 2023)\nargument relation classification\n496\nF1, Accuracy\nCC BY-NC-SA 4.0\nMultiFin (J\u00f8rgensen et al., 2023)\nmulti-class classification\n690\nF1, Accuracy\nPublic\nMA (Yang et al., 2020a)\ndeal completeness classification\n500\nF1, Accuracy\nPublic\nMLESG (Chen et al., 2023a)\nESG Issue Identification\n300\nF1, Accuracy\nCC BY-NC-ND\nFinQA (Chen et al., 2021)\nquestion answering\n1,147\nEM Accuracy\nMIT License\nTATQA (Zhu et al., 2021)\nquestion answering\n1,668\nF1, EM Accuracy\nMIT License\n*Regulations\nlong-form question answering\n254\nROUGE, BERTScore\nPublic\nConvFinQA (Chen et al., 2022b)\nmulti-turn question answering\n1,490\nEM Accuracy\nMIT License\nECTSum (Mukherjee et al., 2022)\ntext summarization\n495\nROUGE, BERTScore, BARTScore\nPublic\nEDTSum (Xie et al., 2023b)\ntext summarization\n2,000\nROUGE, BERTScore, BARTScore\nPublic\nBigData22 (Soun et al., 2022)\nstock movement prediction\n1,470\nAccuracy, MCC\nPublic\nACL18 (Xu and Cohen, 2018)\nstock movement prediction\n3,720\nAccuracy, MCC\nMIT License\nCIKM18 (Wu et al., 2018)\nstock movement prediction\n1,140\nAccuracy, MCC\nPublic\nGerman (Hofmann, 1994)\ncredit scoring\n1,000\nF1, MCC\nCC BY 4.0\nAustralian (Quinlan, [n. d.])\ncredit scoring\n690\nF1, MCC\nCC BY 4.0\nLendingClub (Feng et al., 2023)\ncredit scoring\n2,690\nF1, MCC\nCC0 1.0\nccf (Feng et al., 2023)\nfraud detection\n2,278\nF1, MCC\n(DbCL) v1.0\nccfraud (Feng et al., 2023)\nfraud detection\n2,097\nF1, MCC\nPublic\npolish (Feng et al., 2023)\nfinancial distress identification\n1,736\nF1, MCC\nCC BY 4.0\ntaiwan (Feng et al., 2023)\nfinancial distress identification\n1,364\nF1, MCC\nCC BY 4.0\nProtoSeguro (Feng et al., 2023)\nclaim analysis\n2,381\nF1, MCC\nPublic\ntravelinsurance (Feng et al., 2023)\nclaim analysis\n3,800\nF1, MCC\n(ODbL) v1.0\n*FinTrade\nstock trading\n3,384\nCR, SR, DV, AV, MD\nMIT License\nMultiFin-ES\nmulti-class classification\n2,066\nF1, Accuracy\nMIT License\nFNS-2023\ntext summarization\n232\nROUGE, BERTScore, BARTScore\nPublic\nEFP\nquestion answering\n37\nF1, Accuracy\nPublic\nEFPA\nquestion answering\n228\nF1, Accuracy\nPublic\nTSA\nsentiment analysis\n3,892\nF1, Accuracy\nPublic\nFinanceES\nsentiment analysis\n7,980\nF1, Accuracy\nPublic\nTextual analysis:\nThis encompasses eight classification tasks for evaluating LLMs. 1) Sentiment\nanalysis focuses on extracting sentiment information (positive, negative, or neutral) from financial\ntexts, using three datasets: the Financial Phrase Bank (FPB) (Malo et al., 2014), FiQA-SA (Maia\net al., 2018), and TSA (Cortis et al., 2017). 2) News headline classification analyzes additional\ninformation, like price movements in financial texts, using the Headlines dataset (Sinha and Khandait,\n2021). 3) Hawkish-Dovish classification aims to classify sentences from monetary policy texts\nas \u2019hawkish\u2019 or \u2019dovish\u2019 focusing on the nuanced language and economic implications of financial\ntexts, using the FOMC (Shah et al., 2023a) dataset. 4) Argument unit classification categorizes\nsentences as claims or premises using the FinArg AUC dataset (Sy et al., 2023). 5) Argument\nrelation detection identifies relationships (attack, support, or irrelevant) between social media posts\nusing the FinArg ARC dataset (Sy et al., 2023). 6) Multi-class classification targets categorizing a\nvariety of financial texts, including analyst reports, news articles, and investor comments, utilizing\nthe MultiFin dataset (J\u00f8rgensen et al., 2023). 7) Deal completeness classification predicts if mergers\nand acquisitions events are \"completed\" or remain \"rumors\" based on news and tweets, employing\nthe MA dataset (Yang et al., 2020a). 8) ESG issue identification focuses on detecting Environmental,\nSocial, and Governance (ESG) concerns in financial documents using the MLESG dataset (Chen\net al., 2023a). For all datasets, evaluation utilizes the accuracy and F1 Score.\nQuestion answering.\nIt includes 4 datasets from three QA tasks, challenging LLMs to respond\nto financial queries. 1) Numerical QA focuses on solving questions through multi-step numerical\nreasoning with financial reports and tables, utilizing the FinQA (Chen et al., 2021) and TATQA (Zhu\net al., 2021) dataset. 2) Multi-turn QA is an extension of QA with multi-turn questions and answers\nbased on financial earnings reports and tables, using the ConvFinQA dataset (Chen et al., 2022b). F1\n5\nscore (Derczynski, 2016) and the Exact Match Accuracy (EM Accuracy) metric (Kim et al., 2023)\nare used to evaluate these tasks. 3) Long-form QA involves presenting models with complex, detailed\nquestions that require extensive and nuanced answers, often incorporating legal interpretations and\npractical applications. In our evaluation, we utilize our newly proposed Regulations dataset, which\nfocuses on intricate questions and answers related to financial regulations like EMIR. We assess the\nmodel responses using ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2019).\nText generation. This task assesses the models\u2019 ability to produce coherent and informative text.\nOur focus is on text summarization, utilizing the ECTSUM (Mukherjee et al., 2022) dataset for\nsummarizing earnings call transcripts. We also include EDTSUM, specifically designed for con-\ndensing financial news articles into concise summaries, constructed from original data in (Zhou\net al., 2021). Evaluation employs ROUGE (Lin, 2004), BERTScore (Zhang et al., 2019), and BART\nScore (Yuan et al., 2021) to measure alignment, factual consistency, and information retention\nbetween machine-generated and expert summaries.\nForecasting. The forecasting task challenges models to predict future market and investor behaviors\nfrom emerging patterns. We focus on the stock movement prediction task, forecasting stock directions\nas either positive or negative, based on historical prices and tweets. Three datasets are included:\nBigData22 (Soun et al., 2022), ACL18 (Xu and Cohen, 2018) and CIKM18 (Wu et al., 2018).\nRisk management. It challenges LLMs to accurately identify, extract, and analyze relevant risk-\nrelated information, interpret numerical data, and understand complex relationships. We include 4\ntasks: 1) Credit scoring classifies individuals as \"good\" or \"bad\" credit risks using historical customer\ndata, employing datasets including: German (Hofmann, 1994), Australia (Quinlan, [n. d.]) and\nLendingClub (Feng et al., 2023). 2) Fraud detection involve categorizes transactions as \"fraudulent\"\nor \"non-fraudulent\", using two datasets: ccf (Feng et al., 2023) and ccFraud (Feng et al., 2023). 3)\nFinancial distress identification aims to predict a company\u2019s bankruptcy risk, using the polish (Feng\net al., 2023) and taiwan dataset (Feng et al., 2023). Note that the dataset name describes only\nthe region of the company, and the content within the datasets is in English. 4) Claim analysis\nanonymizes client data for privacy, labeling a \"target\" to indicate claim status, using two datasets:\nPortoSeguro (Feng et al., 2023) and travelinsurance (Feng et al., 2023). It is noticed that the dataset\nname such as German and taiwan, only indicates customer sources and all content is in English. F1\nscore and Matthews correlation coefficient (MCC) (Chicco and Jurman, 2020) are used for evaluating\nthese tasks.\nDecision-making. Strategic decision-making (Punt, 2017) evaluates the model\u2019s proficiency in\nsynthesizing diverse information to formulate and implement trading strategies, a challenge even for\nexperts. We innovatively introduce the SOTA financial LLM agent FinMem (Yu et al., 2023, 2024)\nto evaluate LLMs on the stock trading task. We construct the novel FinTrade dataset, containing\n10 stocks, simulating real-world trading through historical prices, news, and sentiment analysis.\nPerformance is measured by Cumulative Return (CR) (Ariel, 1987), Sharpe Ratio (SR) (Sharpe,\n1998), Daily (DV) and Annualized volatility (AV) (Zhou et al., 2023), and Maximum Drawdown\n(MD) (Magdon-Ismail and Atiya, 2004), offering a comprehensive assessment of profitability, risk\nmanagement, and decision-making prowess.\nSpanish. Spanish financial datasets (Zhang et al., 2024) evaluate model performance in low-resource\nlanguage settings. We include six datasets in our analysis: TSA-ES (Zhang et al., 2024) and\nFinanceES (Zhang et al., 2024), both designed for sentiment analysis in the Spanish financial domain,\nwhere model performance is measured using F1 score. For multi-class classification, we utilize the\nSpanish subset of the MultiFin dataset (J\u00f8rgensen et al., 2023), with F1 score as the primary metric.\nThe EFP (Zhang et al., 2024) and EFPA (Zhang et al., 2024) datasets, focused on Spanish financial\nquestion-answering, are evaluated using F1 score to assess the accuracy of predicted answers. Finally,\nfor summarization tasks, the FNS-2023 (Zhang et al., 2024) dataset, which consists of Spanish\ncompany reports, is evaluated using ROUGE scores to measure the quality of generated summaries.\n3\nEvaluation\nWe evaluate the zero-shot (from our evaluation) and few-shots (results from previous papers) perfor-\nmance of 21 representative general LLMs and financial LLMs on the FinBen benchmark, including:\n1) ChatGPT: A LLM developed by OpenAI. 2) GPT-4 (OpenAI, 2023): The SOTA commercialized\nLLMs proposed by OpenAI. 3) Gemini Pro (Team et al., 2023): A multimodal LLM with 50T\n6\nparameters, released by Google. 4) LLaMA2-7/70B-chat (Touvron et al., 2023b): An open-sourced\ninstruction-following LLM with 7B and 70B parameters developed by MetaAI. 5) LLaMA3-8B8: An\nopen-sourced LLMs developed by MetaAI, using more training data than LLaMA2. 6) ChatGLM3-\n6B (Du et al., 2022): A conversational LLM with 6B parameters, jointly released by Zhipu AI and\nTsinghua KEG. 7) Baichuan2-6B (Baichuan, 2023): An open-source LLM with 6B parameters,\nlaunched by Baichuan Intelligent Technology. 8) InternLM-7B (Team, 2023): An open-sourced\n7B parameter base model tailored for practical scenarios, proposed by SenseTime. 9) Falcon-7B\n(Almazrouei et al., 2023): A 7B parameter causal decoder-only LLM model trained on 1500B\ntokens of RefinedWeb enhanced with curated corpora. 10) Mixtral 8\u00d77B (Jiang et al., 2024): A\nLLM with the Sparse Mixture of Experts (SMoE) architecture. 11) Code LLaMA-7B (Roziere\net al., 2023): An open-source LLM model for generating programming code, launched by Meta AI\nwith 7B parameters. 12) FinGPT (Yang et al., 2023a): A 7B instruction finetuned financial LLM\nbased on LLaMA 7B (Touvron et al., 2023a) with sentiment analysis tasks. 13) FinMA-7B (Xie\net al., 2023b): A 7B instruction finetuned financial LLM based on LLaMA 7B with multiple NLP\nand forecasting tasks. 14) DISC-FinLLM (Chen et al., 2023b): An open-sourced financial LLM,\nfine-tuned from Baichuan-13B-Chat (Baichuan, 2023). 15) CFGPT (Li et al., 2023a): An open-source\nLLM, specifically designed for the financial sector and trained on Chinese financial datasets, which\ncomprises 7B parameters. 16) Qwen2-7B/72B (qwe, 2024): Instruction-tuned LLMs developed by\nAlibaba Cloud with 7B and 72B parameters, optimized for financial and general NLP tasks. 17)\nXuanyuan-6B/70B (Zhang et al., 2023c): Instruction-tuned LLMs designed for financial NLP tasks\nwith 6B and 70B parameters. 18) LLaMA3.1-8B/70B (Dubey et al., 2024): LLaMA3 series models\nwith 8B and 70B parameters, fine-tuned with enhanced data for a wide range of NLP tasks.\nExperimental Settings We set the maximum generation tokens for LLMs to 1024 and the batch size\nto 20,000 for all experiments. These experiments are exclusively conducted on 16 NVIDIA A100\n80G GPUs, taking approximately 600 hours to complete. Including the GPT-4 API costs, the total\nexpenditure amounts to approximately $51,000.\n4\nResults\nTable 3 and Table 4 shows the performance of 14 representative LLMs on all datasets in the FinBen.\nWe also report results of non-LLM methods (traditional methods) in Appendix H.\n4.1\nInformation Extraction and Textual Analysis Results\nAs shown in Table 3, for IE tasks, GPT-4 demonstrates superior performance in named entity\nrecognition tasks, including NER, FINER-ORD, and FinRED. InternLM 7B achieves the best results\nin causal classification (SC). However, for more complex information extraction tasks, such as causal\ndetection (CD) and numerical understanding (FNXL and FSRL), even GPT-4\u2019s performance is limited,\nwith Gemini showing only slightly better results, still falling short of expectations. Additionally,\nwhile financial domain-specific LLMs developed by instruction tuning such as FinMA 7B exhibit\nimprovements over general domain LLMs such as LLaMA2 7B-chat, they continue to struggle with\nboth named entity recognition and complex extraction tasks. These findings highlight significant\nopportunities for advancement in financial causal detection and numerical understanding for LLMs.\nRegarding TA tasks, instruction fine-tuned models like FinMA 7B exhibit the best performance in\nsentiment analysis tasks, including FPB, FiQA-SA, and Headlines. However, the generalization\nability of FinMA 7B is limited due to the diversity of TA tasks in the financial domain. It performs\neven worse than general domain LLMs such as LLaMA2-7B-chat on other TA tasks, where GPT-4,\nGemini, and LLaMA2 70B show superior results. This underscores the limitations of instruction\nfine-tuned models, which may be constrained by the parameter size and ability of their base models.\nModels tailored for the Chinese language, such as CFGPT sft-7B-Full, which is fine-tuned on Chinese\nfinancial data, exhibit limited improvement on some datasets and even a decline in performance on\nothers like MultiFin compared to its base model InternLM 7B. This trend suggests a language-based\ndiscrepancy, indicating that fine-tuning with Chinese data may adversely affect performance on\nEnglish tasks. These findings underscore the complexities of cross-lingual adaptation in model\ntraining, highlighting the challenges in achieving consistent performance across different languages.\n8https://llama.meta.com/llama3/\n7\nTable 3: The zero-shot and few-shot performance of different LLMs in FinBen. All results via our\nevaluations are the average of three runs. \u201c-\u201d represents the result that is currently unable to yield due\nto model size or availability, and \u201c*\u201d represents the result from the previous paper.\nDataset\nMetrics\nChat\nGPT\nGPT\n4\nGemini\nLLaMA2\n7B-chat\nLLaMA2\n70B\nLLaMA3\n8B\nFinMA\n7B\nFinGPT\n7b-lora\nInternLM\n7B\nFalcon\n7B\nMixtral\n7B\nCFGPT\nsft-7B-Full\nNER\nEntityF1\n0.77*\n0.83*\n0.61\n0.18\n0.04\n0.08\n0.69\n0.00\n0.00\n0.00\n0.24\n0.00\nFINER-ORD\nEntityF1\n0.28\n0.77\n0.14\n0.02\n0.07\n0.00\n0.00\n0.00\n0.00\n0.00\n0.05\n0.00\nFinRED\nF1\n0.00\n0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nSC\nF1\n0.80\n0.81\n0.74\n0.85\n0.61\n0.69\n0.19\n0.00\n0.88\n0.67\n0.83\n0.15\nCD\nF1\n0.00\n0.01\n0.03\n0.00\n0.01\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nFNXL\nEntityF1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nFSRL\nEntityF1\n0.00\n0.01\n0.03\n0.00\n0.01\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nFPB\nF1\n0.78*\n0.78*\n0.77\n0.39\n0.73\n0.52\n0.88\n0.00\n0.69\n0.07\n0.29\n0.35*\nAcc\n0.78*\n0.76*\n0.77\n0.41\n0.72\n0.52\n0.88\n0.00\n0.69\n0.05\n0.37\n0.26*\nFiQA-SA\nF1\n0.60\n0.80\n0.81\n0.76\n0.83\n0.70\n0.79\n0.00\n0.81\n0.77\n0.16\n0.42*\nTSA\nRMSE\u2193\n0.53\n0.50\n0.37\n0.71\n0.57\n0.25\n0.80\n0.00\n0.29\n0.50\n0.50\n1.05\nHeadlines\nAvgF1\n0.77*\n0.86*\n0.78\n0.72\n0.63\n0.60\n0.97\n0.60\n0.60\n0.45\n0.60\n0.61*\nFOMC\nF1\n0.64\n0.71\n0.40\n0.35\n0.49\n0.40\n0.49\n0.00\n0.36\n0.30\n0.37\n0.16*\nAcc\n0.6\n0.69\n0.60\n0.49\n0.47\n0.41\n0.46\n0.00\n0.35\n0.30\n0.35\n0.21*\nFinArg-ACC\nMicroF1\n0.50\n0.60\n0.31\n0.46\n0.58\n0.51\n0.27\n0.00\n0.39\n0.23\n0.39\n0.05\nFinArg-ARC\nMicroF1\n0.39\n0.40\n0.60\n0.27\n0.36\n0.28\n0.08\n0.00\n0.33\n0.32\n0.57\n0.05\nMultiFin\nMicroF1\n0.59\n0.65\n0.62\n0.20\n0.63\n0.39\n0.14\n0.00\n0.34\n0.09\n0.37\n0.05\nMA\nMicroF1\n0.85\n0.79\n0.84\n0.70\n0.86\n0.34\n0.45\n0.00\n0.78\n0.39\n0.34\n0.25\nMLESG\nMicroF1\n0.25\n0.35\n0.34\n0.03\n0.31\n0.12\n0.00\n0.00\n0.14\n0.06\n0.17\n0.01\nFinQA\nEmAcc\n0.58*\n0.63*\n0.00\n0.00\n0.06\n0.00\n0.04\n0.00\n0.00\n0.00\n0.00\n0.00\nTATQA\nEmAcc\n0.00*\n0.13*\n0.18\n0.03\n0.01\n0.01\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\nRegulations\nRouge-1\n0.12\n0.11\n-\n0.24\n-\n0.10\n0.12\n0.01\n0.04\n0.03\n-\n0.14\nBertScore\n0.64\n0.62\n-\n0.65\n-\n0.60\n0.59\n0.40\n0.57\n0.14\n-\n0.57\nConvFinQA\nEmAcc\n0.60*\n0.76*\n0.43\n0.00\n0.25\n0.00\n0.20\n0.00\n0.00\n0.00\n0.31\n0.01\nEDTSUM\nRouge-1\n0.17\n0.20\n0.39\n0.17\n0.25\n0.14\n0.13\n0.00\n0.13\n0.15\n0.12\n0.01\nBertScore\n0.66\n0.67\n0.72\n0.62\n0.68\n0.60\n0.38\n0.52\n0.48\n0.57\n0.61\n0.51\nBartScore\n-3.64\n-3.62\n-3.87\n-3.99\n-3.81\n-4.94\n-5.71\n-7.23\n-4.60\n-6.1\n-4.47\n-7.08\nECTSUM\nRouge-1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nBertScore\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nBartScore\n-5.18\n-5.18\n-4.93\n-5.18\n-4.86\n-5.18\n-5.18\n-5.18\n-5.18\n--5.18\n-5.18\n-5.18\nBigData22\nAcc\n0.53\n0.54\n0.55\n0.54\n0.47\n0.55\n0.51\n0.45\n0.56\n0.55\n0.46\n0.45\nMCC\n-0.025\n0.03\n0.04\n0.05\n0.00\n0.02\n0.02\n0.00\n0.08\n0.00\n0.02\n0.03\nACL18\nAcc\n0.50\n0.52\n0.52\n0.51\n0.51\n0.52\n0.51\n0.49\n0.51\n0.51\n0.49\n0.48\nMCC\n0.005\n0.02\n0.04\n0.01\n0.01\n0.02\n0.03\n0.00\n0.02\n0.00\n0.00\n-0.03\nCIKM18\nAcc\n0.55\n0.57\n0.54\n0.55\n0.49\n0.57\n0.50\n0.42\n0.57\n0.47\n0.42\n0.41\nMCC\n0.01\n0.02\n0.02\n-0.03\n-0.07\n0.03\n0.08\n0.00\n-0.03\n-0.06\n-0.05\n-0.07\nGerman\nF1\n0.20\n0.55\n0.52\n0.57\n0.17\n0.56\n0.17\n0.52\n0.41\n0.23\n0.53\n0.53\nMCC\n-0.10\n-0.02\n0.00\n0.03\n0.00\n0.05\n0.00\n0.00\n-0.30\n-0.07\n0.00\n0.00\nAustralian\nF1\n0.41\n0.74\n0.26\n0.26\n0.41\n0.26\n0.41\n0.38\n0.34\n0.26\n0.26\n0.29\nMCC\n0.00\n0.47\n0.00\n0.00\n0.00\n0.00\n0.00\n0.11\n0.13\n0.00\n0.00\n-0.10\nLendingClub\nF1\n0.20\n0.55\n0.65\n0.72\n0.17\n0.10\n0.61\n0.00\n0.59\n0.02\n0.61\n0.05\nMCC\n-0.10\n-0.02\n0.19\n0.00\n0.00\n-0.15\n0.00\n0.00\n0.15\n-0.01\n0.08\n0.01\nccf\nF1\n0.20\n0.55\n0.96\n0.00\n0.17\n0.01\n0.00\n1.00\n1.00\n0.10\n0.00\n0.00\nMCC\n-0.10\n-0.02\n-0.01\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nccfraud\nF1\n0.20\n0.55\n0.90\n0.25\n0.17\n0.36\n0.01\n0.00\n0.57\n0.62\n0.48\n0.03\nMCC\n-0.10\n-0.02\n0.00\n-0.16\n0.00\n-0.03\n-0.06\n0.00\n-0.13\n-0.02\n0.16\n0.01\npolish\nF1\n0.20\n0.55\n0.86\n0.92\n0.17\n0.83\n0.92\n0.30\n0.92\n0.76\n0.92\n0.40\nMCC\n-0.10\n-0.02\n0.14\n0.00\n0.00\n-0.06\n-0.01\n0.00\n0.07\n0.05\n0.00\n-0.02\ntaiwan\nF1\n0.20\n0.55\n0.95\n0.95\n0.17\n0.26\n0.95\n0.60\n0.95\n0.00\n0.95\n0.70\nMCC\n-0.10\n-0.02\n0.00\n-0.01\n0.00\n-0.07\n0.00\n-0.02\n-0.01\n0.00\n0.00\n0.00\nportoseguro\nF1\n0.20\n0.55\n0.95\n0.01\n0.17\n0.94\n0.04\n0.96\n0.96\n0.95\n0.72\n0.00\nMCC\n-0.10\n-0.02\n0.00\n-0.05\n0.00\n-0.01\n0.01\n0.00\n0.00\n0.00\n0.01\n0.00\ntravelinsurance\nF1\n0.20\n0.55\n0.00\n0.00\n0.17\n0.00\n0.00\n0.98\n0.89\n0.77\n0.00\n0.03\nMCC\n-0.10\n-0.02\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.12\n-0.03\n0.00\n0.01\nMultiFin-ES\nACC\n0.48\n0.60\n0.23\n0.23\n0.11\n0.25\n0.09\n0.05\n0.13\n0.02\n0.43\n0.30\nF1\n0.47\n0.60\n0.14\n0.11\n0.12\n0.27\n0.12\n0.07\n0.17\n0.03\n0.42\n0.27\nEFP\nACC\n0.30\n0.27\n0.27\n0.27\n0.27\n0.35\n0.27\n0.27\n0.27\n0.24\n0.41\n0.27\nF1\n0.47\n0.19\n0.12\n0.12\n0.12\n0.21\n0.12\n0.12\n0.12\n0.20\n0.41\n0.14\nEFPA\nACC\n0.31\n0.34\n0.25\n0.26\n0.20\n0.35\n0.25\n0.26\n0.25\n0.23\n0.38\n0.32\nF1\n0.25\n0.27\n0.10\n0.10\n0.09\n0.21\n0.10\n0.10\n0.12\n0.22\n0.37\n0.18\nFinanceES\nACC\n0.13\n0.15\n0.29\n0.14\n0.20\n0.02\n0.12\n0.15\n0.13\n0.01\n0.30\n0.05\nF1\n0.08\n0.09\n0.16\n0.13\n0.23\n0.03\n0.16\n0.18\n0.20\n0.02\n0.30\n0.05\nTSA\nACC\n0.21\n0.47\n0.40\n0.07\n0.03\n0.04\n0.02\n0.06\n0.001\n0.02\n0.53\n0.07\nF1\n0.24\n0.46\n0.44\n0.04\n0.06\n0.07\n0.04\n0.10\n0.002\n0.04\n0.52\n0.05\nFNS\nRouge-1\n0.02\n0.19\n0.30\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.05\n0.02\nRouge-2\n0.04\n0.06\n0.06\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.05\n0.00\nRouge-L\n0.12\n0.13\n0.16\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.05\n0.02\n4.2\nQuestion Answering and Text Generation Results\nIn the QA tasks, closed-source commercial LLMs like GPT-4 and Gemini continue to lead across all\ndatasets. While FinMA 7B shows improvement over its base models, it remains limited by model\nsize and exhibits bottlenecks in numeric reasoning ability. For the regulations dataset, which is the\nfirst intersection dataset requiring both financial and legal knowledge, GPT-4 demonstrates its broad\nknowledge coverage effectively.\nIn the TG tasks, Gemini emerges as the frontrunner on the EDTSUM abstractive text summarization\ndataset, illustrating its prowess in generating coherent summaries. Nevertheless, all models face chal-\nlenges with extractive summarization, which demands the generation of precise label sequences for\n8\nsentences. Among open-source LLMs, LLaMA2 70B stands out in text summarization. Conversely,\nCFGPT sft-7B-Full consistently shows a decrease in performance compared to its foundational model,\nInternLM 7B.\n4.3\nForecasting and Risk Management Results\nFor forecasting, it is crucial to acknowledge that all LLMs fail to meet expected outcomes and lag\nbehind traditional methodologies. This consistent observation with existing studies Xie et al. (2023b)\nunderlines a notable deficiency in LLMs\u2019 capacity to tackle forecasting as effectively as traditional\nmethods. Even the best-performing models, such as GPT-4 and Gemini, only perform slightly better\nthan random guessing. This reveals significant potential for enhancement in LLMs, including industry\nleaders like GPT-4 and Gemini, particularly in forecasting tasks that demand complex reasoning\nabilities.\nIn RM tasks, such as credit scoring, fraud detection, and identifying financial distress, data often\nexhibit significant imbalances. Instances representing individuals with low credit scores, those prone\nto fraud, and companies at risk of financial distress constitute only a small percentage of the overall\ndataset. In such scenarios, LLMs with low instruction-following abilities (such as LLaMA2-7B-chat\nand LLaMA2-70B) tend to classify all cases into a single class, resulting in an MCC score of 0. These\ntasks, with tabular inputs and highly imbalanced distribution, pose a significant challenge for LLMs\nin the financial domain.\n4.4\nDecision Making Results\nThe comparative analysis of various LLMs on the complex task of stock trading, is presented in Table\n49. This task requires models to understand, summarize, and reason with multimodal financial data\n(texts and time series), leading to sophisticated trading decisions that necessitate a range of skills,\nfrom fundamental comprehension and summarization to reasoning and decision-making.\nAmong the evaluated LLMs, GPT-4 distinguishes itself by achieving the highest Sharpe Ratio (SR)\nover 1, indicating superior investment performance through optimal risk-return balance. It also\nrecords the minimal Maximum Drawdown (MDD), suggesting effective limitation of potential losses,\nthereby offering a more secure investment avenue compared to other models, including those using\nreinforcement learning methods like DQN, PPO, and A2C, which show significantly lower SR and\nhigher MDD.\nTables 4 and 10 reinforce these findings, highlighting GPT-4\u2019s exceptional performance in this\nchallenging domain. Additional results and analyses from these models in Table 5 contrast their\nperformances with the traditional Buy & Hold strategy, which considerably lags behind.\nTable 4: The average trading performance (95% Confidence Interval) comparison for different LLMs\nacross 10 stocks. The results include large LLMs only (\u226570B), as models with smaller contexts\nhave difficulty understanding the instructions and producing a static strategy of holding.\nModel\nCR (%)\u2191\nSR\u2191\nDV (%)\u2193\nAV (%)\u2193\nMD (%)\u2193\nBuy & Hold\n-4.00 \u00b1 22.39\n0.02 \u00b1 0.87\n3.59 \u00b1 1.34\n56.43 \u00b1 21.00\n30.67 \u00b1 17.48\nGPT-4\n28.19 \u00b1 25.27\n1.51 \u00b1 1.08\n2.52 \u00b1 1.30\n39.88 \u00b1 20.66\n18.34 \u00b1 9.77\nGPT-4o\n-5.54 \u00b1 19.12\n-0.19 \u00b1 0.84\n2.73 \u00b1 1.30\n43.62 \u00b1 20.67\n29.96 \u00b1 18.89\nGPT3.5-Turbo\n4.48 \u00b1 22.23\n0.15 \u00b1 0.82\n2.84 \u00b1 1.47\n45.39 \u00b1 23.35\n28.83 \u00b1 15.40\nllama2-70B\n4.02 \u00b1 24.65\n0.52 \u00b1 1.48\n2.18 \u00b1 1.28\n34.86 \u00b1 20.38\n25.55 \u00b1 16.83\nllama3-70B\n-2.57 \u00b1 22.63\n-0.04 \u00b1 1.19\n2.71 \u00b1 1.54\n43.42 \u00b1 24.65\n29.31 \u00b1 15.57\ngemini\n14.95 \u00b1 28.04\n1.03 \u00b1 1.24\n2.17 \u00b1 1.39\n34.67 \u00b1 22.23\n20.13 \u00b1 11.36\nIn contrast, ChatGPT exhibits significantly lower performance metrics, indicating limitations in its\nfinancial decision-making capabilities. Gemini, on the other hand, secures the position of second-best\nperformer, showcasing lower risk and volatility compared to GPT-4, yet maintaining commendable\nreturns. When considering open-source models, LLaMA-70B, despite its lower volatility, yields the\nleast profit among the LLMs, highlighting a trade-off between risk management and profitability.\n9For detailed trading performance, please see Appendix F\n9\nTable 5: Traditional model performances on stock trading.\nModel\nCumulative Return\nSharpe Ratio\nStandard Deviation\nAnnualized Volatility\nMax Drawdown\nA2C\n-4.2232\n-0.2586\n2.7522\n43.6898\n30.5819\nPPO\n-0.5586\n0.0085\n2.7531\n43.7048\n28.9496\nDQN\n-2.9924\n-0.1656\n2.7486\n43.6319\n31.78\nFor smaller models with parameters less than 70 billion, a marked inability to adhere to trading\ninstructions consistently across transactions is noted. This is attributed to their limited comprehension,\nextraction capabilities, and constrained context windows. This limitation underscores the critical chal-\nlenges smaller LLMs face in tasks requiring intricate financial reasoning and decision-making, thereby\nspotlighting the necessity for more advanced models to tackle decision making tasks effectively.\n4.5\nSpanish Results\nTable 3 presents the performance of various models on six Spanish financial datasets, highlighting\nsignificant language disparities. ChatGPT, GPT-4 and Gemini show limited performance compared\nwith English datasets. Mixtral 7B performs competitively, showing that the multilingual ability can\nimprove language-specific tasks. Smaller models, particularly from the LLaMA family, struggle with\ndomain complexities, reinforcing the importance of robust multilingual pretraining. While top models\nexcel in sentiment analysis, all models underperform in summarization tasks on FNS, stressing the\nneed for enhanced adaptation to specialized Spanish financial language.\n5\nConclusion\nIn this work, we present FinBen, a comprehensive benchmark specifically designed to evaluate\nLLMs in the financial domain. FinBen includes 42 diverse datasets spanning 24 tasks, meticulously\norganized to assess LLMs across eight critical aspects: information extraction, textual analysis,\nquestion answering, text generation, risk management, forecasting, decision-making, and Spanish.\nThis breadth of coverage sets FinBen apart from existing financial benchmarks, enabling a more\nrobust and nuanced evaluation of LLM capabilities. Our evaluation of 21 LLMs, including GPT-4,\nChatGPT, and Gemini, reveals their key advantages and limitations, highlighting directions for future\nwork. Looking ahead, FinBen continuously evolves into an open FinLLM leaderboard (Lin et al.,\n2024). We will incorporat additional languages and multimodal financial tasks (Yanglet and Deng,\n2024) and expand the range of financial tasks to further enhance its applicability and impact.\nOpenness: Our FinBen project follows the model openness framework (White et al., 2024) by\nproviding a comprehensive set of financial datasets and evalution codes under OSI-approved licenses.\nLimitations: We acknowledge several limitations that could impact FinBen\u2019s effectiveness and\napplicability. The restricted size of available datasets may affect the models\u2019 financial understanding\nand generalization across various contexts. Computational constraints limited our evaluation to the\nLLaMA 70B model, potentially overlooking the capabilities of larger models. Additionally, the tasks\nare based on American market data and English texts, which may limit the benchmark\u2019s applicability\nto global financial markets. Responsible usage and safeguards are essential to prevent potential\nmisuse, such as financial misinformation or unethical market influence10.\nEthical Statement: The authors take full responsibility for any potential legal issues arising from\nFinBen\u2019s development and dissemination. All data used are publicly available, non-personal, and\nshared under the MIT license, adhering to privacy and ethical guidelines. This manuscript and\nassociated materials are for academic and educational use only and do not provide financial, legal, or\ninvestment advice. The authors disclaim any liability for losses or damages from using the material,\nand users agree to seek professional consultation and indemnify the authors against any claims arising\nfrom its use11.\n10For a detailed limitation concerning this work, please see Appendix.\n11For a detailed ethical and legal statement concerning this work, please see Appendix.\n10\nAcknowledgements\nThe authors acknowledge UFIT Research Computing, NVAITC, and HPG for providing computa-\ntional resources and support that have contributed to the research results reported in this publication.\nURL: http://www.rc.ufl.edu. This work is supported by the project JPNP20006 from New Energy\nand Industrial Technology Development Organization (NEDO). This work has also been partially\nsupported by project MIS 5154714 of the National Recovery and Resilience Plan Greece 2.0 funded\nby the European Union under the Next Generation EU Program. Additionally, we gratefully acknowl-\nedge FINOS (Fintech Open Source Foundation) for supporting the Open Financial LLM Leaderboard\ninitiative. Xiao-Yang Liu acknowledges the support from NSF IUCRC CRAFT center research\ngrant (CRAFT Grant 22017) for this research. The opinions expressed in this publication do not\nnecessarily represent the views of NSF IUCRC CRAFT. Haoqiang Kang and Xiao-Yang Liu also\nacknowledge the support from Columbia\u2019s SIRS and STAR Program, The Tang Family Fund for\nResearch Innovations in FinTech, Engineering, and Business Operations.\nReferences\n2024. Qwen2 Technical Report. (2024).\nYaser S Abu-Mostafa and Amir F Atiya. 1996. Introduction to financial forecasting. Applied\nintelligence 6 (1996), 205\u2013213.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojo-\ncaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al.\n2023. The falcon series of open language models. arXiv preprint arXiv:2311.16867 (2023).\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. 2015. Domain adaption of\nnamed entity recognition to support credit risk assessment. In Proceedings of the Australasian\nLanguage Technology Association Workshop 2015. 84\u201390.\nDogu Araci. 2019. FinBERT: Financial Sentiment Analysis with Pre-trained Language Models.\narXiv:1908.10063 [cs.CL]\nRobert A Ariel. 1987. A monthly effect in stock returns. Journal of financial economics 18, 1 (1987),\n161\u2013174.\nSaqib Aziz and Michael Dowling. 2019. Machine learning and AI for risk management. Springer\nInternational Publishing.\nBaichuan. 2023. Baichuan 2: Open Large-scale Language Models. arXiv preprint arXiv:2309.10305\n(2023). https://arxiv.org/abs/2309.10305\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models\nare few-shot learners. Advances in Neural Information Processing Systems 33 (2020), 1877\u20131901.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712 (2023).\nLongbing Cao. 2022. Ai in finance: challenges, techniques, and opportunities. ACM Computing\nSurveys (CSUR) 55, 3 (2022), 1\u201338.\nChung-Chi Chen, Yu-Min Tseng, Juyeon Kang, Ana\u00efs Lhuissier, Min-Yuh Day, Teng-Tsai Tu,\nand Hsin-Hsi Chen. 2023a. Multi-Lingual ESG Issue Identification. In Proceedings of the Fifth\nWorkshop on Financial Technology and Natural Language Processing and the Second Multimodal\nAI For Financial Forecasting. 111\u2013115.\nWei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu, Bingxuan Li, Siyuan Wang,\nJiarong Xu, Xiang Bai, Xuanjing Huang, et al. 2023b. Disc-finllm: A chinese financial large\nlanguage model based on multiple experts fine-tuning. arXiv preprint arXiv:2310.15205 (2023).\n11\nZhiyu Chen, Wenhu Chen, Charese Smiley, and et al. Sameena Shah. 2022a. FinQA: A Dataset of\nNumerical Reasoning over Financial Data. arXiv:2109.00122 [cs.CL]\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema\nMoussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al. 2021. FinQA: A Dataset of\nNumerical Reasoning over Financial Data. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing. 3697\u20133711.\nZhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang.\n2022b. ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance\nQuestion Answering. arXiv:2210.03849 [cs.CL]\nDavide Chicco and Giuseppe Jurman. 2020. The advantages of the Matthews correlation coefficient\n(MCC) over F1 score and accuracy in binary classification evaluation. BMC genomics 21, 1 (2020),\n1\u201313.\nKeith Cortis, Andr\u00e9 Freitas, Tobias Daudert, Manuela Huerlimann, Manel Zarrouk, Siegfried Hand-\nschuh, and Brian Davis. 2017. Semeval-2017 task 5: Fine-grained sentiment analysis on financial\nmicroblogs and news. In Proceedings of the 11th international workshop on semantic evaluation\n(SemEval-2017). 519\u2013535.\nMarco Costantino and Paolo Coletti. 2008. Information extraction in finance. Vol. 8. Wit Press.\nYongfu Dai, Duanyu Feng, Jimin Huang, Haochen Jia, Qianqian Xie, Yifang Zhang, Weiguang Han,\nWei Tian, and Hao Wang. 2024. LAiW: A Chinese Legal Large Language Models Benchmark.\narXiv:2310.05620 [cs.CL]\nLeon Derczynski. 2016. Complementarity, F-score, and NLP Evaluation. In Proceedings of the Tenth\nInternational Conference on Language Resources and Evaluation (LREC\u201916), Nicoletta Calzolari,\nKhalid Choukri, Thierry Declerck, Sara Goggi, Marko Grobelnik, Bente Maegaard, Joseph Mariani,\nHelene Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (Eds.). European Language\nResources Association (ELRA), Portoro\u017e, Slovenia, 261\u2013266.\nhttps://aclanthology.org/\nL16-1040\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022.\nGLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings\nof the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers). 320\u2013335.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha\nLetman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of\nmodels. arXiv preprint arXiv:2407.21783 (2024).\nDuanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Zhengyu\nChen, Alejandro Lopez-Lira, and Hao Wang. 2024. Empowering Many, Biasing a Few: Generalist\nCredit Scoring through Large Language Models. arXiv:2310.00566 [cs.LG]\nDuanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Alejandro\nLopez-Lira, and Hao Wang. 2023. Empowering many, biasing a few: Generalist credit scoring\nthrough large language models. arXiv preprint arXiv:2310.00566 (2023).\nCyril Goutte and Eric Gaussier. 2005. A probabilistic interpretation of precision, recall and F-\nscore, with implication for evaluation. In European conference on information retrieval. Springer,\n345\u2013359.\nWeiguang Han, Jimin Huang, Qianqian Xie, Boyi Zhang, Yanzhao Lai, and Min Peng. 2023a.\nMastering Pair Trading with Risk-Aware Recurrent Reinforcement Learning. arXiv:2304.00364 [q-\nfin.CP]\nWeiguang Han, Boyi Zhang, Qianqian Xie, Min Peng, Yanzhao Lai, and Jimin Huang. 2023b. Select\nand Trade: Towards Unified Pair Trading with Hierarchical Reinforcement Learning. arXiv\npreprint arXiv:2301.10724 (2023).\n12\nHans Hofmann. 1994. Statlog (German Credit Data). UCI Machine Learning Repository. DOI:\nhttps://doi.org/10.24432/C5NC77.\nDong Hongyuan, Che Wanxiang, He Xiaoyu, Zheng Guidong, and Wen Junjie. 2023. FinBART: A\nPre-trained Seq2seq Language Model for Chinese Financial Tasks. In Proceedings of the 22nd\nChinese National Conference on Computational Linguistics, Maosong Sun, Bing Qin, Xipeng Qiu,\nJing Jiang, and Xianpei Han (Eds.). Chinese Information Processing Society of China, Harbin,\nChina, 906\u2013917. https://aclanthology.org/2023.ccl-1.77\nGang Hu, Ke Qin, Chenhan Yuan, Min Peng, Alejandro Lopez-Lira, Benyou Wang, Sophia Anani-\nadou, Wanlong Yu, Jimin Huang, and Qianqian Xie. 2024. No Language is an Island: Unifying\nChinese and English in Financial Large Language Models, Instruction Data, and Benchmarks.\narXiv preprint arXiv:2403.06249 (2024).\nJiajia Huang, Haoran Zhu, Chao Xu, Tianming Zhan, Qianqian Xie, and Jimin Huang. 2024. Au-\nditWen: An Open-Source Large Language Model for Audit. arXiv preprint arXiv:2410.10873\n(2024).\nPranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen.\n2023. FinanceBench: A New Benchmark for Financial Question Answering. arXiv preprint\narXiv:2311.11944 (2023).\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris\nBamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.\n2024. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024).\nRasmus J\u00f8rgensen, Oliver Brandt, Mareike Hartmann, Xiang Dai, Christian Igel, and Desmond\nElliott. 2023. MultiFin: A Dataset for Multilingual Financial NLP. In Findings of the Association\nfor Computational Linguistics: EACL 2023. 864\u2013879.\nKisub Kim, Xin Zhou, Dongsun Kim, Julia Lawall, Kui Liu, Tegawend\u00e9 F Bissyand\u00e9, Jacques Klein,\nJaekwon Lee, and David Lo. 2023. How are We Detecting Inconsistent Method Names? An\nEmpirical Study from Code Review Perspective. arXiv preprint arXiv:2308.12701 (2023).\nRik Koncel-Kedziorski, Michael Krumdick, Viet Lai, Varshini Reddy, Charles Lovering, and Chris\nTanner. 2023. Bizbench: A quantitative reasoning benchmark for business and finance. arXiv\npreprint arXiv:2311.06602 (2023).\nMoreno La Quatra and Luca Cagliero. 2020. End-to-end training for financial report summarization.\nIn Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing\nFinancial Summarisation. 118\u2013123.\nMatthew Lamm, Arun Tejasvi Chaganty, Christopher D Manning, Dan Jurafsky, and Percy Liang.\n2018. Textual analogy parsing: What\u2019s shared and what\u2019s compared among analogous facts. arXiv\npreprint arXiv:1809.02700 (2018).\nJean Lee, Nicholas Stevens, Soyeon Caren Han, and Minseok Song. 2024. A Survey of Large\nLanguage Models in Finance (FinLLMs). arXiv:2402.02315 [cs.CL]\nYang Lei, Jiangtong Li, Ming Jiang, Junjie Hu, Dawei Cheng, Zhijun Ding, and Changjun Jiang.\n2023.\nCFBenchmark: Chinese Financial Assistant Benchmark for Large Language Model.\narXiv:2311.05812 [cs.CL]\nJiangtong Li, Yuxuan Bian, Guoxuan Wang, Yang Lei, Dawei Cheng, Zhijun Ding, and\nChangjun Jiang. 2023a.\nCFGPT: Chinese Financial Assistant with Large Language Model.\narXiv:2309.10654 [cs.CL]\nXianzhi Li, Xiaodan Zhu, Zhiqiang Ma, Xiaomo Liu, and Sameena Shah. 2023c. Are ChatGPT and\nGPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical\nTasks. arXiv preprint arXiv:2305.05862 (2023).\nYinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023b. Large Language Models in Finance:\nA Survey. arXiv:2311.10723 [q-fin.GN]\n13\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out. 74\u201381.\nShengyuan Colin Lin, Keyi Wang Felix Tian, Xingjian Zhao, Jimin Huang, Qianqian Xie, Luca\nBorella, Matt White, Christina Dan Wang, Kairong Xiao, Xiao-Yang Liu Yanglet, and Li Deng.\n2024. Open FinLLM Leaderboard: Towards Financial AI Readiness. International Workshop on\nMultimodal Financial Foundation Models (MFFMs), ACM ICAIF (2024).\nXiao-Yang Liu, Guoxuan Wang, and Daochen Zha. 2023a. Data-Centric FinGPT: Democratizing\nInternet-scale data for financial large language models. Workshop on Instruction Tuning and\nInstruction Following, NeurIPS (2023).\nXiao-Yang Liu, Ziyi Xia, Jingyang Rui, Jiechao Gao, Hongyang Yang, Ming Zhu, Christina Dan\nWang, Zhaoran Wang, and Jian Guo. 2022. FinRL-Meta: Market Environments and Benchmarks\nfor Data-Driven Financial Reinforcement Learning. NeurIPS, Special Track on Datasets and\nBenchmarks (2022).\nXiao-Yang Liu, Ziyi Xia, Hongyang Yang, Jiechao Gao, Daochen Zha, Ming Zhu, Christina Dan\nWang, Zhaoran Wang, and Jian Guo. 2023b. Dynamic Datasets and Market Environments for\nFinancial Reinforcement Learning. Machine Learning Journal, Springer Nature (2023).\nXiao-Yang Liu, Jie Zhang, Guoxuan Wang, Weiqing Tong, and Anwar Walid. 2024a. FinGPT-HPC:\nEfficient Pretraining and Finetuning Large Language Models for Financial Applications with\nHigh-Performance Computing. arXiv preprint arXiv:2402.13533 (2024).\nZhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li, and Jun Zhao. 2020. FinBERT: A Pre-trained\nFinancial Language Representation Model for Financial Text Mining. In Proceedings of the Twenty-\nNinth International Joint Conference on Artificial Intelligence, IJCAI-20, Christian Bessiere (Ed.).\nInternational Joint Conferences on Artificial Intelligence Organization, 4513\u20134519. Special Track\non AI in FinTech.\nZhiwei Liu, Xin Zhang, Kailai Yang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024b.\nFMDLlama: Financial Misinformation Detection based on Large Language Models. arXiv preprint\narXiv:2409.16452 (2024).\nAlejandro Lopez-Lira and Yuehua Tang. 2023. Can chatgpt forecast stock price movements? return\npredictability and large language models. arXiv preprint arXiv:2304.07619 (2023).\nTim Loughran and Bill McDonald. 2020. Textual analysis in finance. Annual Review of Financial\nEconomics 12 (2020), 357\u2013375.\nMalik Magdon-Ismail and Amir F Atiya. 2004. Maximum drawdown. Risk Magazine 17, 10 (2004),\n99\u2013102.\nMacedo Maia, Siegfried Handschuh, Andre Freitas, Brian Davis, Ross McDermott, Manel Zarrouk,\nand Alexandra Balahur. 2018. WWW\u201918 Open Challenge: Financial Opinion Mining and Question\nAnswering. WWW \u201918: Companion Proceedings of the The Web Conference 2018, 1941\u20131942.\nPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. 2014. Good debt\nor bad debt: Detecting semantic orientations in economic texts. Journal of the Association for\nInformation Science and Technology 65, 4 (2014), 782\u2013796.\nDominique Mariko, Hanna Abi Akl, Estelle Labidurie, Stephane Durfort, Hugues De Mazancourt,\nand Mahmoud El-Haj. 2020. Financial document causality detection shared task (fincausal 2020).\narXiv preprint arXiv:2012.02505 (2020).\nRajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen\nShaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, et al. 2022.\nEctsum: A new benchmark dataset for bullet point summarization of long earnings call transcripts.\narXiv preprint arXiv:2210.12467 (2022).\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n14\nFelipe Dias Paiva, Rodrigo Tom\u00e1s Nogueira Cardoso, Gustavo Peixoto Hanaoka, and Wendel Moreira\nDuarte. 2019. Decision-making for financial trading: A fusion approach of machine learning and\nportfolio selection. Expert Systems with Applications 115 (2019), 635\u2013655.\nAndr\u00e9 E Punt. 2017. Strategic management decision-making in a complex world: quantifying,\nunderstanding, and using trade-offs. ICES Journal of Marine Science 74, 2 (2017), 499\u2013510.\nRoss Quinlan. [n. d.]. Statlog (Australian Credit Approval). UCI Machine Learning Repository. DOI:\nhttps://doi.org/10.24432/C59012.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023. Code llama: Open foundation models for\ncode. arXiv preprint arXiv:2308.12950 (2023).\nJulio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. 2015. Domain Adaption\nof Named Entity Recognition to Support Credit Risk Assessment. In Proceedings of the Aus-\ntralasian Language Technology Association Workshop 2015, Ben Hachey and Kellie Webster\n(Eds.). Parramatta, Australia, 84\u201390. https://aclanthology.org/U15-1010\nAgam Shah, Suvan Paturi, and Sudheer Chava. 2023a. Trillion Dollar Words: A New Financial\nDataset, Task & Market Analysis. In Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), Anna Rogers, Jordan Boyd-Graber, and\nNaoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 6664\u20136679.\nAgam Shah, Ruchit Vithani, Abhinav Gullapalli, and Sudheer Chava. 2023b. Finer: Financial named\nentity recognition dataset and weak-supervision model. arXiv preprint arXiv:2302.11157 (2023).\nRaj Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman,\nCharese Smiley, Jiaao Chen, and Diyi Yang. 2022. When FLUE Meets FLANG: Benchmarks and\nLarge Pretrained Language Model for Financial Domain. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing. 2322\u20132335.\nSoumya Sharma, Subhendu Khatuya, Manjunath Hegde, Afreen Shaikh, Koustuv Dasgupta, Pawan\nGoyal, and Niloy Ganguly. 2023. Financial Numeric Extreme Labelling: A dataset and bench-\nmarking. In Findings of the Association for Computational Linguistics: ACL 2023. 3550\u20133561.\nSoumya Sharma, Tapas Nayak, Arusarka Bose, Ajay Kumar Meena, Koustuv Dasgupta, Niloy\nGanguly, and Pawan Goyal. 2022. FinRED: A dataset for relation extraction in financial domain.\nIn Companion Proceedings of the Web Conference 2022. 595\u2013597.\nWilliam F Sharpe. 1998. The sharpe ratio. Streetwise\u2013the Best of the Journal of Portfolio Management\n3 (1998), 169\u201385.\nAnkur Sinha and Tanmay Khandait. 2021. Impact of news on the commodity market: Dataset\nand results. In Advances in Information and Communication: Proceedings of the 2021 Future of\nInformation and Communication Conference (FICC), Volume 2. Springer, 589\u2013601.\nYejun Soun, Jaemin Yoo, Minyong Cho, Jihyeong Jeon, and U Kang. 2022. Accurate Stock Movement\nPrediction with Self-supervised Learning from Sparse Noisy Tweets. In 2022 IEEE International\nConference on Big Data (Big Data). IEEE, 1691\u20131700.\nEugene Sy, Tzu-Cheng Peng, Shih-Hsuan Huang, Heng-Yu Lin, and Yung-Chun Chang. 2023. Fine-\nGrained Argument Understanding with BERT Ensemble Techniques: A Deep Dive into Financial\nSentiment Analysis. In Proceedings of the 35th Conference on Computational Linguistics and\nSpeech Processing (ROCLING 2023). 242\u2013249.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu\nSoricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805 (2023).\nInternLM Team. 2023. Internlm: A multilingual language model with progressively enhanced\ncapabilities.\n15\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023b. Llama 2: Open\nfoundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\nMatt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang Yanglet Liu, Ahmed Abdelmonsef, Sachin\nVarghese, and Arnaud Le Hors. 2024. The model openness framework: Promoting completeness\nand openness for reproducibility, transparency and usability in Artificial Intelligence. arXiv\npreprint arXiv:2403.13784 (2024).\nHuizhe Wu, Wei Zhang, Weiwei Shen, and Jun Wang. 2018. Hybrid deep sequential modeling for\nsocial text-driven stock prediction. In Proceedings of the 27th ACM international conference on\ninformation and knowledge management. 1627\u20131630.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhan-\njan Kambadur, David Rosenberg, and Gideon Mann. 2023. BloombergGPT: A Large Language\nModel for Finance. arXiv:2303.17564 [cs.LG]\nQianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. 2023a. The Wall Street\nNeophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction\nChallenges. arXiv preprint arXiv:2304.05351 (2023).\nQianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin\nHuang. 2023b. PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark\nfor Finance. Advances in Neural Information Processing Systems, Special Track on Datasets and\nBenchmarks (2023).\nQianqian Xie, Dong Li, Mengxi Xiao, Zihao Jiang, Ruoyu Xiang, Xiao Zhang, Zhengyu Chen, Yueru\nHe, Weiguang Han, Yuzhe Yang, et al. 2024. Open-FinLLMs: Open multimodal large language\nmodels for financial applications. arXiv preprint arXiv:2408.11878 (2024).\nYumo Xu and Shay B Cohen. 2018. Stock movement prediction from tweets and historical prices. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers). 1970\u20131979.\nHongyang Yang, Xiao-Yang Liu, and Christina Dan Wang. 2023a. FinGPT: Open-Source Financial\nLarge Language Models. Symposium on FinLLM, IJCAI 2023 (2023).\nLinyi Yang, Eoin M Kenny, Tin Lok James Ng, Yi Yang, Barry Smyth, and Ruihai Dong. 2020a. Gen-\nerating plausible counterfactual explanations for deep transformers in financial text classification.\narXiv preprint arXiv:2010.12512 (2020).\nYi Yang, Yixuan Tang, and Kar Yan Tam. 2023b. InvestLM: A Large Language Model for Investment\nusing Financial Domain Instruction Tuning. arXiv:2309.13064 [q-fin.GN]\nYi Yang, Mark Christopher Siy UY, and Allen Huang. 2020b. FinBERT: A Pretrained Language\nModel for Financial Communications. arXiv:2006.08097 [cs.CL]\nYuzhe Yang, Yifei Zhang, Yan Hu, Yilin Guo, Ruoli Gan, Yueru He, Mingcong Lei, Xiao Zhang,\nHaining Wang, Qianqian Xie, et al. 2024. UCFE: A User-Centric Financial Expertise Benchmark\nfor Large Language Models. arXiv preprint arXiv:2410.14059 (2024).\nXiao-Yang Liu Yanglet and Li Deng. 2024. Multimodal Financial Foundation Models (MFFMs):\nProgress, Prospects, and Challenges. International Workshop on Multimodal Financial Foundation\nModels (MFFMs) at 5th ACM International Conference on AI in Finance (MFFM at ICAIF \u201924),\n(2024).\nYangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu, Jordan W.\nSuchow, and Khaldoun Khashanah. 2023. FinMem: A Performance-Enhanced LLM Trading\nAgent with Layered Memory and Character Design. arXiv:2311.13743 [q-fin.CP]\n16\nYangyang Yu, Zhiyuan Yao, Haohang Li, Zhiyang Deng, Yupeng Cao, Zhi Chen, Jordan W Suchow,\nRong Liu, Zhenyu Cui, Denghui Zhang, et al. 2024. FinCon: A Synthesized LLM Multi-Agent\nSystem with Conceptual Verbal Reinforcement for Enhanced Financial Decision Making. arXiv\npreprint arXiv:2407.06567 (2024).\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text\ngeneration. Advances in Neural Information Processing Systems 34 (2021), 27263\u201327277.\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi\nHu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2023a. Instruction Tuning for Large Language\nModels: A Survey. arXiv:2308.10792 [cs.CL]\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore:\nEvaluating text generation with bert. arXiv preprint arXiv:1904.09675 (2019).\nXuanyu Zhang, Bingbing Li, and Qing Yang. 2023b. CGCE: A Chinese Generative Chat Evaluation\nBenchmark for General and Financial Domains. arXiv:2305.14471 [cs.CL]\nXiao Zhang, Ruoyu Xiang, Chenhan Yuan, Duanyu Feng, Weiguang Han, Alejandro Lopez-Lira,\nXiao-Yang Liu, Sophia Ananiadou, Min Peng, Jimin Huang, and Qianqian Xie. 2024. D\u00f3lares\nor Dollars? Unraveling the Bilingual Prowess of Financial LLMs Between Spanish and English.\narXiv:2402.07405 [cs.CL]\nXuanyu Zhang, Qing Yang, and Dongliang Xu. 2023c. XuanYuan 2.0: A Large Chinese Financial\nChat Model with Hundreds of Billions Parameters. arXiv:2305.12002 [cs.CL]\nHuaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing\nDai, Lin Zhao, Gengchen Mai, et al. 2024b. Revolutionizing finance with llms: An overview of\napplications and insights. arXiv preprint arXiv:2401.11641 (2024).\nYilun Zhao, Hongjun Liu, Yitao Long, Rui Zhang, Chen Zhao, and Arman Cohan. 2024a. Finance-\nMATH: Knowledge-Intensive Math Reasoning in Finance Domains. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-\nWei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\nBangkok, Thailand, 12841\u201312858. https://aclanthology.org/2024.acl-long.693\nYilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru\nTang, Rui Zhang, and Arman Cohan. 2024c. DocMath-Eval: Evaluating Math Reasoning Capa-\nbilities of LLMs in Understanding Long and Specialized Documents. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Lun-\nWei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\nBangkok, Thailand, 16103\u201316120. https://aclanthology.org/2024.acl-long.852\nXianzheng Zhou, Hui Zhou, and Huaigang Long. 2023. Forecasting the equity premium: Do deep\nneural network models work? Modern Finance 1, 1 (2023), 1\u201311.\nZhihan Zhou, Liqian Ma, and Han Liu. 2021. Trade the Event: Corporate Events Detection for\nNews-Based Event-Driven Trading. arXiv:2105.12825 [cs.CL]\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng,\nand Tat-Seng Chua. 2021. TAT-QA: A question answering benchmark on a hybrid of tabular and\ntextual content in finance. arXiv preprint arXiv:2105.07624 (2021).\n17\nA\nContributions\nScience Leadership: Qianqian Xie, Min Peng, Sophia Ananiadou, Alejandro Lopez-Lira, Hao Wang,\nYanzhao Lai, Benyou Wang, Xiao-Yang Liu, Gang Hu, Jiajia Huang, Jimin Huang.\nContributors: Mengxi Xiao, Dong Li, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang,\nYueru He, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang Kang, Ziyan Kuang, Chenhan Yuan,\nKailai Yang, Zheheng Luo, Tianlin Zhang, Zhiwei Liu, Guojun Xiong, Zhiyang Deng, Yuechen Jiang,\nZhiyuan Yao, Haohang Li, Yangyang Yu\nB\nFintrade Dataset\nTable 6: Summary of FinTrade dataset statistics.\nTicker\nNumber of News\nNumber of 10-K/10-Q Files\nNumerical Price Data\nTSLA\n3,233\n8\n497\nNFLX\n965\n8\n497\nAMZN\n1,675\n8\n497\nMSFT\n1,362\n8\n497\nAAPL\n2,082\n8\n497\nGOOG\n1,144\n7\n497\nDIS\n1,445\n9\n497\nGM\n2,252\n9\n497\nNIO\n957\n0\n497\nCOIN\n1,022\n0\n497\nC\nOther LLMs Performance\nTable 7 presents other LLMs\u2019 performance in the FinBen.\nTable 7: The zero-shot and few-shots performance of other LLMs on the FinBen.\nDataset\nMetrics\nBaichuan\n7B\nCodeLLaMA\n7B\nDISC-\nFinLLM\nChatGLM3\n6B\nQwen2\n7B\nXuanyuan\n6B\nQwen2\n72B\nXuanyuan\n70B\nLLaMA3.1\n8B\nLLaMA3.1\n70B\nNER\nEntityF1\n0.00\n0.07\n0.12\n0.25\n0.07\n0.06\n0.02\n0.08\n0.14\n0.05\nFINER-ORD\nEntityF1\n0.00\n0.00\n0.00\n0.02\n0.02\n0.02\n0.02\n0.33\n0.12\n0.18\nFinRED\nF1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n0.00\nSC\nF1\n0.74\n0.85\n0.00\n0.81\n0.60\n0.70\n0.82\n0.23\n0.83\n0.87\nCD\nF1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n0.00\n0.00\nFNXL\nEntityF1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nFSRL\nEntityF1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n0.00\n0.00\nFPB\nF1\n0.17\n0.34\n0.29\n0.74\n0.52\n0.74\n0.75\n0.52\n0.76\n0.79\nAcc\n0.23\n0.39\n0.26\n0.74\n0.52\n0.75\n0.74\n0.55\n0.75\n0.79\nFiQA-SA\nF1\n0.32\n0.66\n0.32\n0.56\n0.57\n0.56\n0.63\n0.82\n0.75\n0.74\nTSA\nRMSE\u2193\n0.44\n0.43\n0.32\n0.35\n0.43\n0.33\n0.30\n0.54\n0.17\n0.42\nHeadlines\nAvgF1\n0.60\n0.60\n0.60\n0.66\n0.60\n0.65\n0.60\n0.73\n0.60\n0.60\nFOMC\nF1\n0.17\n0.14\n0.19\n0.47\n0.63\n0.45\n0.65\n0.60\n0.48\n0.64\nAcc\n0.25\n0.27\n0.28\n0.46\n0.64\n0.51\n0.66\n0.61\n0.56\n0.67\nFinArg-ACC\nMicroF1\n0.36\n0.28\n0.29\n0.25\n0.43\n0.47\n0.57\n0.58\n0.53\n0.65\nFinArg-ARC\nMicroF1\n0.27\n0.25\n0.29\n0.50\n0.53\n0.60\n0.63\n0.67\n0.55\n0.55\nMultiFin\nMicroF1\n0.12\n0.21\n0.29\n0.47\n0.39\n0.54\n0.55\n0.63\n0.62\n0.69\nM&A\nMicroF1\n0.33\n0.54\n0.29\n0.79\n0.83\n0.84\n0.84\n0.79\n0.85\n0.84\nMLESG\nMicroF1\n0.04\n0.10\n0.29\n0.16\n0.34\n0.24\n0.43\n0.26\n0.31\n0.44\nFinQA\nEmAcc\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nTATQA\nEmAcc\n0.00\n0.00\n0.00\n0.07\n0.00\n0.11\n0.00\n0.02\n0.04\n0.44\nRegulations\nRouge-1\n0.13\n-\n-\n0.26\n0.31\n0.24\n0.31\n0.30\n0.27\n0.10\nBertScore\n0.60\n-\n-\n0.65\n0.68\n0.64\n0.69\n0.67\n0.65\n0.61\nConvFinQA\nEmAcc\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.01\n0.00\n0.00\n0.00\nEDTSUM\nRouge-1\n0.02\n0.10\n0.22\n0.13\n0.22\n0.24\n0.22\n0.25\n0.20\n0.18\nBertScore\n0.47\n0.67\n0.61\n0.47\n0.67\n0.66\n0.67\n0.68\n0.64\n0.63\nECTSUM\nRouge-1\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nBertScore\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\n0.00\nBigData22\nAcc\n0.53\n0.52\n0.44\n0.47\n0.55\n0.50\n0.56\n0.56\n0.54\n0.45\nMCC\n-0.01\n-0.01\n-0.05\n0.00\n0.01\n0.00\n0.05\n0.06\n0.03\n-0.00\nACL18\nAcc\n0.50\n0.51\n0.50\n0.50\n0.50\n0.51\n0.50\n0.49\n0.52\n0.49\nMCC\n-0.01\n0.00\n0.02\n0.02\n-0.02\n0.01\n-0.01\n-0.02\n0.05\n0.03\nCIKM18\nAcc\n0.48\n0.51\n0.44\n0.42\n0.52\n0.50\n0.55\n0.50\n0.57\n0.44\nMCC\n0.02\n0.02\n-0.03\n0.02\n-0.03\n-0.03\n-0.01\n0.00\n-0.01\n0.02\n18\nD\nInstructions\nFor detail instruction of each dataset, please see Table 8 and Table 9.\nE\nRelated Work\nE.1\nFinancial Large Language Models\nRecent years have seen a significant surge in research on finance-specific LLMs, expanding on the\ngroundwork laid by general-purpose language models (Lee et al., 2024; Liu et al., 2023b; Xie et al.,\n2023a; Zhang et al., 2024; Dai et al., 2024; Xie et al., 2024). Financial pre-trained language models\n(FinPLMs) like FinBERT (Araci, 2019; Yang et al., 2020b; Liu et al., 2020), derived from BERT, and\nFLANG (Shah et al., 2022), based on ELECTRA, have been developed using domain-specific data\nfor enhanced performance in tasks like sentiment analysis and stock prediction. The open-source\nrelease of Meta AI\u2019s LLaMA (Touvron et al., 2023a,b) has fueled further innovation in Financial\nLLMs (FinLLMs), with models like FinMA (Xie et al., 2023b), InvestLM (Yang et al., 2023b), and\nFinGPT (Liu et al., 2023a, 2024a; Yang et al., 2023a) leveraging advanced tuning strategies (Zhang\net al., 2023a) for financial applications. BloombergGPT (Wu et al., 2023) stands out as a BLOOM-\nbased, closed-source model tailored for the financial industry. Additionally, the Chinese financial\nsector has seen the emergence of models like XuanYuan 2.0 (Zhang et al., 2023c), integrating broad\nand specialized knowledge, FinBART (Hongyuan et al., 2023) for financial communication, and\nCFGPT (Li et al., 2023a), which includes a comprehensive dataset for targeted pre-training and\nfine-tuning.\nE.2\nFinancial Evaluation Benchmarks\nFinancial evaluation benchmarks, such as the pioneering FLUE (Shah et al., 2022), have been\nintroduced to measure model performance in the financial sector, covering five key NLP tasks:\nfinancial sentiment analysis (Shah et al., 2022), news headline classification (Sinha and Khandait,\n2021), named entity recognition (NER) (Salinas Alvarado et al., 2015), structure boundary detection\nand question answering (QA) (Chen et al., 2022a). Building upon FLUE, FLARE (Xie et al., 2023b)\nadded the evaluation of time-series processing capabilities, i.e., forecasting stock price movements.\nIn addition, in Chinese financial benchmarks, there are more recently released Chinese datasets\nlike CFBenchmark (Lei et al., 2023), DISC-FINSFT (Chen et al., 2023b), and CGCE (Zhang et al.,\n2023b). However, these benchmarks have a limited scope and have not yet addressed more complex\nfinancial NLP tasks such as event detection (Zhou et al., 2021), and realistic financial tasks, despite\nthe fact that there were previous efforts on stock trading (Liu et al., 2022; Han et al., 2023a,b).\nF\nTrading Accumulative Returns\nTable 10 and the figures below show detailed trading performance.\nFigure 2: Accumulative Returns of LLM Trading Strategies on AAPL\n19\nTable 8: Quantification task datasets prompt overview.\nData\nPrompt\nFPB\n\u201cAnalyze the sentiment of this statement extracted from a financial news article.\nProvide your answer as either negative, positive or neutral.\nFor instance, \u2019The company\u2019s stocks plummeted following the scandal.\u2019 would be classified as negative.\"\nFiQA-SA\n\u201cWhat is the sentiment of the following financial {category}:\nPositive, Negative, or Neutral?\"\nHeadlines\n\u201cConsider whether the headline mentions the price of gold.\nIs there a Price or Not in the gold commodity market indicated in the news headline?\nPlease answer Yes or No.\"\nNER\n\u201cIn the sentences extracted from financial agreements in U.S. SEC filings,\nidentify the named entities that represent a person (\u2019PER\u2019), an organization (\u2019ORG\u2019),\nor a location (\u2019LOC\u2019). The required answer format is: \u2019entity name, entity type\u2019.\nFor instance, in \u2019Elon Musk, CEO of SpaceX, announced the launch from Cape Canaveral.\u2019,\nthe entities would be: \u2019Elon Musk, PER; SpaceX, ORG; Cape Canaveral, LOC\u2019\"\nFiNER-ORD\n\u201cIn the list of tokens, identify {tid}each accordingly.\nIf the entity spans multiple tokens, use the prefix B-PER, B-LOC, or B-ORG for the first token, and I-PER,\nI-LOC, or I-ORG for the subsequent tokens of that entity.\nThe beginning of each separate entity should always be labeled with a B-PER, B-LOC, or B-ORG prefix.\nIf the token does not fit into any of the three named categories, or is not a named entity, label it as \u2019O\u2019.\"\nFinQA\n\u201cGiven the financial data and expert analysis, please answer this question:\"\nRegulations\n\u201cPlease answer following questions.\"\nConvFinQA\n\u201cIn the context of this series of interconnected finance-related queries and the additional information\nprovided by the pretext, table data, and post text from a company\u2019s financial filings,\nplease provide a response to the final question. This may require extracting information\nfrom the context and performing mathematical calculations. Please take into account the information provided in\nthe preceding questions and their answers when formulating your response:\"\nBigData22\n\u201c Contemplate the data and tweets to guess whether the closing price of {tid} will surge or decline at {point}.\nPlease declare with either Rise or Fall.\"\nACL18\n\u201cScrutinize the data and tweets to envisage if the closing price of {tid}will swell or contract at {point}.\nRespond with either Rise or Fall.\"\nCIKM18\n\u201cReflect on the provided data and tweets to anticipate if the closing price of {tid}is going to increase or decrease at {point}.\nRespond with either Rise or Fall.\"\nECTSum\n\u201cGiven the following article, please produce a list of 0 and 1, each separated by \u2019 \u2019 to indicate which sentences\nshould be included in the final summary. The article\u2019s sentences have been split by \u2019 \u2019. Please mark each sentence\nwith 1 if it should be included in the summary and 0 if it should not.\"\nEDTSum\n\u201cYou are given a text that consists of multiple sentences. Your task is to perform abstractive summarization on this text. Use\nyour understanding of the content to express the main ideas and crucial details in a shorter, coherent, and natural sounding text.\"\nGerman\n\u201cAssess the creditworthiness of a customer using the following table attributes for financial status. Respond with either\n\u2019good\u2019 or \u2019bad\u2019. And the table attributes including 13 categorical attributes and 7 numerical attributes are as follows:\"\nAustralian\n\u201cAssess the creditworthiness of a customer using the following table attributes for financial status. Respond with either\n\u2019good\u2019 or \u2019bad\u2019. And the table attributes including 13 categorical attributes\nand 7 numerical attributes and values have been changed to meaningless symbols to protect confidentiality of the data. :\"\nFOMC\n\u201cExamine the excerpt from a central bank\u2019s release below. Classify it as HAWKISH if it advocates for a tightening\nof monetary policy, DOVISH if it suggests an easing of monetary policy, or NEUTRAL if the stance is unbiased.\nYour response should return only HAWKISH, DOVISH, or NEUTRAL.\"\nTSA\n\u201cGiven the following financial text, return a sentiment score for Ashtead as a floating-point number\nranging from -1 (indicating a very negative or bearish sentiment) to 1 (indicating a very positive or bullish sentiment),\nwith 0 designating neutral sentiment. Return only the numerical score first,\nfollow it with a brief reasoning behind your score.\"\nFinArg - ACC\n\u201cAnalyze sentences from earnings conference calls and identify\ntheir argumentative function.\nEach sentence is either a premise, offering evidence or reasoning, or a claim,\nasserting a conclusion or viewpoint. Return only premise or claim.\"\nFinArg - ARC\n\u201cIn this task, you are given a pair of sentences.\nYour objective is to ascertain the type of argumentative relation between these two sentences.\nThe relation could either be \u2019NoRelation\u2019, indicating no discernible relation between the sentences,\n\u2019Support\u2019, indicating that the first sentence supports the second, or \u2019Attack\u2019, indicating that the first sentence disputes\nor contradicts the second. Return only one of the three classifications: \u2019norelation\u2019, \u2019support\u2019, or \u2019attack\u2019.\"\nMultiFin\n\u201cIn this task, you\u2019re working with English headlines from the MULTIFIN dataset.\nThis dataset is made up of real-world article headlines from a large accounting firm\u2019s websites.\nYour objective is to categorize each headline according to its primary topic.\nThe potential categories are {category}.\nYour response should only include the category that best fits the headline.\"\nMA\n\u201cIn this task, you will be given Mergers and Acquisitions news articles or tweets.\nYour task is to classify each article or tweet based on whether the mentioned deal was completed or remained a rumour.\nYour response should be a single word - either \u2019complete\u2019 or \u2019rumour\u2019 -\nrepresenting the outcome of the deal mentioned in the provided text.\"\nMLESG\n\u201cYou\u2019re given English news articles related to Environmental, Social, and Corporate Governance (ESG) issues.\nYour task is to classify each article based on the ESG issue it pertains to, according to the MSCI ESG rating guidelines.\nThe ESG issues include {category}.\nYour output should be the most relevant ESG issue label, followed by a brief rationale based on the article content.\"\n20\nTable 9: The example prompts of remaining tasks. FiQA-SA has two types of text, including news\nheadlines and tweets. We will fill the detailed text type into {category} for each data sample. For\nstock movement prediction data such as BigData22, we will fill {tid} and {point} with the detailed\nstock name and time from each data sample. For Spanish tasks, please refer to (Zhang et al., 2024).\nData\nPrompt\nFinRED\n\u201cGiven the following sentence, identify the head, tail, and relation of each triplet present in the sentence.\nThe relations you should be looking for are {category}.\nIf a relation exists between two entities, provide your answer in the format {category}.\nIf there are multiple triplets in a sentence, provide each one on a new line.\"\nSC\n\u201cIn this task, you are provided with sentences extracted from financial news and SEC data.\nYour goal is to classify each sentence into either \u2019causal\u2019 or \u2019noise\u2019 based on whether or not it indicates a causal relationship between financial events.\nPlease return only the category \u2019causal\u2019 or \u2019noise\u2019.\"\nCD\n\u201cYour job in this task is to perform sequence labeling on a provided text section, marking the chunks that represent the cause of an event and the effects\nthat result from it. For each token in the text, assign a label to indicate its role in representing cause or effect.\nThe labels you should use are \u2019B-CAUSE\u2019, \u2019I-CAUSE\u2019, \u2019B-EFFECT\u2019, \u2019I-EFFECT\u2019, and \u2019O\u2019.\nA \u2019B-\u2019 prefix is used to denote the beginning of a cause or effect sequence,\nwhile an \u2019I-\u2019 prefix is used for continuation of a cause or effect sequence.\nIf a token is not part of either a cause or effect sequence, label it as \u2019O\u2019.\nProvide your answer as a sequence of \u2019token:label\u2019 pairs, with each pair on a new line.\"\nTATQA\n\u201cPlease answer the given financial question based on the context. Context: {context}Question: What is the amount of total sales in 2019?\"\nFNXL\n\u201cIn the task of Financial Numeric Extreme Labelling (FNXL),\nyour job is to identify and label the semantic role of each token in a sentence.\nThe labels can include {category}\"\nFSRL\n\u201cIn the task of Textual Analogy Parsing (TAP), your job is to identify and label the semantic role of each token in a sentence.\nThe labels can include {category}.\"\nLendingClub\n\u201cAssess the client\u2019s loan status based on the following loan records from Lending Club.\nRespond with only \u2019good\u2019 or \u2019bad\u2019, and do not provide any additional information.\nFor instance, \u2019The client has a stable income, no previous debts, and owns a property.\u2019 should be classified as \u2019good\u2019.\"\nccf\n\u201cDetect the credit card fraud using the following financial table attributes.\nRespond with only \u2019yes\u2019 or \u2019no\u2019, and do not provide any additional information.\nTherein, the data contains 28 numerical input variables V1, V2, ...,\nand V28 which are the result of a PCA transformation and 1 input variable Amount which has not been transformed with PCA.\nThe feature \u2019Amount\u2019 is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning.\nFor instance, \u2019The client has attributes:{category}\"\nccfraud\n\u201cDetect the credit card fraud with the following financial profile.\nRespond with only \u2019good\u2019 or \u2019bad\u2019, and do not provide any additional information. For instance,\n\u2019The client is a female, the state number is 25, the number of cards is 1, the credit balance is 7000,\nthe number of transactions is 16, the number of international transactions is 0,\nthe credit limit is 6.\u2019 should be classified as \u2019good\u2019.\"\npolish\n\u201cPredict whether the company will face bankruptcy based on the financial profile attributes provided in the following text.\nRespond with only \u2019no\u2019 or \u2019yes\u2019, and do not provide any additional information.\"\ntaiwan\n\u201cPredict whether the company will face bankruptcy based on the financial profile attributes provided in the following text.\nRespond with only \u2019no\u2019 or \u2019yes\u2019, and do not provide any additional information.\"\nPorto-Seguro\n\u201cIdentify whether or not to files a claim for the auto insurance policy holder using the following table attributes about individual financial profile.\nRespond with only \u2019yes\u2019 or \u2019no\u2019, and do not provide any additional information.\nAnd the table attributes that belong to similar groupings are tagged as such in the feature names (e.g., ind, reg, car, calc).\nIn addition, feature names include the postfix bin to indicate binary features and cat to indicate categorical features.\nFeatures without these designations are either continuous or ordinal.\nValues of -1 indicate that the feature was missing from the observation.\"\ntravelinsurace\n\u201cIdentify the claim status of insurance companies using the following table attributes for travel insurance status.\nRespond with only \u2019yes\u2019 or \u2019no\u2019, and do not provide any additional information.\nAnd the table attributes including 5 categorical attributes and 4 numerical attributes are as follows:{category}\"\nFinTrade\n\u201cGiven the information, can you make an investment decision? Just summarize the reason of the decision.\nplease consider only the available short-term information, the mid-term information, the long-term information, the\nreflection-term information.\nplease consider the momentum of the historical stock price.\nWhen cumulative return is positive or zero, you are a risk-seeking investor.\nBut when cumulative return is negative, you are a risk-averse investor.\nplease consider how much share of the stock the investor holds now.\nYou should provide exactly one of the following investment decisions: buy or sell.\nWhen it is really hard to make a \u2019buy\u2019-or-\u2019sell\u2019 decision, you could go with \u2019hold\u2019 option.\nYou also need to provide the id of the information to support your decision.\n{investment_info}\n{gr\u02d9complete_json_suffix_v2}\nYour output should strictly conforms the following json format without any additional contents:\n{\"investment_decision\" : string, \"summary_reason\": string, \"short_memory_index\": number,\n\"middle_memory_index\": number, \"long_memory_index\": number, \"reflection_memory_index\": number}\"\nG\nFinLLM challenge\nBased on our proposed FinBen, we organized the FinLLM Share Task during the FinNLP-AgentScen\nWorkshop at IJCAI 202412, known as the FinLLM Challenge. This challenge not only tests the\nabilities of LLMs but also promotes ongoing research into their application within the financial sector,\nhighlighting FinBen\u2019s critical contribution to the advancement of financial analytics.\n12https://sites.google.com/nlg.csie.ntu.edu.tw/finnlp-agentscen/shared-task-finllm?\nauthuser=0\n21\nTable 10: The overall trading performance comparison for different LLMs across various stocks.\nThe results include large LLMs only (\u226570B), as models with smaller contexts have difficulty\nunderstanding the instructions and producing a static strategy of holding.\nTicker\nModel\nCR (%)\nSR\nDV (%)\nAV (%)\nMD (%)\nTSLA\nBuy & Hold\n-25.2137\n-0.7203\n4.4099\n70.0043\n57.6765\nGPT-4\n68.3089\n2.8899\n2.9780\n47.2739\n10.7996\nGPT-4o\n-0.8789\n-0.0321\n3.4531\n54.8156\n44.6842\nGPT3.5-Turbo\n25.2137\n0.7203\n4.4099\n70.0043\n51.3186\nllama2-70B\n-31.4144\n-1.0412\n3.8014\n60.3450\n48.6173\nllama3-70B\n-16.4424\n-0.4847\n4.2743\n67.8519\n55.5486\ngemini\n-0.3790\n-0.0148\n3.2271\n51.2280\n35.6707\nNFLX\nBuy & Hold\n34.6251\n1.3696\n3.1852\n50.5634\n20.9263\nGPT-4\n36.4485\n2.0088\n2.2860\n36.2894\n15.8495\nGPT-4o\n5.5829\n0.2592\n2.7132\n43.0702\n17.4715\nGPT3.5-Turbo\n7.9337\n0.4610\n2.1680\n34.4160\n17.9578\nllama2-70B\n33.8460\n1.4741\n2.8928\n45.9216\n20.3910\nllama3-70B\n21.7374\n0.9513\n2.8788\n45.6989\n21.3478\ngemini\n11.6298\n1.0073\n1.4546\n23.0906\n16.5106\nAMZN\nBuy & Hold\n-16.4428\n-0.7448\n2.7812\n44.1508\n33.8847\nGPT-4\n10.5539\n0.4923\n2.7012\n42.8802\n22.9294\nGPT-4o\n11.3626\n0.7334\n1.9520\n30.9864\n19.5964\nGPT3.5-Turbo\n19.9636\n0.9611\n2.6171\n41.5454\n19.2191\nllama2-70B\n8.3595\n1.9715\n0.5342\n8.4804\n0.0000\nllama3-70B\n11.1479\n0.5405\n2.5986\n41.2509\n28.2174\ngemini\n-2.3838\n-0.5321\n0.5645\n8.9605\n6.4291\nMSFT\nBuy & Hold\n17.2161\n0.9710\n2.2339\n35.4623\n15.0097\nGPT-4\n25.7826\n1.5818\n2.0535\n32.5989\n14.9889\nGPT-4o\n-5.3731\n-0.5209\n1.2997\n20.6317\n18.8223\nGPT3.5-Turbo\n20.4179\n1.3600\n1.8915\n30.0259\n20.3212\nllama2-70B\n27.7664\n1.5708\n2.2270\n35.3524\n15.0097\nllama3-70B\n21.1983\n1.2628\n2.1149\n33.5724\n15.0097\ngemini\n21.5081\n1.3701\n1.9777\n31.3957\n17.5051\nAAPL\nBuy & Hold\n12.7371\n0.7759\n2.0682\n32.8323\n20.6590\nGPT-4\n21.2335\n1.9274\n1.3879\n22.0328\n6.4237\nGPT-4o\n-6.7540\n-0.5693\n1.4948\n23.7285\n20.7600\nGPT3.5-Turbo\n0.7110\n0.0758\n1.1817\n18.7581\n6.0818\nllama2-70B\n11.4856\n1.1550\n1.2529\n19.8885\n9.2776\nllama3-70B\n-16.0835\n-1.1985\n1.6907\n26.8394\n25.9520\ngemini\n18.1718\n1.7214\n1.3300\n21.1134\n9.6467\nGOOG\nBuy & Hold\n6.3107\n0.3081\n2.5806\n40.9660\n21.1907\nGPT-4\n13.2811\n0.9667\n1.7308\n27.4762\n12.2209\nGPT-4o\n16.5072\n1.0654\n1.9520\n30.9872\n11.8863\nGPT3.5-Turbo\n0.9990\n0.0614\n2.0490\n32.5265\n20.9316\nllama2-70B\n17.0030\n1.1057\n1.9374\n30.7546\n13.2088\nllama3-70B\n17.5630\n0.8872\n2.4942\n39.5934\n19.2783\ngemini\n38.7956\n3.0341\n1.6110\n25.5732\n13.7311\nDIS\nBuy & Hold\n-0.0700\n-0.0037\n2.3667\n37.5695\n22.7722\nGPT-4\n31.3383\n2.3931\n1.6498\n26.1904\n12.3417\nGPT-4o\n-20.2500\n-1.3737\n1.8573\n29.4830\n27.0246\nGPT3.5-Turbo\n-7.1533\n-0.5109\n1.7641\n28.0048\n20.4278\nllama2-70B\n-3.8257\n-1.4323\n0.3365\n5.3420\n4.1451\nllama3-70B\n-25.5829\n-1.5579\n2.0690\n32.8437\n31.3391\ngemini\n8.6692\n0.8015\n1.3627\n21.6321\n18.4815\nGM\nBuy & Hold\n0.3393\n0.0179\n2.3823\n37.8181\n23.0317\nGPT-4\n10.5648\n0.7671\n1.7351\n27.5443\n11.1285\nGPT-4o\n-7.0147\n-0.5263\n1.6792\n26.6569\n21.5978\nGPT3.5-Turbo\n-17.6385\n-0.9692\n2.2928\n36.3976\n23.0317\nllama2-70B\n8.4911\n2.6369\n0.4057\n6.4402\n2.1318\nllama3-70B\n25.9335\n1.9823\n1.6483\n26.1657\n13.2485\ngemini\n18.6257\n2.4672\n0.9511\n15.0989\n3.0369\nNIO\nBuy & Hold\n-49.4263\n-1.1895\n5.2351\n83.1048\n52.2083\nGPT-4\n24.7684\n0.9438\n3.3063\n52.4861\n29.3384\nGPT-4o\n-48.3748\n-1.5026\n4.0562\n64.3897\n59.4037\nGPT3.5-Turbo\n-28.9321\n-1.0096\n3.6105\n57.3149\n39.5907\nllama2-70B\n-49.6947\n-2.7868\n2.2466\n35.6639\n42.6221\nllama3-70B\n-28.6668\n-0.7094\n5.0912\n80.8202\n37.1544\ngemini\n14.5673\n0.6212\n2.9543\n46.8977\n23.0110\nCOIN\nBuy & Hold\n-18.4787\n-0.3369\n6.9098\n109.6904\n60.5084\nGPT-4\n25.7631\n0.5619\n5.7761\n91.6934\n35.7526\nGPT-4o\n-14.2451\n-0.2892\n6.2049\n98.4997\n65.3090\nGPT3.5-Turbo\n25.1141\n0.4772\n6.6312\n105.2669\n53.9628\nllama2-70B\n15.1836\n0.4395\n4.3528\n69.0979\n35.3249\nllama3-70B\n19.8876\n0.3749\n6.6842\n106.1076\n55.7225\ngemini\n89.4782\n1.7648\n6.3879\n101.4048\n40.3246\nThe FinLLM Challenge is a specialized shared task tailored for LLMs, targeting a comprehensive\nrange of financial problems through three subtasks: financial classification, financial text summariza-\ntion, and single stock trading. To rigorously evaluate the capabilities of financial LLMs, we have\ncurated three distinct datasets corresponding to each of these subtasks, as detailed in Table 11. This\nstructured approach ensures a holistic and effective assessment of LLM performance across diverse\nfinancial scenarios.\n22\nFigure 3: Accumulative Returns of LLM Trading Strategies on AMZN\nFigure 4: Accumulative Returns of LLM Trading Strategies on COIN\nFigure 5: Accumulative Returns of LLM Trading Strategies on GOOG\nFigure 6: Accumulative Returns of LLM Trading Strategies on MSFT\nG.1\nTasks and Datasets\nTask 1: Financial Classification. This task, inherited from FinBen\u2019s financial classification task,\nfocuses on argument unit classification to test the capabilities of LLMs to identify and categorize texts\nas premises or claims. It consists of 7.75K training data and 969 test data to categorize sentences as\nclaims or premises. We use two metrics to evaluate classification capability, like F1 and Accuracy. F1\nscore is used as the final ranking metric.\n23\nFigure 7: Accumulative Returns of LLM Trading Strategies on NFLX\nFigure 8: Accumulative Returns of LLM Trading Strategies on NIO\nFigure 9: Accumulative Returns of LLM Trading Strategies on TSLA\nFigure 10: Accumulative Returns of LLM Trading Strategies on DIS\nTask 2: Financial Text Summarization. This task, inherited from FinBen\u2019s generation task, is\ndesigned to test the capabilities of LLMs to generate coherent summaries. It provides 8k training\ndata and 2k test data for abstracting financial news articles into concise summaries. We utilize three\nmetrics, such as ROUGE (1, 2, and L) and BERTScore, to evaluate generated summaries in terms of\nRelevance. ROUGE -1 score is used as the final ranking metric.\n24\nFigure 11: Accumulative Returns of LLM Trading Strategies on GM\nTable 11: Tasks and Datasets of FinLLM Challenge.\nCategory\nTasks\nDatasets\nEvaluation Metrics\nTraining set\nTest set\nTask 1\nFinancial Classification\n7.75k\n969\nF1 Score, Acc\nTask 2\nFinancial Text Summarization\n8k\n2k\nROUGE-1, ROUGE-2, ROUGE-L, BERTScore\nTask 3\nSingle Stock Trading\n291\n225\nSharpe Ratio, Cumulative Return,\nMaximum Drawdown, Daily and Annualized Volatility,\nTask 3: Single Stock Trading. This task, inherited from FinBen\u2019s Trading task, aims to evaluate\nLLMs\u2019 ability to make sophisticated decisions in trading activities, which is currently restricted\nby human\u2019s limited ability to process large volumes of data rapidly. It specifically provides 291\ndata different from FinBen datasets, to evaluate LLMs on sophisticated stock Decisions. We offer\na comprehensive assessment of profitability, risk management, and decision-making prowess by a\nseries of metrics, such as Sharpe Ratio (SR), Cumulative Return (CR), Daily (DV) and Annualized\nvolatility (AV), and Maximum Drawdown (MD). Sharpe Ratio (SR) score is used as the final ranking\nmetric.\nG.2\nModel Cheating Detection\nTo measure the risk of data leakage from the test set used in training, we introduce the Data Leakage\nTest (DLT). The DLT calculates the difference in perplexity between the training set and the test\nset. A larger difference indicates a lower likelihood of model cheating, while a smaller difference\nsuggests a higher likelihood. For our FinLLM Challenge, we invite Top-3 participant teams per task\nfor cheating detection.\nG.3\nParticipants and Automatic Evaluation\nThere are 35 teams registered for FinLLM Challenge, with 12 teams submitting their system descrip-\ntion papers. Participants can opt to join one or more task(s).\nAs shown in Table 12, the top 3 teams achieved outstanding performance in Task 1. Their models\u2019 F1\nscores were comparable to LlaMA3-8B, although slightly inferior to GPT-4 and LLaMA2-70B, yet\nsignificantly outperformed FinMA and other models. The results in Table 12 further demonstrate that\nour FinLLM share task provides an excellent framework for participating teams to achieve superior\nexperimental outcomes.\nTable 12: The Result of Taks 1: Financial Classification\nTeams\nACC\nF1\nMCC\nTeam Barclays\n0.7626\n0.5237\n0.7427\nAlbatross\n0.7574\n0.5174\n0.7555\nL3iTC\n0.7544\n0.5149\n0.7581\nWealth Guide\n0.7513\n0.5018\n0.7406\nFinance Wizard\n0.7286\n0.4554\n0.7008\nCatMemo\n0.711\n0.4199\n0.6818\nUpaya\n0.709\n0.4166\n0.6941\nVidra\n0.7079\n0.4141\n0.69\njt\n0.4933\n0.0141\n0.5905\n25\nAs illustrated in Table 13, in terms of the Rouge-1 metric, the models of these three teams surpassed all\nother models, demonstrating superior performance. The results in Table 2 indicate that, for financial\ngeneration tasks, our provided dataset and model framework help participating teams leverage their\nstrengths and achieve better outcomes.\nTable 13: The Result of Taks 2: Financial Text Summarization\nTeams\nRouge-1\nRouge-2\nRouge-L\nBertScore\nBartScore\nWealth Guide\n0.308893532\n0.179468097\n0.281924302\n0.85959909\n-4.961457408\nAlbatross\n0.369077581\n0.201058395\n0.322684316\n0.872049115\n-3.933526929\nLBZ\n0.534616211\n0.358105428\n0.492179554\n0.911732047\n-3.407560172\nL3iTC\n0.366093426\n0.187210467\n0.304610677\n0.875037043\n-4.257126737\nFinance Wizard\n0.521037018\n0.34060938\n0.473530112\n0.90836845\n-3.497988865\nVidra\n0.284955468\n0.134760859\n0.228638961\n0.858682767\n-4.169740305\nRevelata\n0.500411369\n0.333023818\n0.464356474\n0.907018743\n-3.805486962\nUpaya\n0.529459817\n0.358203218\n0.486046685\n0.910644962\n-3.45155009\nAs shown in Table14, the Top-1 Wealth Guide team excelled in the Sharpe Ratio metric, surpassing\nother teams and demonstrating outstanding performance. While it may not match the performance of\nGPT-4, it still outperforms other large models. These results from Table 3 once again underscore\nthe significance of organizing the FinLLM share task. The FinLLM Challenge not only assesses the\nperformance of large language models (LLMs) but also fosters further research into applying LLMs\nin the financial domain.\nTable 14: The Result of Taks 3: Single Stock Trading\nTeams\nSharpe Ratio\nSharpe Ratio-DRIV\nSharpe Ratio-FORM\nSharpe Ratio-JNJ\nSharpe Ratio-MSFT\nWealth Guide\n0.9263852228\n0.485625528\n1.585611423\n0.078737051\n1.555566991\nUpaya\n0.467489019\n0.380232272\n0.108506918\n-1.102831656\n-0.278385232\nAlbatross\n0.48383204\n0.251306057\n-1.435471054\n-1.558522674\n1.309971626\nCatMemo\n-0.619939784\n-1.393291177\n0.175932289\n0.383243051\n-0.879157198\nH\nPerformances of non-LLM methods\nIn this section, we present the performances of non-LLM methods on stock movement prediction\nand financial NLP tasks from previous papers. Note that non-LLM methods are task-oriented, each\nmodel can only run on a specific task.\nH.1\nStock Movement Prediction\nStock movement prediction performance of non-LLM models are shown in Table 15. The results are\nfrom (Xie et al., 2023b).\nTable 15: Stock movement prediction performance of non-LLM models, measured with the accuracy\n(ACC) and the Matthews correlation coefficient (MCC). The best performance is in bold.\nMethod\nBIGDATA22\nACL18\nCIKM18\nACC\nMCC\nACC\nMCC\nACC\nMCC\nLogistic regression (LR)\n0.53\n0.02\n0.52\n0.04\n0.53\n-0.04\nRandom forest (RF)\n0.47\n-0.11\n0.52\n0.03\n0.54\n0.01\nLSTM\n0.51\n0.01\n0.53\n0.06\n0.53\n0.02\nAttention LSTM (ALSTM)\n0.49\n-0.03\n0.52\n0.04\n0.53\n-0.01\nAdv-ALSTM\n0.50\n0.01\n0.53\n0.07\n0.54\n0.02\nDTML\n0.52\n0.07\n0.58\n0.18\n0.54\n-0.00\nXGBoost\n0.52\n-0.04\n0.49\n-0.02\n0.58\n0.07\nXGBRegressor\n0.46\n-0.13\n0.50\n-0.01\n0.53\n-0.03\nALSTM-W\n0.48\n-0.01\n0.53\n0.08\n0.54\n0.03\nALSTM-D\n0.49\n0.01\n0.53\n0.07\n0.50\n-0.04\nStockNet\n0.53\n-0.00\n0.54\n-0.03\n0.52\n-0.02\nSLOT\n0.55\n0.10\n0.59\n0.21\n0.56\n0.09\nH.2\nFinancial NLP Tasks\nBERT-based model results of financial NLP tasks are shown in Table 16. The results are from (Shah\net al., 2022).\n26\nTable 16: Financial NLP tasks performances of BERT-based models. The best performance is in\nbold.\nMethod\nFPB\nHeadline\nNER\nFiQA SA\nAccuracy\nAvgF1\nF1\nMSE\nBERT-base\n0.856\n0.967\n0.79\n0.073\nFinBERT\n0.872\n0.968\n0.8\n0.070\nFLANG-BERT\n0.912\n0.972\n0.83\n0.054\nELECTRA\n0.881\n0.966\n0.78\n0.066\nFLANG-ELECTRA\n0.919\n0.98\n0.82\n0.034\nH.3\nFinancial Risk Management Tasks\nTraditional model results of financial risk management tasks are shown in Table 17. The results are\nfrom (Feng et al., 2024).\nTable 17: Performance of various models on financial risk management datasets. The best perfor-\nmance for each metric is in bold.\nDataset\nMethod\nMetric\nValue\nCredit Card Fraud\nANN\nF1\n0.85\nMCC\n0.17\nccfraud\nEGRNN++\nF1\n0.90\nMCC\n0.34\nPolish\nBayesian\nF1\n0.99\nMCC\n0.57\nTravel Insurance\nRandom Forest\nF1\n0.91\nMCC\n0.15\nLimitations\nDespite the novel efforts to benchmark LLMs in the financial domain through FinBen, we acknowl-\nedge several inherent limitations that could impact the benchmark\u2019s effectiveness and applicability:\nDataset Size Limitations: The restricted size of available datasets, a common issue in open-source\nfinancial data, may affect the models\u2019 financial understanding and generalization across various\ncontexts. Model Size Limitations: Due to computational constraints, our evaluation was limited to\nthe LLaMA 70B model, potentially overlooking the capabilities of larger or differently architected\nmodels. Generalizability: The tasks, particularly trading and forecasting, are based on American\nmarket data and English texts, possibly limiting the benchmark\u2019s applicability to global financial mar-\nkets. Potential Negative Impacts: While FinBen aims to advance financial language understanding,\nit is crucial to consider potential misuse, such as propagating financial misinformation or exerting\nunethical influence on markets. Responsible usage and further safeguards are essential13.\nEthical Statement\nThe development and dissemination of the FinBen by the authors carry full responsibility for any\npotential violation of rights or arising legal issues. All raw data we used are publicly available and do\nnot contain any personal information. Diligent efforts have been undertaken to ensure the construction\nof the FinBen respects privacy and conforms to established ethical guidelines. The datasets compiled\nwithin FinBen are shared under the MIT license, with the expectation that users agree to adhere to its\nconditions.\nThis manuscript, inclusive of any associated source codes, datasets, and appendices (\"Material\"),\nis designated exclusively for academic and educational pursuits. It is crucial to acknowledge that\nthe Material does not provide financial, legal, or investment counsel, nor should it be utilized as a\nfoundation for any form of decision-making.\nWhile the authors have exerted reasonable diligence to verify the accuracy and reliability of the\nMaterial, no explicit or implied warranty is extended regarding its completeness or suitability for any\n13For a detailed ethical and legal statement concerning this work, please see Appendix.\n27\nspecific application. The authors, along with their affiliated entities, absolve themselves of liability\nfor any losses, damages, or other consequences, whether direct or indirect, that may emanate from\nthe employment or reliance upon the Material. It is incumbent upon the user to seek professional\nconsultation for financial, legal, or investment determinations.\nBy referencing or employing this Material, individuals consent to indemnify, defend, and hold the\nauthors, along with any affiliated organizations or persons, harmless against any claims or damages\nthat may arise from such utilization.\nDisclaimer: We are sharing codes for academic purposes under open-source license. Nothing\nherein is financial advice, and NOT a recommendation to trade real money. Please use common\nsense and always first consult a professional before trading or investing.\nChecklist\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] See Limitation (Section H.3).\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] See Ethical\nStatement (Section H.3).\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes] See Ethical Statement (Section H.3).\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n(b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments (e.g. for benchmarks)...\n(a) Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes] See Introduction\n(Section 1).\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [N/A] Our benchmark only includes the evaluation process.\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-\nments multiple times)? [Yes] See Table 4.\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [Yes] See Experimental Settings (Section\n3).\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes]\n(b) Did you mention the license of the assets? [Yes]\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\nOur Introduction (Section 1) contains a link for all data used in FinBen.\n(d) Did you discuss whether and how consent was obtained from people whose data you\u2019re\nusing/curating? [Yes] Our dataset statistics (Table 2) contains licenses for all used\ndatasets.\n(e) Did you discuss whether the data you are using/curating contains personally identifiable\ninformation or offensive content? [Yes] See Ethical Statement (Section H.3).\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\n28"
    },
    {
        "paper": "Magnet We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function",
        "input": "Magnet: We Never Know How Text-to-Image\nDiffusion Models Work, Until We Learn How\nVision-Language Models Function\nChenyi Zhuang1\nYing Hu1\nPan Gao1,2\u2217\n1 Nanjing University of Aeronautics and Astronautics\n2 Key Laboratory of Brain-Machine Intelligence Technology, Ministry of Education\n{chenyi.zhuang,ying.hu,pan.gao}@nuaa.edu.cn\nAbstract\nText-to-image diffusion models particularly Stable Diffusion, have revolutionized\nthe field of computer vision. However, the synthesis quality often deteriorates\nwhen asked to generate images that faithfully represent complex prompts involving\nmultiple attributes and objects. While previous studies suggest that blended text\nembeddings lead to improper attribute binding, few have explored this in depth.\nIn this work, we critically examine the limitations of the CLIP text encoder in\nunderstanding attributes and investigate how this affects diffusion models. We\ndiscern a phenomenon of attribute bias in the text space and highlight a contextual\nissue in padding embeddings that entangle different concepts. We propose Magnet,\na novel training-free approach to tackle the attribute binding problem. We introduce\npositive and negative binding vectors to enhance disentanglement, further with a\nneighbor strategy to increase accuracy. Extensive experiments show that Magnet\nsignificantly improves synthesis quality and binding accuracy with negligible com-\nputational cost, enabling the generation of unconventional and unnatural concepts.\nCode is available at https://github.com/I2-Multimedia-Lab/Magnet.\n1\nIntroduction\nRecently, Text-to-Image (T2I) diffusion models [1, 2, 3, 4] have drawn considerable attention from\nboth the research community and industry. Among these models, Stable Diffusion (SD) [2] uses the\nCLIP text encoder [5] to encode the given prompt, which is relatively lightweight than other diffusion\nmodels that adopt T5 [6]. Unfortunately, generating text-aligned images is still challenging for SD\nand requires multiple runs to achieve the desirable results. Several works [7, 8, 9, 10] have pointed\nout that the blended context by the CLIP text encoder causes improper binding. However, few have\nanalyzed in detail how the text encoder affects the generation of the diffusion model.\nStep back and refocus on the CLIP text encoder\u2014an integral part of the Vision-Language Model\n(VLM). Prior studies [11, 12] have observed VLMs lacking compositional understanding and investi-\ngated them on image-to-text retrieval benchmarks. In this work, we are motivated to answer how\nthe text encoder understands attribute, and how it affects the attribute binding of T2I diffusion\nmodels. Upon closer inspection, we observe a phenomenon of attribute bias and discern a contextual\nproblem in padding embeddings, leading to a well-known T2I issue\u2014concept bleeding [10].\nBased on the observation, we introduce the binding vector, which is applied to the text embedding\nof each object. With positive and negative binding vectors, each object can pull target attributes\nand push unrelated attributes to distinguish them from each other. We further introduce a neighbor\nstrategy to ensure an accurate estimation of the binding vector. Our manipulation is performed strictly\nin the textual space, without training, fine-tuning, or additional datasets and inputs. Overall, the main\n\u2217Corresponding author\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\nFigure 1: Analysis of the CLIP text encoder for understanding attributes. There is a discrepancy\nbetween the word and [EOT] embeddings of the attribute bias on different objects.\ncontributions of this work are: (1) We highlight a contextual issue in the text encoder, which impacts\ndiffusion-based image generation. (2) We propose a novel training-free method to address the binding\nproblem. (3) Extensive experiments are conducted to verify the effectiveness of Magnet.\n2\nAnalysis of the CLIP text encoder and the diffusion model\nIn this section, we aim to recognize the pattern of the CLIP text encoder for understanding attributes,\nthen go deep into the diffusion model to analyze the underlying reason for improper attribute binding.\nThe CLIP text encoder uses the causal mask mechanism to produce a unidirectional context, i.e., each\nword can only consider the words to their left [13, 14]. It performs contrastive learning on a specific\nEnd of Text ([EOT]) embedding without word-level supervision, while prior studies [15, 16, 17]\nsuggest each word has a semantic effect on the generated image. In this case, we categorize two\ntypes of embedding as word and [EOT] for fine-grained analysis. Consider a prompt encoded to\ntext embeddings c = {cSOT , cp1, ..., cpN , cEOT } consisting N word embeddings. Different from\nthe [EOT] embedding cEOT , word embeddings cp1, ..., cpN are unsupervised during training. We\nskip the embedding of Start of Text ([SOT]) cSOT for simplicity. To study how the two types of\nembedding understand attributes, we select 60 familiar objects and 7 common colors to obtain text\nembeddings c = {cobject, cEOT } and c\u2032 = {c\u2032\ncolor, c\u2032\nobject, c\u2032\nEOT }, i.e., without and with the color\ncontext, respectively. We aim to compare: (1) contextualized word embedding c\u2032\nobject with the\noriginal cobject ; (2) contextualized [EOT] embedding c\u2032\nEOT with the original cEOT .\nHow do two types of embedding understand attributes? Fig. 1 (a) compares the Euclidean distance\nand the cosine similarity between embeddings with and without the color context. The pattern varies\nbetween cases or objects. As to the word embedding, the similarity curve of \"chair\" is relatively\nsmooth, but that of \"sheep\" has a large gap between \"black\" and \"white\". Similarly, \"blue apple\"\ndiverges from others. The above phenomenon, which we call attribute bias, describes the tendency\nof an object to favor certain attributes over others. We compare the attribute bias per object for two\nembedding types in Fig. 1 (b). If the object has a natural composition in human knowledge, it presents\na serious attribute bias (e.g., \"yellow banana\" v.s. \"blue banana\"). Meanwhile, the word embedding\nis more volatile than the [EOT] embedding, which shows less dramatic change. Our hypothesis is the\nabsence of word-level supervision during CLIP training, as well as the bags-of-words behavior of\nVLMs [11, 12]: the [EOT] embedding is trying to remember all important words in the given\nprompt, including adjectives and nouns. However, it leads to an inaccurate textual representation\nof the [EOT] embedding, affecting the interaction between the image latent and semantic word\nembeddings. We have provided more analysis and examples in Appendix A.1 (see Fig. 12, 13).\nHow do two types of embedding affect SD? In practice, SD pads the input prompt to a fixed length\nL = 77 using additional [EOT] embeddings (i.e., the padding token is initialized by the symbol\n[EOT]), denoted as cpadl, where l = 1, ..., L\u2212N \u22122. To study the attribute binding during generation,\nwe modify the above two text embeddings c and c\u2032 with L \u2212N \u22122 padding token embeddings, then\ndesign 4 fine-grained cases: (1) standard generation conditioned on c\u2032 where all text embeddings\ncontain the color context; (2) only replace c\u2032\nobject with cobject to eliminate the context on the word\nembedding; (3) only replace c\u2032\nEOT with cEOT , as well as padding embeddings c\u2032\npadl with cpadl; (4)\neliminate the color context on word, [EOT], and padding embeddings. Fig. 2 (a) presents 3 examples,\n2\n(a) Fine-grained 4 cases  \n(b) Single-concept context issue \n(c) Multi-concept context issue \n(1)\n\" red chair \"\n(2)\n(4)\n(3)\n\" blue apple \"\n\" black sheep \"\n(1)\n(2)\n(4)\n(3)\n(1)\n(2)\n(4)\n(3)\nFigure 2: (a) Fine-grained study through our designed embedding swapping experiment. The context\nissue in padding embeddings for (b) single-concept scenario, and (c) multi-concept scenario.\nincluding a natural concept \"red chair\", unnatural concepts \"black sheep\" and \"blue apple\". Cases 1-2\ncan be observed in all examples with less realistic and painting-like images. Note that the used SD\nis trained to generate photo-realistic images. These results indicate a deviation from the learned\ndistribution. Conversely, cases 3-4 produce realistic images, but neglect the target color when the\nconcept is unnatural. This suggests that the context in [EOT] and padding embeddings do have a\nsignificant impact on attribute generation. In Appendix A.2, we describe the above 4 cases in detail\nand provide more examples, as well as 3 additional cases (see Fig. 14).\nWhy improper binding? We posit that the first [EOT] has a close-knit context with word embeddings.\nThe padding embedding, however, may deviate due to the causal attention mechanism. We then\ncompute the cosine similarity between [EOT] and each padding embedding l = 1, ..., L, and dive into\ntheir cross-attention activations on single- and multi-concept scenarios. Fig. 2 (b) studies a prompt\nwith only one object. The curve drops more drastically on the unnatural concept \"blue apple\" than\nthe natural one \"red chair\". Cross-attention shows that \"apple\" overlaps with padding embeddings\n(e.g., pad73) rather than the [EOT] embedding. It is like these padding embeddings have forgotten\npart of the context remembered in the first [EOT]. Without interference from other concepts, the\ninaccurate context in these padding embeddings causes out-of-distribution, or the binding of another\nattribute if the model learns an underlying bias in the training dataset (e.g., \"apple\" prefers \"red\").\nFig. 2 (c) further studies the context issue on multiple concepts. For two objects \"bird\" and \"car\",\neven though the activations of their word embeddings do not overlap, cross-attention shows obvious\nentanglement in these padding embeddings. This multi-concept context issue in padding embeddings,\ni.e., entangled concept representations, explains color leakage and object sticking. We refer the reader\nto Appendix A.3 for a comprehensive analysis of this context issue.\nHow to disentangle different concepts? Prior studies [8, 18] prove that these padding embeddings\nare essential for image quality and can not simply be removed. Also, it is impossible to manipulate\none single concept in these padding embeddings due to their entangled property. On the other hand,\nthese naturally separated word embeddings show editability. For instance, Fig. 2 (a) \"black sheep\"\nfrom case 2 to case 1 changes only the word embedding of \"sheep\" while encouraging the desired\nattribute. We are inspired to manipulate the word embedding of each object, therefore strengthening\nthe binding within each concept and enhancing the distinction between concepts.\n3\nMagnet: disentangling concepts with the binding vector\nOur approach is based on two key observations: the context issue of the padding embedding, and the\ncontrollability of the word embedding. We introduce the binding vector, which can be applied on the\nobject embedding to attract the target attribute and repulse other attributes, analogous to a Magnet.\nPreliminary: Given a prompt P, we use Stanza\u2019s dependency parsing module [19] to extract each\nconcept, denoted A&E, where E is the object word and its target attribute as A. The dependency\n3\n[SOT]\nCandicate Set\nText\u00a0 \u00a0\nEncoder\u00a0 \u00a0\ngreen\u00a0 \u00a0\u00a0\n...\nbrown\u00a0 \u00a0\u00a0\nbear\u00a0 \u00a0\u00a0\napple\u00a0 \u00a0\u00a0\ngreen\u00a0 \u00a0 \u00a0\n...\nbrown\u00a0 \u00a0 \u00a0\napple\u00a0 \u00a0\u00a0\nbear\u00a0 \u00a0 \u00a0\nSD\ntop-\nwolf\u00a0 \u00a0 \u00a0 \u00a0\ngreen\u00a0 \u00a0 \u00a0\u00a0\n[SOT]\n...\nwolf\u00a0 \u00a0 \u00a0 \u00a0\n[SOT]\n...\nwolf\u00a0 \u00a0 \u00a0\u00a0\nbrown\u00a0 \u00a0 \u00a0\n...\nUnet\n...\nbear\u00a0 \u00a0\u00a0\ngreen\u00a0 \u00a0\u00a0\n[SOT]\n...\n[PAD]\n...\n...\n...\nMagnet\n[EOT]\n(b) Adaptive strength\n(c) Neighbor-guided vector estimation\nEq. (4)\nEq. (3)\nEq. (5)\nEq. (6)\n(a) Generation pipeline\u00a0\n- lion\n- wolf\n...\n...\nFigure 3: Overview of the proposed Magnet. We manipulate the object embedding with the positive\nand negative binding vectors, which are estimated with the guidance of neighbor objects.\nset with M concepts is D = {A1&E1, ..., AM&EM}. Detailed dependency extraction is given in\nAppendix B.1. Then the pre-trained CLIP text encoder E is applied to map P to the text embed-\nding c = {cSOT , cA1, cE1, ..., cAM , cEM , cEOT , cpad1, ..., cpadL\u2212N\u22122}. For simplicity, we omit the\nlinking words. We treat the diffusion model as a black box and leave its background in Appendix B.2.\nFor each object Ei in D with the word embedding cEi, we aim to estimate its positive binding vector\nvpos\ni\nto pull the target attribute Ai, and its negative binding vector vneg\ni\nto push other attributes.\n3.1\nApply the binding vector on the object embedding\nInstinctively, the binding vector can be estimated by the object itself. To be specific, we compose new\nconcepts out of the current context of P, which are: (1) unconditional concepts, \u02dcPuc\ni\n= {\u2205&Ei},\nwhere \u2205is a blank text \u201c\u201d; (2) positive concepts, \u02dcPpos\ni\n= {Ai&Ei}; (3) negative concepts, \u02dcPneg\ni\n=\n{Aj&Ei|j = 1, ..., M, j \u0338= i}. The positive and negative binding vectors are estimated by:\nvpos\ni\n= F(Ei, \u02dcPpos\ni\n) \u2212F(Ei, \u02dcPuc\ni )\n(1)\nvneg\ni\n= F(Ei, \u02dcPneg\ni\n) \u2212F(Ei, \u02dcPuc\ni )\n(2)\nwhere F(\u00b7) extracts the word embedding of the object Ei in a specific decontextualized prompt. Note\neach object has M \u22121 negative concepts, resulting in M \u22121 negative binding vectors to punish\nall unrelated attributes Aj, j = 1, ..., M, j \u0338= i. Note that these positive and negative attributes are\nprompt-dependent 2. We introduce the unconditional concept as a pivot to avoid the need for manual\ndefinition or semantic contrast between positive and negative attributes.\nBased on our analysis of the context issue in the padding embedding in Section 2, we hypothesize an\nassociation between the attribute bias and the strength. Intuitively, unnatural concepts (e.g., \"blue\nbanana\") suffer more attribute bias and their padding embeddings are more tend to forget the concept.\nIn this case, we need to manipulate the word embedding significantly to ensure strong binding. We\nintroduce the adaptive strength of the binding vector for each object Ei:\n\u03b1i = e\u03bb\u2212\u03c9i, \u03b2i = 1 \u2212\u03c92\ni ,\nwhere\n\u03c9i = cos(G( \u02dcPpos\ni\n), H( \u02dcPpos\ni\n))\n(3)\nwhere G(\u00b7), H(\u00b7) extract the first [EOT] embedding and the last padding embedding in text embed-\ndings E( \u02dcPpos\ni\n), respectively. \u03bb is a positive constant. Please refer to Appendix B.3 for the inspiration\nof the formula, and the statistical analysis for the choice of the hyperparameter \u03bb.\nFinally, the object embedding cEi in the initial text embeddings c = E(P) is modified by:\n\u02c6cEi = cEi + \u03b1i \u00b7 vpos\ni\n\u2212\u03b2i \u00b7 vneg\ni\n(4)\n3.2\nNeighbor-guided vector estimation\nIn practice, we find that using a single object to estimate the binding vector can be inaccurate and fail\nto disentangle concepts (see Fig. 7 and Fig. 19). In this case, we introduce the neighbor strategy\n2The same object in different prompts may have different positive and negative attributes.\n4\nto ensure an accurate estimation. These neighbor objects should have similar representations to\nthe target object in the learned textual space. We define a candidate set S = {B1, ..., BR} with R\nobjects that has pre-processed to {cB1, ..., cBR}, which is the collection of the word embedding cBr\nin E(Br) = {cSOT , cBr, cEOT , ...}, r = 1, ..., R. The top-K neighbor objects for the target object\nEi are determined by d(cBr, F(Ei, \u02dcPuc\ni )), where Br \u2208S, d(\u00b7) denotes the cosine similarity.\nIn Appendix B.4, we describe this neighbor strategy in detail, and further discuss a way to predict\nsemantic neighbors using pre-trained large language models.\nWith the selected neighbors of the target object Ei, denoted {B(i)\nk }K\nk=1, we compose the unconditional\nconcepts \u02dcPuc\nk\n= {\u2205&B(i)\nk }, positive concepts \u02dcPpos\nk\n= {Ai&B(i)\nk }, and negative concepts \u02dcPneg\nk\n=\n{Aj&B(i)\nk |j = 1, ..., M, j \u0338= i}. The estimation of the binding vector is then rewritten as:\nvpos\ni\n= 1\nK\nK\nX\nk=1\n(F(B(i)\nk , \u02dcPpos\nk\n) \u2212F(B(i)\nk , \u02dcPuc\nk ))\n(5)\nvneg\ni\n= 1\nK\nK\nX\nk=1\n(F(B(i)\nk , \u02dcPneg\nk\n) \u2212F(B(i)\nk , \u02dcPuc\nk ))\n(6)\n3.3\nOverall workflow\nFig. 3 depicts the workflow of Magnet. The target text embedding \u02c6c can be obtained after replacing\nall object embeddings cEi with \u02c6cEi, i = 1, ..., M. To generate the image, the pre-trained U-Net\ndenoises the latent zt\u22121 = \u03f5\u03b8(zt, t, \u02c6c), where timesteps t = T, ..., 1. We set the hyperparameters\n\u03bb = 0.6, K = 5. Please refer to Appendix C for implementation details.\n4\nExperiments\n4.1\nDatasets\nWe evaluate our proposed Magnet on two existing benchmarks:\n(1) Attribute Binding Contrast set (ABC-6K) [8]. This dataset consists of natural compositional\nprompts from MS-COCO [20], each prompt includes at least two concepts (e.g., \"a bathroom with a\ntan sink and white toilet\", \"a brown cow standing in a lush green field\"). We randomly sample 600\nprompts from this dataset and generate 5 images per prompt to compare all methods.\n(2) Concept Conjunction 500 (CC-500) [8]. The dataset contains prompts that conjunct two\nconcepts, each with one color attribute. Following [7], objects are divided into two types: living\n(i.e., animals and plants) and other non-living nouns. Prompts type is categorized into (1) two living\nobjects, (2) one living object and one non-living object, and (3) two non-living objects. We adopt 80\nprompts for each case to avoid bias and maintain fairness. In total, we have used 240 prompts and\ngenerated 10 images for each prompt to compare all methods.\nBoth datasets are augmented using contrast settings [21]. The position of attribute words for different\nobjects is swapped (e.g., \"a red chair and a blue cup\" \u2194\"a blue chair and a red cup\").\n4.2\nMetrics\nWe mainly rely on human evaluation since the common metrics (e.g. CLIP text-image similarity) are\nunreliable for assessing attribute binding, which is discussed in Appendix D.\nCoarse-grained comparison. We assess the generated image for image quality and concept disentan-\nglement on two adopted datasets. To measure image quality, human evaluators were asked \"Which\nimage is more realistic or visually appealing?\". The evaluation of concept disentanglement is divided\ninto two types: (1) object disentanglement by asking \"Which image shows different objects more\nclearly?\"; (2) attribute disentanglement by asking \"Which image shows different attributes more\nclearly?\". If all images are equally good or bad, evaluators can indicate \"no winner\". We randomly\nsample one image for each prompt from ABC-6K and two images for each prompt from CC-500 to\nconduct this coarse-grained comparison.\n5\nTable 1: Coarse-grained comparison on the ABC-6k and CC-500 datasets for image quality, object\ndisentanglement, and attribute disentanglement. Values are normalized to sum to 100.\nABC-6K\nCC-500\nImage\nDisentanglement\nImage\nDisentanglement\nQuality\nObject\nAttribute\nQuality\nObject\nAttribute\nMagnet (Ours)\n26.57\n25.71\n27.14\n25.43\n24.86\n29.43\nAttend-and-Excite\n15.43\n21.43\n19.71\n22.86\n26.29\n18.57\nStructure Diffusion\n12.28\n7.14\n10.29\n12.29\n6.86\n11.14\nStable Diffusion\n10.29\n6.57\n8.57\n11.14\n7.71\n13.42\nNo Winner\n35.43\n39.15\n34.29\n28.28\n34.28\n27.44\nTable 2: Fine-grained comparison on the CC-500 dataset. For reference, we provide the average\nconfidence (Conf.) of GroundingDINO [22] to detect the object (Det.). Manual evaluation concerns\nthe object existence (Obj.) and the attribute alignment (Attr.).\nAutomatic\nManual\nRuntime\nMemory Usage\nMethod\nDet.\nConf.\nObj.\nAttr.\n(s)\n(GB)\nStable Diffusion\n71.5\n56.4\n65.8\n59.1\n6.62\n6.1\nStructure Diffusion\n72.1\n56.0\n64.0\n63.9\n7.94 (+20.0%)\n7.0 (+14.7%)\nAttend-and-Excite\n84.3\n62.6\n84.6\n66.2\n13.4 (+102.4%)\n15.6 (+155.7%)\nMagnet (Ours)\n76.5\n59.8\n68.6\n74.0\n6.81 (+2.9%)\n6.5 (+6.5%)\nFine-grained comparison. This comparison is conducted on the CC-500 dataset based on two\nkey criteria: (1) object existence, counting the target objects in the generated images; (2) attribute\nalignment, concerning the correct binding between the object and its attribute. We ask annotators\nto identify the object mentioned in the prompt per generated image. Take prompt \"a red car and a\nyellow cat\" as an example, each image will be indicated the number two (show both objects), one\n(show either \"car\" or \"cat\"), or zero (no distinct object). Attribute alignment is assessed by counting\nwhether the generated object presents the desired attribute (maximum to the number of the generated\nobjects). All generated images on CC-500 are used for this fine-grained comparison. In addition,\nwe adopt the phrase grounding model GrondingDINO [22] to detect the target objects automatically.\nNote that this automatic detection can not reflect the proper binding.\n4.3\nQuantitative comparison\nCoarse-grained comparison. In Tab. 1, we present the human evaluation results of Magnet compared\nto three baseline methods: SD V1.4 [23], Structure Diffusion [8] and Attend-and-Excite [7]. Note\nthat Magnet and Structure Diffusion are both training-free. The ABC-6K benchmark has more\ncomplicated and challenging prompts. In this case, all methods may fail to include all objects and\nattributes, resulting in a higher number of no winner. Overall, Magnet achieves the best scores in\nterms of image quality and attribute disentanglement on both datasets.\nFine-grained comparison. As shown in Tab. 2, Magnet alleviates the missing problem more than\nStructure Diffusion on both automatic and manual evaluation, with 3.8% (Det.) and 4.6% (Obj.)\nimprovement. We are inferior to the optimization method, Attend-and-Excite in object existence.\nIn attribute alignment (Manual Attr.), Magnet outperforms all baseline methods. In addition, we\ncompare the runtime and memory used for generation. The data is obtained by generating 100\nprompts each with two images. Obviously, Attend-and-Excite requires more resources which affects\nefficiency. Conversely, Magnet only adds 2.9% to runtime and 6.5% to memory.\nEvaluation on image quality metric. We also evaluate Magnet on the commonly used metric FID\n[24] for two SD versions (V1.4 [23] and V2.1 [25]). We follow the standard evaluation process and\ngenerate 10k images from randomly sampled MS-COCO [20] captions. SD V1.4 gets 19.04, with\nMagnet 18.92; SD V2.1 gets 19.76, with Magnet 19.20, the lower the better. This shows that Magnet\nwill not deteriorate the image quality while improving the text alignment.\n6\nStable \nDiffusion\nStructure \nDiffusion\nMagnet \n(Ours)\nAttend-and- \nExcite\na pink cake with  \nwhite roses on silver plate\nsome blue bananas with  \nlittle yellow stickers on them\na red chair and \na yellow dog\na green apple and \na brown sheep\nhot dog with fries \non yellow plate on red tile\nFigure 4: Qualitative comparison using prompts from ABC-6K and CC-500 datasets. For each\nprompt, we show the image generated by each method under the same seed.\na blue strawberry \n on a red plate\nStable \nDiffusion\nMagnet \n(Ours)\nAttend-and- \nExcite\na yellow shelf with bunches of \n  ripe purple bananas \nplaced together\ntwo green bears next to \n each other on a patch of \n  brown grass\na yellow apple \nand red bananas\na lone, green fire hydrant \nsits in red grass\nFigure 5: Prompts with unnatural concepts. Baselines generate exchanged colors (row 1) or unwanted\nartifacts (row 2) while Magnet demonstrates the anti-prior ability with high-quality outputs.\n4.4\nQualitative comparison\nFig. 4 shows the qualitative comparison of the ABC-6K and CC-500 datasets. The results demonstrate\nthat baselines suffer from the entanglement of objects and attributes.\nObject entanglement includes the neglect of the object or sticking structures. In columns 1-2,\nbaselines struggle to be faithful to the complex prompt with 4 objects, missing \"fries\" or \"tile\".\nIn columns 5-6, the objects \"banana\" and \"stickers\" are indistinguishable. Similarly, SD presents\nblended objects \"dog\" and \"chair\" in columns 7-8 and neglects the target object \"green apple\"\nin columns 9-10. Note that the results of Structure Diffusion resemble that of SD. On the other\nhand, the optimization of Attend-and-Excite encourages the attendance of objects but leads to\nout-of-distribution results, showing strong artifacts (e.g., \"green apple\" in columns 9-10).\nAttribute entanglement includes the generation of incorrect attributes or the leakage of attributes.\nFor instance, for the prompt \"a pink cake with white roses on silver plate\" with three colors in\ncolumns 3-4, SD and Structure Diffusion generate \"white cake\" and \"pink roses\". In columns 7-8,\nthey generate \"chair\" with mixed colors \"yellow\" and \"red\". On the other hand, Attend-and-Excite\nmay produce less aesthetic images, which can be attributed to the over-optimized image latent.\nNotice that baselines fail to produce unnatural concepts like \"blue banana\" in columns 5-6 in Fig.\n4. Instead, they generate \"yellow banana\", which is a natural concept learned as the prior knowledge.\nConversely, Magnet is capable of disentangling different concepts and hence generating unnatural\nconcepts, which we call the anti-prior ability. Fig. 5 displays the results on prompts with anti-prior\nconcepts. We skip Structure Diffusion for its limited improvement over SD.\n7\nStable \nDiffusion\n(Ours)\nFigure 6: Ablation study on the hyperparameter \u03bb given the prompt \"a pink cake with white roses\non silver plate\". A small value of \u03bb can not well disentangle different concepts, while a large value\ncauses artifacts in the generated image (best viewed zoomed in). We empirically set \u03bb = 0.6.\na red chair and a blue cup\nw/o neighbor\nw/ neighbor\n(a) attribute disentanglement\nw/o neighbor\nw/ neighbor\n(b) object disentanglement\na blue apple and a green backpack\nFigure 7: Ablation study. The neighbor strategy im-\nproves the binding vector estimation, separating dif-\nferent attributes (\"cup\" is purely \"blue\") and objects\n(\"backpack\" and \"apple\" are distinguishable).\nTable 3: Ablation study. Human evalua-\ntors were asked to indicate which image\ncan better separate attributes or objects.\nDisentanglement\nObject\nAttribute\nw/ neighbor\n27.1\n28.6\nw/o neighbor\n9.1\n6.4\nStable Diffusion\n2.3\n2.1\nNo winner\n61.5\n62.9\n4.5\nAblation study\nHyperparameter \u03bb. We study the effect of \u03bb in Fig. 6. When setting \u03bb = 0, \u03b1, \u03b2 are still positive\nnumbers but the manipulation is in relatively low strength. In this case, concepts are still entangled:\n\"roses\" appear in shades of \"white\" and \"pink\". When setting \u03bb = 1, the result presents artifacts:\ndistorted \"plate\" and watermarked background. We find using \u03bb = 0.6 can achieve the balance\nbetween concept disentanglement and image quality based on the statistic analysis in Fig. 16.\nSelection strategy of the neighbor strategy. The effectiveness of the neighbor strategy is shown in\nFig. 7. The neighbors improve the estimation accuracy and the disentanglement of concepts. In Tab.\n3, we ask human evaluators to evaluate both settings using the disentanglement criteria. Evaluators\nindicate the generated images using the neighbor strategy more disentanglement. This verifies the\neffectiveness of the neighbor-guided vector estimation.\nEffectiveness of the binding vector. In Fig. 8, we verify the effectiveness of the binding vector by\nmanually changing \u03b1, \u03b2 instead of adaptively calculating by Eq. (3). The value of \u03b1, \u03b2 changed from\npositive to negative shows a swapped binding between objects and attributes. This is because that\nthe context problem in padding embeddings has caused the entanglement of concepts. Our proposed\nbinding vectors can improve the discrimination between objects and lead to designated attributes.\nWe have conducted additional ablation experiments for the hyperparameter K (Appendix E.1, Fig.\n19), and the importance of using both positive and negative binding vectors (Appendix E.2, Fig. 20).\n4.6\nExtensions\nIncorporate with optimization-based methods. Manipulated in the textual space, Magnet can be\nreadily integrated with Attend-and-Excite. Fig. 9 (a) compares the optimization loss of Attend-and-\nExcite with and without Magnet. The loss can start at a lower value with Magnet to strengthen the\ndistinction between concepts. Fig. 9 (b) shows vanilla Attend-and-Excite with strong artifacts or\ninaccurate colors, which should be attributed to the entangled concept representations in padding\nembeddings. More examples are displayed in Fig. 23 in the Appendix.\nDifferent text encoders. In Fig. 10 (a) and (b), we assess Magnet on three T2I models with different\ntext encoders to SD V1.4. Specifically, SD V2.1 [25] adopts CLIP ViT-H/14, SDXL [10] combines\n8\na yellow           towel           and a white           bowl\nFigure 8: Ablation study on the effectiveness of the binding vector.\nAttend-and-Excite\n+Magnet\na green bench and a yellow cat\na blue apple and a green vase\nAttend-and-Excite\n+Magnet\n(a) Loss Comparison\n(b) Qualitative Comparison\nFigure 9: Magnet can be combined with the optimization method, Attend-and-Excite [7]. (a) Magnet\nimproves the loss during optimization. (b) Magnet improves the disentanglement of concepts.\nmultiple CLIP text encoders, and PixArt [26] uses the T5 encoder [6]. We use the same setting of all\nhyperparameters and equations for all CLIP-based models while using fixed strength for PixArt. The\nredesign of the strength formula for the adaptation of T5 is a matter for future work.\nIncorporate with T2I controlling modules. In Fig. 10 (c) and (d), we investigate the plug-and-play\nnature of Magnet. Magnet shows compatibility when integrated with existing controlling modules:\n(1) layout-guidance [27], which constrains the image layout by bounding boxes and intervenes\ncross-attention layer, and (2) ControlNet [28] conditioned on Depth Map [29] to add spatial control.\nImage editing. In Fig. 11, we compare the image editing ability of Magnet to Prompt-to-Prompt\n(P2P) [15], which edits the generated image by manipulating the cross-attention layers. Given the\nsource prompt \"a car on the side of the street\", we aim to change the attribute of the object \"car\" or\n\"street\". In column 1, Magnet applies a positive binding vector vpos (here, the strength \u03b1 is stated\nmanually) on the word embedding cEcar toward the attribute \"old\". With no control of the attention\nmaps, Magnet surprisingly edits the image with fewer changes in the background than P2P.\n5\nRelated work\nText-to-image diffusion models. Diffusion models that [30] pioneered, have emerged with great\nimprovement in both unconditional [31, 32] or conditional [28, 33] image generation, together with\nthe advance in synthesis quality [34, 35] and sampling speed [36, 37, 38]. However, the semantic\nflaw of the text encoder affects the performance of the diffusion models [7, 10, 39]. In this work, we\ndiscern the attribute bias and the context issue, providing novel insights about attribute binding.\nAttribute binding. The binding problem occurs when the model blends improper concepts. To\ntackle complicated prompts, [9] collaborates different pre-trained diffusion models. [8] suggests word\nembeddings with blended context and manipulate cross-attention features. In contrast, we highlight\nthe entanglement of the padding embedding and modify solely the text embedding. [7] optimizes the\nlatent to guarantee the attendance of each object. Yet, the optimization may lead to out-of-distribution\nand require more resources to generate images. Other works [40, 41, 42] introduce layout constraints\nin the attention layers. Magnet differs from the above approaches in that it can be executed entirely in\nthe textual space. This distinguishes it as a more efficient solution.\nIt is noteworthy that a line of works [15, 43] achieves image editing on specific visual aspects.\nHowever, none have gone as far as this paper in exploring the contextual influences on SD from the\nperspective of text embedding. Most are subject to a subset of attributes (e.g., texture [44]), control\nthe global object rather than fine-grained attributes [45, 46], or depend on a predefined text pair [47],\nrequiring a learning process or additional datasets. Conversely, our method enhances binding towards\narbitrary attributes without the need for new inputs to the standard pipeline.\n9\nBounding \nBoxes\na black cat \nsitting in a \nred flower pot\na fluffy sofa with leather pillow on it\na lone, green fire hydrant sits in red grass\nSDXL\n+Magnet\nPixArt\na brown backpack and a blue cat\nSD 2.1\n+Magnet\na small cactus with a happy face in the Sahara desert\nSD\n+Magnet\nlayout-guidance\n+Magnet\na lone, green fire hydrant sits in red grass\n(c)\n(d)\n(a)\n(b)\na smiling teddy bear with white bow\nControlNet\n+Magnet\nDepth map\n+Magnet\nFigure 10: Magnet can be integrated into other T2I models and with existing controlling modules.\nSource Prompt: \n\"a car on the side of  \nthe street\"\nStable Diffusion\nPrompt-to-Prompt\nMagnet (Ours)\n\"... old car ...\"\n\"... crushed car ...\"\n\"... sport car ...\"\n\"... snowy street \"\n\"... flooded street\"\n\"... forest street\"\nFigure 11: Image editing comparison using prompts from Prompt-to-Prompt [15].\n6\nLimitations\nWhile we have demonstrated improvement in the synthesis quality and text alignment, Magnet is still\nsubject to a few limitations (see Fig. 21). First, it still suffers from the missing problem. In some\ncases, the manipulation may be overstrength and cause artifacts. An interesting phenomenon is that\nMagnet generates the correct concepts while rendering errors in positional relations. Finally, it is still\nchallenging to generate an unnatural concept when the object is strongly biased towards one specific\nattribute. (e.g., \"broccoli\"). We have described the limitations of Magnet in detail in Appendix F.\n7\nConclusion\nIn this work, we propose a novel training-free method, Magnet, to tackle the attribute binding issue.\nFirst, we conduct a fine-grained analysis of the CLIP text encoder. We observe the phenomenon of\nattribute bias and point out the context issue of padding embeddings, where the representations of\ndifferent concepts are entangled, and hence provide potential explanations for existing T2I issues.\nSecond, we introduce the positive and negative binding vectors to enhance the binding within the\nconcept and strengthen the distinction between concepts. Further with the neighbor strategy, the\nvector estimation can be more accurate. Evaluated in various ways, Magnet shows the ability to\ndisentangle different attributes and generate anti-prior concepts. Performed in the textual space,\nMagnet improves the synthesis quality and text alignment, with an impressively low increase in\ncomputational cost. We sincerely hope that this work will motivate the exploration of generative\ndiffusion models and the discovery of other interesting phenomena.\nAcknowledgements\nThis work was supported in part by the Natural Science Foundation of China (No. 62272227), and\nthe Postgraduate Research & Practice Innovation Program of NUAA (No. xcxjh20231604).\n10\nReferences\n[1] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3,\n2022.\n[2] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 10684\u201310695, 2022.\n[3] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. Advances in\nNeural Information Processing Systems, 35:36479\u201336494, 2022.\n[4] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5,\n2022.\n[5] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021.\n[6] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.\n[7] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite:\nAttention-based semantic guidance for text-to-image diffusion models. ACM Transactions on\nGraphics (TOG), 42(4):1\u201310, 2023.\n[8] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato\nBasu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for\ncompositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022.\n[9] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional\nvisual generation with composable diffusion models. In European Conference on Computer\nVision, pages 423\u2013439. Springer, 2022.\n[10] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\n[11] Yingtian Tang, Yutaro Yamada, Yoyo Zhang, and Ilker Yildirim. When are lemons purple? the\nconcept association bias of vision-language models. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing, pages 14333\u201314348, 2023.\n[12] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When\nand why vision-language models behave like bags-of-words, and what to do about it? In The\nEleventh International Conference on Learning Representations, 2022.\n[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[14] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[15] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\n11\n[16] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion\nfor editing real images using guided diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 6038\u20136047, 2023.\n[17] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng Song, Mengwei Ren, Ruijiang Gao,\nAnastasis Stathopoulos, Xiaoxiao He, Yuxiao Chen, et al. Proxedit: Improving tuning-free real\nimage editing with proximal guidance. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 4291\u20134301, 2024.\n[18] Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz Khan, Qibin Hou, Yaxing Wang,\nand Jian Yang. Get what you want, not what you don\u2019t: Image content suppression for text-to-\nimage diffusion models. arXiv preprint arXiv:2402.05375, 2024.\n[19] Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D Manning. Stanza:\nA python natural language processing toolkit for many human languages. arXiv preprint\narXiv:2003.07082, 2020.\n[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pages 740\u2013755. Springer, 2014.\n[21] Matt Gardner, Yoav Artzi, Victoria Basmova, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep\nDasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, et al. Evaluating models\u2019 local\ndecision boundaries via contrast sets. arXiv preprint arXiv:2004.02709, 2020.\n[22] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection. arXiv preprint arXiv:2303.05499, 2023.\n[23] Compvis.\nStable Diffusion v1-4 Model Card, https://huggingface.co/CompVis/\nstable-diffusion-v1-4. 2022.\n[24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017.\n[25] Stabilityai. Stable Diffusion v2-1 Model Card, https://huggingface.co/stabilityai/\nstable-diffusion-2-1. 2022.\n[26] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang,\nJames Kwok, Ping Luo, Huchuan Lu, et al. Pixart-alpha: Fast training of diffusion transformer\nfor photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.\n[27] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention\nguidance. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision, pages 5343\u20135353, 2024.\n[28] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image\ndiffusion models. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 3836\u20133847, 2023.\n[29] Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards\nrobust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE\ntransactions on pattern analysis and machine intelligence, 44(3):1623\u20131637, 2020.\n[30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin neural information processing systems, 33:6840\u20136851, 2020.\n[31] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. arXiv\npreprint arXiv:2011.13456, 2020.\n12\n[32] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic\nmodels. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.\n[33] Ziqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. Collaborative diffusion for multi-\nmodal face generation and editing. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6080\u20136090, 2023.\n[34] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in neural information processing systems, 34:8780\u20138794, 2021.\n[35] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\n[36] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space\nof diffusion-based generative models. Advances in Neural Information Processing Systems,\n35:26565\u201326577, 2022.\n[37] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan Yan. Post-training quantization\non diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 1972\u20131981, 2023.\n[38] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\n[39] Yutong Feng, Biao Gong, Di Chen, Yujun Shen, Yu Liu, and Jingren Zhou. Ranni: Taming\ntext-to-image diffusion for accurate instruction following. arXiv preprint arXiv:2311.17002,\n2023.\n[40] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image\ngeneration with attention modulation. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 7701\u20137711, 2023.\n[41] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and\nMike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffu-\nsion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n7452\u20137461, 2023.\n[42] Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong\nTang, Yujiu Yang, and Bin Cui. Realcompo: Dynamic equilibrium between realism and\ncompositionality improves text-to-image diffusion models. arXiv preprint arXiv:2402.12908,\n2024.\n[43] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features\nfor text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 1921\u20131930, 2023.\n[44] Julia Guerrero-Viu, Milos Hasan, Arthur Roullier, Midhun Harikumar, Yiwei Hu, Paul Guerrero,\nDiego Gutierrez, Belen Masia, and Valentin Deschaintre. Texsliders: Diffusion-based texture\nediting in clip space. In ACM SIGGRAPH 2024 Conference Papers, pages 1\u201311, 2024.\n[45] Hu Yu, Hao Luo, Fan Wang, and Feng Zhao. Uncovering the text embedding in text-to-image\ndiffusion models. arXiv preprint arXiv:2404.01154, 2024.\n[46] Zirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, and Zhuowen Tu. Tokencompose: Text-to-\nimage diffusion with token-level supervision. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 8553\u20138564, 2024.\n[47] Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu,\nand Bj\u00f6rn Ommer. Continuous, subject-specific attribute control in t2i models by identifying\nsemantic directions. arXiv preprint arXiv:2403.17064, 2024.\n[48] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1,\npage 2, 2019.\n13\n[49] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis\nGermanidis. Structure and content-guided video synthesis with diffusion models. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 7346\u20137356, 2023.\n[50] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-\nimage pre-training for unified vision-language understanding and generation. In International\nConference on Machine Learning, pages 12888\u201312900. PMLR, 2022.\n14\n(a) CLIP ViT-L/14\nWord embeddings\n[EOT] embeddings\n(b) CLIP ViT-H/14\n[EOT] embeddings\nWord embeddings\nFigure 12: Principal Component Analysis (PCA) analysis of CLIP ViT-H/14 and CLIP ViT-L/14.\nThe word embedding and the [EOT] embedding have a different understanding of the attribute.\nA\nAdditional analysis of the CLIP text encoder and the diffusion model\nA.1\nAnalysis of the CLIP text encoder\nPrincipal Component Analysis (PCA). We study two types of text embedding through the PCA\ntechnique in Fig. 12 for a low-dimensional comparison. We analyze two CLIP text encoders,\nwhich are (a) ViT-L/14 (here, the dimension of embedding is d = 768), and (b) ViT-H/14 (here,\nd = 1024). We obtain text embedding c = {cSOT , cobject, cEOT } without the context of the attribute\nfrom 60 object nouns (including animals, plants, and non-living entities). We have extended the\nnumber of attributes to 16 (including colors and materials), and ended up with 960 text embeddings\nc\u2032 = {c\u2032\nSOT , c\u2032\nattribute, c\u2032\nobject, c\u2032\nEOT }. We use 60 object embeddings cobject or cEOT without the\nattribute context to fit the model, and then transform contextualized embeddings c\u2032\nobject or c\u2032\nEOT\nto the same space. This setting allows us to observe how two types of text embedding understand\ndifferent attributes. The result indicates that the word and [EOT] embeddings produce different\nfeature spaces with the attribute context. Overall, the distribution of the word embedding is denser,\nwhile the [EOT] embedding with attribute context is distributed dispersedly.\nAttribute bias analysis. Fig. 13 investigates the phenomenon we call attribute bias on two types of\ntext embedding, obtained from the text encoder of CLIP ViT-L/14 and ViT-H/14, respectively. The\nword embedding without supervision during training has shown severe attribute bias. For example,\nthe word embedding of the object \"tiger\" indicates an extreme preference for the color \"yellow\".\nConversely, the [EOT] embedding produces a relatively small variation in the similarity curve. In\nthe main paper, we conjecture that VLMs\u2019 poor compositional understanding and the behavior of\nbags-of-words [12, 11] on [EOT] lead to an inaccurate textual representation, which affects the\ninteraction between the image latent and semantic word embeddings.\nInterestingly, we find that the learned representations of two text encoders are quite different. For\nexample, to encode \"car\" and \"vase\" with the context of different attributes, CLIP ViT-L/14 gets the\ncosine similarity around 0.6 v.s. 0.7, while CLIP ViT-H/14 gets 0.2 v.s. 0.5, showing a discrepancy.\nWe conjecture that ViT-L/14 in a large-sized network and dimension may have exacerbated the bias.\nYet, it is beyond the scope of our research and we may leave it for future study.\nDespite the above difference, both encoders demonstrate the discrepancy between the word and [EOT]\nembeddings. The stability of the [EOT] embedding can also be explained by entangled context. In\ncontrast, the word embedding without supervision during training may suffer less from entanglement.\nA.2\nFine-grained cases analysis\nTake the concept \"red chair\" as an example.\nThe CLIP text encoder maps it into em-\nbeddings c\u2032\n= {c\u2032\nSOT , c\u2032\nred, c\u2032\nchair, c\u2032\nEOT , c\u2032\npad1, ..., c\u2032\npad73} (here, 73 for L \u22124, L = 77).\nThe counterpart text embedding of the concept \"chair\" without the color modifier is c =\n15\n\"[color] banana\"\u00a0\n\"[color] car\"\u00a0\n\"[color] cat\"\u00a0\n\"[color] vase\"\u00a0\n\"[color] tiger\"\u00a0\n\"[color] broccoli\"\u00a0\nCLIP ViT-L/14\nCLIP ViT-H/14\nFigure 13: The attribute bias of different objects encoded by CLIP ViT-H/14 and CLIP ViT-L/14.\nThe word and [EOT] embeddings show large discrepancies of attribute bias for the objects \"banana\",\n\"broccoli\", etc. Observe that the extracted embeddings by different text encoders differ significantly.\n(1)\n\" green car\"\n(2)\n(4)\n(3)\n\" blue banana \"\n\" red cat \"\n(1)\n(2)\n(4)\n(3)\n(1)\n(2)\n(4)\n(A)\nAdditional Cases\n(B)\n(C)\n(A)\nAdditional Cases\n(B)\n(C)\n(A)\nAdditional Cases\n(B)\n(C)\n(3)\nFigure 14: Fine-grained 4 cases described in the main paper, as well as 3 additional cases.\n{cSOT , cchair, cEOT , cpad1, ..., cpad74} (here, 74 for L \u22123).\nThe designed 4 cases are de-\nfined as:\n(1) standard generation conditioned on vanilla embeddings of the concept, i.e.,\nccase1 = c\u2032; (2) replace the contextualized word embedding in c\u2032, i.e., ccase2 = {c\u2032\nSOT , c\u2032\nred,\ncchair, c\u2032\nEOT , c\u2032\npad1, ..., c\u2032\npad73}; (3) replace all [EOT] and padding embeddings, i.e., ccase3 =\n{c\u2032\nSOT , c\u2032\nred, c\u2032\nchair, cEOT , cpad1, ..., cpad73}; (4) replace the contextualized word, [EOT] and\npadding embeddings, i.e., ccase4 = {c\u2032\nSOT , c\u2032\nred, cchair, cEOT , cpad1, ..., cpad73}. Note that we\nmaintain the attribute word embedding c\u2032\ncolor to observe whether the model can capture the color\ninformation without contextual information in other text embeddings. The results are displayed in\nFig. 14. As we discussed in the main paper, cases 1-2 where padding embeddings with the color\ncontext are still realistic when the concept is natural (i.e., \"green car\"). However, they generate\nout-of-distribution images for the examples \"red cat\" and \"blue banana\".\nIn addition, we have designed 3 new cases to verify that the color information has been gradually\nforgotten in the padding embedding. We divide all [EOT] and padding embeddings into 3 groups: cX\n= {cEOT , ..., cpad23}, cY = {cpad24, ..., cpad49}, cZ = {cpad50, ..., cpad73} (here, these embeddings do\nnot have the color context), and their counterparts c\u2032\nX, c\u2032\nY , c\u2032\nZ (here, these embeddings with the color\ncontext). The results in Fig. 14 (bottom) are consistent with our hypothesis. To be specific, cases\nA and B show light \"green\" or invisible \"red\" compared to the successful binding results in case C,\nwhere embeddings {cEOT , ..., cpad23} are contextualized with the target color.\n16\n(a) Single-concept context issue\n(b) Multi-concept context issue\nFigure 15: Several effects of the context issue in padding embeddings under the scenarios of (a)\nsingle-concept and (b) multi-concept. We refer to the detailed analysis in Appendix A.3.\nNOTICE: we propose Magnet based on two key observations on the word embedding. First, the\ntarget color is invisible in case 4 for concepts \"green car\", \"red cat\". However, these colors can be\nobserved in case 3 (arrive at Eq. (1) for computing vpos). Second, cases 1-2 and cases 3-4 for the\nconcept \"blue banana\" both generate catastrophic images. This indicates the vector estimated by the\nobject itself can be inaccurate. In this case, we introduce the neighbor-guided vector estimation.\nA.3\nVisualization-based analysis of the context issue\nRecall our hypothesis that [EOT] and padding embeddings are trying to remember all important\ninformation (e.g., attributes, objects, and positions) in the given prompt due to the contrastive learning\nand bags-of-words behavior of CLIP. In Fig. 15, we investigate the entangled context in the padding\nembeddings under two scenarios for prompts with a single object or multiple objects.\nSingle-concept scenario aims to generate one object with specific attributes. Fig. 15 (a) shows\nthat the context issue in padding embeddings leads to (1) out-of-distribution and inaccurate\nobject structures, e.g., \"cat\" is painting-like in row 1, \"banana\" is unrecognizable in row 2, though\npresenting correct attributes. Or (2) generate the object with another attribute that can compose\na natural concept, e.g., \"broccoli\" binds to the prior attribute \"green\" rather than \"black\" in row 3.\nOne potential explanation is the image latent is contaminated by inaccurate representation in padding\nembeddings, as evidenced by the overlapped activation of latter padding embeddings with the word\nembedding of each object. The generation of natural concepts proves our hypothesis that latter\npadding embeddings forget attribute context if the object has a preference for certain attributes based\non the training dataset. In row 4, we present an interesting observation that padding embeddings are\naligned with the attribute word rather than the object \"strawberry\". It seems that the word \"gold\" is\ninterpreted as an entity instead of a visual feature, leading to (3) a split of the target object.\nMulti-concept scenario aims to generate multiple objects with the desired attributes. Fig. 15 (b)\nshows that the context issue in padding embeddings leads to (4) color leakage, i.e., one object\npresents the attribute belonging to another object in row 1. Or (5) objects stick together, e.g.,\na strange creature with the head of a \"horse\" but the body of a \"bag\" in row 2. All the above\nphenomena can be attributed to the evident entanglement in padding embeddings with overlapped\ncross-attention activations, which provides inaccurate object representation and indistinguishable\nbinding relationships for each concept. Note that the above effects can occur simultaneously on a\nsingle instance: row 3 indicates an inaccurate \"sheep\" structure, a binding between \"banana\" and\nthe prior color \"yellow\", a split of the object \"banana\", as well as a sticking problem between two\nobjects \"banana\" and \"sheep\". In row 4, we find that the context issue of padding embeddings also\nexplains (6) the issue of missing objects, i.e., the context loses the object \"sheep\" and contains a\ndominant representation of the object \"car\".\n[18] also discussed the semantic information in padded [EOT] embeddings. While their main concern\nis to remove one specific object content, our focus is the understanding of attribute.\n17\nFigure 16: Statistical analysis of \u03c9 = cos(G( \u02dcPpos\ni\n), H( \u02dcPpos\ni\n)) obtained from 19648 samples (614\nobjects and 32 attributes). We set \u03bb = 0.6 where the count drops.\nB\nDetail of the proposed method\nB.1\nDependency parser\nTo extract the dependency set D = {A1&E1, ..., AM&EM} in the given prompt, we adopt an off-\nthe-shelf dependency parsing module in Stanza Library [19] and construct syntax trees using NLTK.\nFollowing [8], the pair is searched by noun phrases (NPs) in the syntax tree and their corresponding\nadjective words. For instance, given the prompt \"a black cat sitting in a white bowl\", the object\n\"cat\" is extracted according to the label NN or NNs, then allocated its attribute \"black\" in the subtree.\nSimilarly, the object \"bowl\" and its attribute \"white\" can be obtained. However, the parser may fail\nto extract the concepts out of the \"[attribute] [object]\" format. For instance, it can not process the\nprompt \"a photo of a streetlight that is green\" with dependency \"green\"&\"streetlight\", or \"apples of\ngreen are in white baskets\" with dependency \"green\"&\"apples\". We leave this for future work.\nB.2\nBackground of diffusion models\nThe conventional diffusion model [30] works in two steps: (1) forward diffusion that gradually adds\nnoise to the image x; (2) reverse diffusion that removes noise from noisy image xt step-by-step.\nLatent Diffusion Models (LDMs) [2] perform the denoising in the latent space. The pre-trained\nencoder \u03d5 compresses the image x to the latent z = \u03d5(x), and the pre-trained decoder \u03c8 reconstructs\nthe latent as \u03c8(z) \u2248x. The forward diffusion produces the noisy latent zt for the step t =\n1, ..., T. The denoising network \u03f5\u03b8 is trained to remove the added noise at each step by minimizing\n||\u03f5\u03b8(zt, t) \u2212\u03f5||2, where zt is the noisy latent at timestep t, \u03f5 \u223cN(0, 1) is the added Gaussian noise.\nThe noisy latent zT is sampled from Gaussian noise N(0, 1) during inference. Finally, the reversed\nlatent z0 is decoded to produce the image x = \u03c8(z0).\nThe proposed Magnet is applied over Stable Diffusion (SD) conditioned on text prompts. The\npre-trained CLIP text encoder E maps the prompt to the text embedding c = E(P). SD appends\nseveral cross-attention layers to inject the text condition into the latent zt. The loss function of the\ntext-image latent diffusion model can be rewritten as ||\u03f5\u03b8(zt, t, v) \u2212\u03f5||2.\nB.3\nStrength of the binding vector\nThe use of the exponential function is inspired by [18]. But in a different way, Eq. (3) that determines\nthe strength \u03b1i, \u03b2i is based on our observation in Fig. 2 (b) and (c) in Section 2.\nThe formula \u03c9 = cos(G( \u02dcPpos\ni\n), H( \u02dcPpos\ni\n)) calculates the cosine similarity between the first [EOT]\nembedding and the last padding embedding of the concept \u02dcPpos\ni\n. In Fig. 16, we have conducted a\nstatistical analysis using Numpy\u2019s histogram to bin the data. Different values \u03c9 are obtained from\n19648 samples encoded by CLIP ViT-L/14. The highest counts are at the values 0.66 and 0.71.\nObserve that the count drops when \u03c9 < 0.6 or \u03c9 > 0.82. Intuitively, smaller \u03c9 indicates a larger\ndeviation from the target context. Empirically, we set \u03bb = 0.6 in Eq. (3) to enhance the weak binding\n(i.e., \u03b1i > 1 when \u03c9i < 0.6 in e\u03bb\u2212\u03c9i). We have conducted an ablation study of the value \u03c9 in Fig.\n18\n6. For the strength \u03b2i of the negative binding vector, we suggest a relatively slight control to avoid\nstrong deviation when the concept number M is large, i.e., \u03b2i = 1 \u2212\u03c92.\nB.4\nNeighbor-guided vector estimation\nFeature Neighbors. The candidate set S = {B1, ..., BR} used for the feature neighbor strategy\nincludes R words. In practice, we gathered 614 object nouns generated from ChatGPT [13] and\nchecked manually. We extract the word embedding cBR of each candidate object. For example, the\ncandidate \"truck\" is mapped into P(\u201ctruck\u201d) = {cSOT , ctruck, cEOT , ...}. The embedding ctruck\nis extracted and used in the formula d(cBr, F(Ei), \u02dcPuc\ni ). Notice the [EOT] embedding is not used.\nThis procedure of extracting candidates\u2019 embeddings is one-for-all, i.e., we compute 641 embeddings\nonce for each new text encoder and save them to the local path.\nSemantic Neighbors. These neighbor objects are semantically related to the target object. We adopt\nChatGPT [13] to predict the semantic neighbors. The instruction follows the sentence pattern of\n\"Which objects are highly related to the word <*> ?\". Optionally, the large language model BERT\n[48] for fill-mask is considered. We mask the object in the prompt to get its neighbors. For example,\nto predict the neighbor object for \"brown bear\", the masked prompt is composed as \"brown bear and\na [MASK].\". We hypothesize the conjecture \"and\" can implicitly restrict the close relation. The first\ntwo nouns output by BERT are \u201cwolf\" and \u201clion\", which are similar objects to \u201cbear\".\nC\nImplementation details\nConfigure. All experiments are conducted on RTX 3090 in a single GPU. Our proposed Magnet is\nbuilt upon SD V1.4 [23] with the pre-trained text encoder of CLIP ViT-L/14 [5].\nHyperparameters. The choice of \u03bb = 0.6 is explained in Appendix B.3 and verified by the ablation\nstudy in Fig. 6. We set K = 5 to conduct qualitative and quantitative experiments. We have discussed\nother choices of K in Appendix E.1. We generate images with 50 diffusion steps with a fixed\nclassifier-free guidance scale of 7.5.\nBaselines. We compare Magnet to SD V1.4 [2], the training-free method, Structure Diffusion [49],\nand the optimization method, Attend-and-Excite [7]. Since the official Attend-and-Excite does\nnot provide an automatic parsing process, we extract the required object words (in bold) using the\nStanza\u2019s package (same to Magnet, see Appendix B.1).\nDatasets. We have conducted statistics on the CC-500 dataset based on the three types of classification.\nWe find the number of valid prompts for each type are 84, 212, and 136, respectively. This data bias\nmay lead to unfair comparisons. In this case, we randomly select 80 prompts per type and obtain 240\nprompts in total.\nResource. The runtime to generate an image and the required maximum GPU resources for each\nmethod are listed in Tab. 2. The data of each method is obtained by generating 200 images (randomly\nsampling 50 prompts from each dataset and generating 2 images per prompt). Each method is tested\nunder the same setting to maintain fairness.\nD\nMetric discussion\nWe rely on human evaluation since the commonly used metrics for text-to-image synthesis are\nunreliable for our concern about attribute binding. We discuss three models as the automatic\nevaluation metrics, which are retrieval models CLIP [5] and BLIP [50], as well as the phrase\ngrounding model GroundingDINO [22].\nFig. 17 (a) shows the drawback of CLIP score, which computes the cosine similarity between the\ntext and the image embeddings. Failure and success cases present relatively equal values. The [EOT]\nembedding suffers from attribute bias and can not measure the unnatural concept \"blue apple\".\nSimilar to CLIP, the metric of BLIP score in Fig. 17 (b) diverges from the human evaluator. Given\nthe target prompt with multiple concepts, the image of SD (top) presents entangled attributes and\nobjects. In this case, human evaluators indicate no instance of object existence and attribute alignment.\nHowever, BLIP text-image similarity can not align with the assessment of human evaluators.\n19\nBLIP\nMatch\nProb.\n99.97%\nText-Image\nSim.\n0.496\nTarget Prompt:  a pink cat and a brown sheep \nHuman\nObject \nExistence\ncat(\u2718) \nsheep(\u2718 )\nAttribute \nAlignment\ncat(\u2718) \nsheep(\u2718)\nBLIP\nMatch\nProb.\n99.95%\nText-Image\nSim.\n0.504\nHuman\nObject \nExistence\ncat(\u2714) \nsheep(\u2714 )\nAttribute \nAlignment\ncat(\u2714) \nsheep(\u2714)\n(b) BLIP Score\nSD\nMagnet\n(a) CLIP Score\nTarget Prompt:  a blue apple \nScore: 17.82\nScore: 17.56\nScore: 17.14\nScore: 17.22\n(c) GroundingDINO\nObject\nModel\nHuman\nbird\n\u2714\n\u2718\ncat\n\u2714\n\u2718\nObject\nModel\nHuman\ncake\n\u2718\n\u2714\nclock\n\u2718\n\u2714\n a brown brid and a yellow cat \n a gold cake and a blue clock \nFigure 17: Failure cases of three automatic metrics: (a) CLIP text-image similarity can not assess the\nbinding of unnatural concepts. (b) BLIP text-image similarity fails to capture the entanglement of\nconcepts. (c) The detection of GroundingDINO diverges from human annotators.\nTable 4: Quantitative comparison following Attend-and-Excite [7].\nType\nMethod\nCLIP\nBLIP\nFull Prompt\nMin. Object\nFull Prompt\nMin. Object\nStable Diffusion\n32.40\n22.40\n45.32\n30.35\nTraining-free\nStructureDiffusion\n32.24\n22.38\n44.68\n30.28\nMagnet(Ours)\n33.11\n22.79\n46.53\n30.84\nOptimization\nAttend-and-Excite\n34.12\n24.63\n49.65\n34.83\nIn the main paper, we adopt GrondingDINO [22] to detect the object in the generated image. However,\nit fails to capture the structural deviation and suffers from attribute bias. As shown in Fig. 17 (c),\nthe entangled concepts \"bird\" and \"cat\" are detected by GroundingDINO, which diverges from the\nhuman evaluator. Conversely, the model can not detect \"gold cake\". This may be attributed to the\nattribute bias, which we have discussed in the main paper.\nAdditionally, we follow Attend-and-Excite [7] and compare the full prompt similarity and minimum\nobject similarity using CLIP and BLIP. The quantitative comparison is listed in Tab. 4. Magnet shows\nimprovement on all metrics compared to SD and Structure Diffusion. Meanwhile, we compare the\ntext-text similarity [7] using the BLIP model for image captioning, resulting in SD (66.08), Structure\nDiffusion (65.71), Magnet (68.22), and Attend-and-Excite (71.22) as the highest. However, we do\nemphasize that the above quantitative metrics can not reflect the disentanglement of objects and\nattributes that we are concerned about.\nIn conclusion, we refer to the human evaluation to ensure a fair and reliable comparison. A screenshot\nexample of the coarse-grained comparison is given in Fig. 18.\nE\nAdditional ablation experiments\nE.1\nHyperparameter K\nIn Fig. 19, we have conducted an ablation study on the hyperparameter K to select neighbor objects.\nNote that the positive and negative vectors are estimated by each object Ei itself when K = 1 as\nEq. (1). The difference is slight if concepts in the target prompt are relatively natural. For example,\nin row 2, using K = 1, 3, 5 (column 2-4) can generate the correct concept \"red ball\" compared to\n\"white ball\" in SD. However, the results of K = 1 (column 2) in rows 3-4 present a catastrophic\nstructure of \"blue bananas\". This verifies the effectiveness of the neighbor strategy. On the other\nhand, K in a large number can lead to inaccurate binding vectors. For example, in rows 1-2, results\nof K \u226510 are similar to SD. This can be attributed to the introduction of a multitude of unrelated\nobjects that have an impact on the estimation accuracy. Similarly, \"stickers\" are indistinguishable in\nrow 3, columns 7-8 using K = 20, 50.\n20\nFigure 18: A screenshot of the human evaluation for assessing image quality, disentanglement of\nobjects and attributes. For each question, the order of images generated by Magnet and other methods\nis randomized to maintain fairness.\na brown teddy bear sitting on a wooden table next to a red ball\nseed \nA\nseed \nB\nsome blue bananas with little yellow stickers on them\nseed \nA\nseed \nB\n(0.18s)\n(0.25s)\n(0.33s)\n(0.44s)\n(0.81s)\n(1.9s)\n(0.14s)\nStable \nDiffusion\nFigure 19: Ablation study on the hyperparameter K. We emphasize that K = 5 may not always be\nthe best choice because of the randomly initialized latent. For example, the result of K = 3 is more\nappealing than K = 5. We choose K = 5 which can stabilize the generate of unnatural concepts\n(e.g., \"blue bananas\" and \"yellow stickers\" can be more distinguishable in K = 5 than K = 3), as\nwell as balance the processing time.\nInterestingly, when using different seeds, the most visually appealing image may not always come\nfrom the same K. For example, we subjectively prefer the result of K = 3 in row 1, but in row 2 the\nresult of K = 5 is more appealing. This is due to the randomly initialized latent. Since Magnet\u2019s\nresource requirements are relatively low, we believe it is possible to use different K for the same\nprompt and generate images simultaneously for freedom of choice to the user.\nIn conclusion, the reason for the use of K = 5 is the balance between synthesis quality and pre-\nprocessing time for manipulation. Here, we obtain the data of time by processing 20 prompts, i.e.,\nadding 0.25s to SD to generate an image using K = 5. Meanwhile, our code can be improved to\nshorten the time, which is left for future work.\n21\npositive \nonly\nnegative \nonly\nboth\na brown cat and a red cup\n(a)\n(b)  \na brown purse is sitting on a green bench\n(c)\na orange dog wearing an gray bow tie laying on a sofa\nStable \nDiffusion\nFigure 20: Ablation study on negative and positive bind-\ning vectors. (a) depicts similar results. (b) verifies using\nboth vectors can alleviate the missing object (i.e.,\"green\nbench\"). (c) verifies using both vectors can enhance the\nbinding (\"orange dog\" and \"gray bow tie\").\nTable 5: Ablation study on negative and\npositive binding vectors. In most cases,\nthe images generated by three cases are\nequally good or bad, resulting in a high\nnumber of no winner.\nDisentanglement\nObject\nAttribute\nboth\n13.8\n9.4\npositive only\n3.9\n3.0\nnegative only\n5.1\n1.3\nStable Diffusion\n1.4\n0.2\nno winner\n75.8\n86.1\nE.2\nImportance of both positive and negative vectors\nIn Fig. 20 and Tab. 5, we conduct an ablation study on the proposed positive and negative vectors. The\nobject disentanglement is assessed by asking \"Which image shows different objects more clearly?\",\nand the attribute disentanglement by asking \"Which image shows different attributes more clearly?\".\nWe randomly select 12 prompts from CC-500 and 20 prompts from ABC-6K, generating 25 images\nper prompt (800 images in total) for three settings.\nBoth qualitative and quantitative comparisons verify the importance of both vectors. For instance,\nthe concept \"green bench\" is interpreted to \"green grass\" when using only one type of the binding\nvectors. This occurs because of the entanglement of two objects. For the attribute disentanglement,\nusing both vectors is capable of generating objects with desired attributes. Notice that the negative\nvector improves the object disentanglement (presents \"bench\" in column 5), while the positive vector\nimproves the attribute disentanglement (presents \"orange dog\" in column 3). The human evaluation\nresults are consistent with the above analysis, i.e., negative only (5.1) overpasses positive only (3.9)\nin terms of object disentanglement, and positive only (3.0) overpasses negative only (1.3) in terms of\nattribute disentanglement. In conclusion, using both vectors significantly improves text alignment.\nF\nLimitations\nAlthough Magnet provides an efficient and effective way to address the attribute binding problem, we\nacknowledge our technique is subject to a few limitations.\nFig. 21 displays the failure cases of Magnet. First, the neglect of the object (columns 1-2), may\nbe attributed to the model\u2019s limited ability to foreground limited subjects. Second, the excessive\nmanipulation of the object embedding leads to out-of-distribution (columns 3-4). An interesting\nobservation is that Magnet sometimes generates images with correct concepts, but incorrect positional\nrelations (columns 5-6). We suspect that the color layout has been determined in the early stage. In\nthis case, Magnet maps the object to the position of the attribute in the image, rather than blending\nthe attribute with the object. Magnet inherits the well-known issue of T2I models, presenting merged\nobjects (columns 7-8). Finally, it is still challenging to generate an unnatural concept when the object\nhas a strong attribute bias (columns 9-10).\n22\nStable \nDiffusion\nMagnet \n(Ours)\n(b) over-manipulation\na black cat \nsitting in a \nred flower pot\na red bear is nestled \nin a thicket of \nblack flowered brush \na brown and white \nhorse standing in \nfront of a red silo\na blue tabby \nkitten playing with \nnavy grey shoe \nstrings on a shoe\na few brown \nbananas that has \na green bear in it\n(a) neglect of object\na yellow cat and \na brown sheep\na red cake and \na blue suitcase\n(c) wrong positional relation\na large blue polar \nbear swimming in a \npool of white water\n(d) concept entanglement\n(e) strong attribute bias\na green bowl \nfilled with lots of \nwhite broccoli\na green and white \nzebra is eating \nsome black grass\nFigure 21: Limitations of the proposed Magnet. (a) shows two cases that amend the concept, while\nstill missing one object; (b) includes out-of-distribution results caused by the excessive value of \u03b1, \u03b2;\n(c) depicts an interesting phenomenon that Magnet correctly disentangles concepts while failing to in\naccordance with the location word \"in\". (d) shows Magnet will produce entangled concepts due to\nthe limited power of SD. (e) provides two fail cases to generate unnatural concepts.\na long, narrow \nyellow kitchen \nwith black and white \nfloor tiles\nA white dog \nstanding in front\nof brown cabinets\na red car \nand a brown \nelephant\na blue boat \nand a red car\na brown backpack \nand a blue cow\na red and white \ntrain and lots of \nyellow cranes \nand wire\na black and white \nphoto shows a group \nof people dressed in \nblack tie attire \nplaying cards and \nhaving drinks\nSD\nMagnet\nFigure 22: Similar images generated by Magnet and SD.\nAdditionally, Fig. 22 displays examples that Magnet generates similar images with SD. Most happen\nwhen SD has produced relatively faithful images (columns 1-2), or prompts with excessively detailed\nconcepts (columns 3-4), as well as the generation of two unrelated concepts (columns 5-7).\nWe consider combining Magnet with optimization-based methods to tackle the neglect of objects, e.g.,\nthe integration of Attend-and-Excite and Magnet (see Fig. 9 and Fig. 23). Magnet is also compatible\nwith existing T2I controlling modules to address the inability to change spatial relationships, e.g., the\nintegration of ControlNet [28] or layout-guidance [27] and Magnet (see Fig. 10). The excessive or\ninsufficient manipulation may be addressed by improving the formula in Eq. (3), or simply stating\nthe strength \u03b1i, \u03b2i manually. We leave these for our future work.\nG\nAdditional results\nFig. 24 provides examples that Magnet improves the image quality compared to SD.\nIn Fig. 25, we compare Magnet to SD by visualizing the cross-attention activation.\nFig. 26 provides examples of typical indoor scenes using prompts from the ABC-6K dataset.\nFig. 27 and Fig. 28 provide additional qualitative comparisons on the ABC-6K and CC-500 datasets,\nrespectively.\n23\nMagnet \nonly\ntwo brown birds \nsitting on a \ngrey stick\nA&E\nA&E \n+Magnet\nlarge yellow sheets \non a blanket with \na white headboard\na woman riding \na green horse \non a lush black field \nSD\na dog chasing after \na green frisbee on \ntop of a purple lawn\na shaggy brown dog \nhas a red ball \nin it's mouth\nFigure 23: Additional results of extension to Attend-and-Excite. In columns 1-2, Magnet only may\nneglect the object (e.g., \"gray stick\"). In columns 3-4, Magnet can generate images with unnatural\nconcepts but would be painting-like. The combination (row 4) demonstrates improvement. Column 5\ndisplays a failure case. The parameters may need to be modified to fit Magnet.\na white dog sitting inside a red car next to \na string of flowers hanging off the mirror\nmeat with bright white vegetables \n sitting on a green plate\na yellow and brown butterfly \nsitting on top of an orange\na brown bear wearing a pair of red glasses\na large silver pot sitting on a counter \nnext to a red brick wall\na black cat lying on a brown suitcase \nStable Diffusion\nMagnet\nStable Diffusion\nMagnet\nFigure 24: Magnet improves the synthesis quality by disentangling different concepts. Best viewed\nzoomed in.\n24\nStable\nDiffusion\nMagnet\nStable\nDiffusion\nMagnet\nPrompt: hot dog with fries on red plate on yellow tile\nStable\nDiffusion\nMagnet\nStable\nDiffusion\nMagnet\nStable\nDiffusion\nMagnet\nPrompt: some blue bananas with little yellow stickers on them\nPrompt: a bowl of broccoli and red rice with a white sauce\nPrompt: a red and green plate holding a pink cake with frosting\nPrompt: a green fire hydrant sitting on a patch of red grass\nFigure 25: Visualisation of attention maps. The activations of different object are more distinct in\nMagnet compared to SD. For instance, bananas are overlapped with stickers in row 1, while row 2\nindicates disentangled concepts.\n25\nan orange kitchen with a white refrigerator stove oven and dishwasher\na furnished decorated living room with white walls, paintings and a teal sofa\na clean living area with a green sofa with brown pillows\nthe kitchen with white oven atop green tiled floor\nStable Diffusion\nStructure Diffusion\nMagnet (Ours)\nAttend-and-Excite\nFigure 26: Qualitative comparison using prompts from the ABC-6K dataset. We provide some typical\nindoor scene prompts and compare Magnet to baseline methods. Best viewed zoomed in.\n26\nStable Diffusion\nStructure Diffusion\nMagnet (Ours)\nAttend-and-Excite\na black train with three cars is blowing green smoke\na yellow teddy bear in a white shirt against a red wall\na white fire hydrant sitting in front of a yellow fence\na red bench in front of a stone style wall and a bush with blue flowers to the side of it\nFigure 27: Additional results using prompts from the ABC-6K dataset.\n27\nStable Diffusion\nStructure Diffusion\nMagnet (Ours)\nAttend-and-Excite\na black apple and a green backpack\na blue dog and a brown suitcase\na blue sheep and a brown vase\na gold car and a red clock\nFigure 28: Additional results using prompts from the CC-500 dataset.\n28\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper\u2019s contributions and scope?\nAnswer: [Yes]\nJustification: The main contributions and scope have been included in the abstract and\nintroduction.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: The limitations of this work have been discussed in Section 6 and Appendix F\n(see Figure 21).\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n\u2022 The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n\u2022 While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren\u2019t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\n29\nAnswer: [Yes]\nJustification: Section 2 and Appendix A provide the full set of assumptions of the context\nissue in the padding embedding, which is the motivation of the proposed method.\nGuidelines:\n\u2022 The answer NA means that the paper does not include theoretical results.\n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.\n\u2022 The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: Details of the proposed method are described in Section 3 and Appendix B.\nWe clarify that the main experimental results can be reproduced.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n\u2022 While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\n30\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: We provide the used two datasets (ABC-6K and CC-500) and a piece of code\nwith sufficient instructions in the supplemental material.\nGuidelines:\n\u2022 The answer NA means that paper does not include experiments requiring code.\n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n\u2022 While we encourage the release of code and data, we understand that this might not be\npossible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n\u2022 The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\u2022 The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n\u2022 The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n\u2022 At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n\u2022 Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: Section 4.1 describes the detail of datasets. Section 4.2 and Appendix D have\ndescribed evaluation metrics. We provide the implementation details in Appendix C.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n\u2022 The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment Statistical Significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: The information is provided in Appendix C.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n31\n\u2022 The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n\u2022 The assumptions made should be given (e.g., Normally distributed errors).\n\u2022 It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments Compute Resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: The information is provided in Appendix C.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n\u2022 The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n\u2022 The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn\u2019t make it into the paper).\n9. Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: Our research has been conducted following the NeurIPS Code of Ethics.\nGuidelines:\n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\u2022 If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJustification: Section 5, Appendix E.1 and F have discussed both potential and negative\nsocietal impacts of the proposed method.\nGuidelines:\n\u2022 The answer NA means that there is no societal impact of the work performed.\n\u2022 If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n32\n\u2022 Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n\u2022 The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n\u2022 The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: This paper poses no such risks.\nGuidelines:\n\u2022 The answer NA means that the paper poses no such risks.\n\u2022 Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n\u2022 We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We adopt the ABC-6K and CC-500 datasets from [8]. Our method is built\nupon Stable Diffusion 1.4 [2] with the pre-trained text encoder of CLIP ViT-L/14 [2].\nGuidelines:\n\u2022 The answer NA means that the paper does not use existing assets.\n\u2022 The authors should cite the original paper that produced the code package or dataset.\n\u2022 The authors should state which version of the asset is used and, if possible, include a\nURL.\n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n33\n\u2022 If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n\u2022 For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n\u2022 If this information is not available online, the authors are encouraged to reach out to\nthe asset\u2019s creators.\n13. New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: The assets introduced in the paper have been documented. In the supplemental\nmaterial, we have provided a Jupiter file with visualized results for reference.\nGuidelines:\n\u2022 The answer NA means that the paper does not release new assets.\n\u2022 Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n\u2022 The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [Yes]\nJustification: The instructions given to human evaluators are described in Section 4.2, and a\nscreenshot instance in Figure 18.\nGuidelines:\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022 Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [Yes]\nJustification: We have obtained approval from an equivalent to the Institutional Review\nBoard to conduct our quantitative experiments.\nGuidelines:\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n34\n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n\u2022 We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n\u2022 For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n35"
    },
    {
        "paper": "Deep Graph Mating",
        "input": "Deep Graph Mating\nYongcheng Jing1\nSeok-Hee Hong1\nDacheng Tao2\n1University of Sydney\n2Nanyang Technological University\n{yongcheng.jing,seokhee.hong}@sydney.edu.au, dacheng.tao@ntu.edu.sg\nAbstract\nIn this paper, we introduce the \ufb01rst learning-free model reuse task within the\nnon-Euclidean domain, termed as Deep Graph Mating (GRAMA). We strive to\ncreate a child Graph Neural Network (GNN) that integrates knowledge from pre-\ntrained parent models without requiring re-training, \ufb01ne-tuning, or annotated labels.\nTo this end, we begin by investigating the permutation invariance property of\nGNNs, which leads us to develop two vanilla approaches for GRAMA: Vanilla\nParameter Interpolation (VPI) and Vanilla Alignment Prior to Interpolation (VAPI),\nboth employing topology-independent interpolation in the parameter space. How-\never, neither approach has achieved the anticipated results. Through theoretical\nanalysis of VPI and VAPI, we identify critical challenges unique to GRAMA, in-\ncluding increased sensitivity to parameter misalignment and further the inherent\ntopology-dependent complexities. Motivated by these \ufb01ndings, we propose the\nDual-Message Coordination and Calibration (DuMCC) methodology, comprising\nthe Parent Message Coordination (PMC) scheme to optimise the permutation ma-\ntrices for parameter interpolation by coordinating aggregated messages, and the\nChild Message Calibration (CMC) scheme to mitigate over-smoothing identi\ufb01ed in\nPMC by calibrating the message statistics within child GNNs. Experiments across\ndiverse domains, including node and graph property prediction, 3D object recog-\nnition, and large-scale semantic parsing, demonstrate that the proposed DuMCC\neffectively enables training-free knowledge transfer, yielding results on par with\nthose of pre-trained models.\n1\nIntroduction\nThe remarkable progress made in deep neural networks has resulted in a growing number of pre-\ntrained models being made publicly available for the purpose of performance reproducibility and\nfurther development [64, 12, 27, 23, 61, 24]. As such, there is a mounting interest in the community\non the reusability of existing pre-trained neural networks for the sake of strengthening performance,\nreducing model size, or alleviating training efforts, with abundant inspiring works proposed [16, 42,\n46, 40, 47, 45, 63, 67]. Despite the growing interests in model reuse, current research endeavors have\npredominantly focused on the Euclidean domain, which are speci\ufb01cally designed to handle image\ndata with regular grid structures [39, 64, 34, 12, 51, 14, 22].\nOn the other hand, the study of reusing pre-trained Graph Neural Networks (GNNs) to tackle non-\nEuclidean and irregular graph data is still in its early stage and remains limited in scope. Almost\nall existing research on GNN reuse is established upon non-Euclidean Knowledge Distillation (KD)\npioneered by Yang et al. [60], where a favourable student GNN is learned from a single pre-trained\nteacher [11, 49, 9, 66, 29]. Subsequent research has extended the scope of Yang et al. [60] from a\nsingle-teacher to a multi-teacher context, introducing the novel task of non-Euclidean Knowledge\nAmalgamation (KA) [25, 15, 36]. However, all these existing approaches to GNN reuse necessitate\nresource-intensive re-training of the student model, imposing substantial computational burdens and\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\nGraph-Centric Model Reuse Tasks\nMulti-model Reuse Annotation Free Training/Fine-tuning Free\nKnowledge Distillation [60, 11, 49, 9, 66, 29]\n\u00d7\n\u00d7\n\u00d7\nKnowledge Amalgamation [25, 15, 36]\n\u221a\n\u221a\n\u00d7\nDeep Graph Mating (GRAMA)\n\u221a\n\u221a\n\u221a\nTable 1: Comparison of various model reuse tasks in the non-Euclidean domain, tailored for GNNs.\nmemory costs. This challenge from re-training is particularly pronounced when dealing with large\nmodels and large-scale graphs [31, 7, 37, 20, 8, 26, 17].\nIn this paper, we strive to push the boundaries of resource-ef\ufb01cient GNN reuse by introducing the\n\ufb01rst training-free model reuse task in the non-Euclidean domain, termed as Deep Graph Mating\n(GRAMA). Our objective is to derive a child GNN, without re-training or \ufb01ne-tuning, from pre-trained\nparent GNNs, each possessing unique knowledge from different datasets, all while operating without\naccess to human-annotated labels\u2014a common constraint in using publicly available models. The\nchild model born from GRAMA is expected to seamlessly integrate the diverse expertise of its parent\nmodels in a completely learning-free manner. A comparative analysis of GRAMA with existing\nnon-Euclidean model reuse approaches is presented in Tab. 1. As a pilot study of this novel task, this\npaper focuses on homogeneous GRAMA scenarios with identical parent architectures, and sets the\nstage for future investigations into the more complex cross-architecture heterogeneous GRAMA.\nTo achieve the ambitious goals of GRAMA, we \ufb01rst investigate the permutation invariance property\nof GNNs to establish correspondence between neurons in pre-trained models. This investigation\nguides the development of two vanilla methods: Vanilla Parameter Interpolation (VPI) and Vanilla\nAlignment Prior to Interpolation (VAPI). VPI involves straightforward linear interpolation of parent\nGNN parameters, while VAPI incorporates data-independent parameter matching before interpolation.\nHowever, the performance of both VPI and VAPI has not been as promising as expected. By analysing\nthe mechanisms underlying these methods, we have identi\ufb01ed and theoretically demonstrated unique\nchallenges associated with GRAMA, including increased susceptibility to parameter misalignment\nand topology-dependent complexity in GNNs. These \ufb01ndings underscore the necessity for tailored\nmethods that are speci\ufb01cally adapted to GRAMA.\nTo this end, we introduce the Dual-Message Coordination and Calibration (DuMCC) methodology,\nspeci\ufb01cally tailored for GRAMA to incorporate the topological characteristics of graphs. DuMCC\nconsists of two distinct schemes: Parent Message Coordination (PMC) and Child Message Calibration\n(CMC). PMC seeks to identify optimal permutation matrices for parameter matching in a topology-\naware manner, by coordinating aggregated messages from parent GNNs. Although PMC shows\npromising results, our empirical and theoretical analyses indicate that the child GNN derived from\nthis coordination is more prone to over-smoothing. To mitigate this issue, we propose the CMC\nscheme, which calibrates the message statistics of the child GNN using a specialised, learning-free\nmessage normalisation (LFNorm) layer, drawing on the statistics of the parent GNNs. Together, these\ntwo schemes contribute to a training-free and label-free GRAMA process, enabling the derivation of\nchild GNNs that effectively embody the knowledge from their pre-trained parent models.\nIn summary, our contribution is a novel non-Euclidean model reuse paradigm that allows for the\ncreation of a student GNN, which integrates the capabilities of pre-trained parent GNNs without\nrequiring human-annotated labels or re-training. We evaluated our proposed approach on seven\nbenchmarks across various tasks, including node/graph classi\ufb01cation, object recognition, and large-\nscale indoor semantic segmentation, and across \ufb01ve different GNN architectures: Graph Convolutional\nNetwork (GCN) [30], GraphSAGE [13], Graph Attention Network (GAT) [48], Graph Isomorphism\nNetworks (GIN) [56], and Dynamic Graph CNN (DGCNN) [53]. Experimental results demonstrate\nthat the generated child GNN is competent to handle the pre-trained tasks of parent models. We also\npresent a discussion of the limitations, highlighting potential future research directions that can be\nexplored based on GRAMA.\n2\nRelated Work\nThis section brie\ufb02y reviews topics relevant to GRAMA, including existing GNN reuse techniques and\nmodel merging, which, while conceptually akin to GRAMA, is con\ufb01ned to the Euclidean domain.\nExtended related work is provided in Appendix A.\n2\nNon-Euclidean Model Reuse. Recent advancements in graph-centric intelligence have led to\nthe widespread availability of pre-trained models for reproducibility. Despite their availability,\nthe exploration of their reuse for downstream tasks, particularly in the non-Euclidean domain,\nis still in its infancy. Existing research primarily revolves around knowledge distillation (KD)\n[60, 9, 66, 11, 49, 29] and knowledge amalgamation (KA) [25, 15, 36], aimed at single-GNN and\nmulti-GNN reusing, respectively. In particular, the foundational KD research [60] introduced a\nmethod speci\ufb01cally designed for GNNs that conserves local structural integrity. The scheme was\nlater expanded by KA [25] to adapt to multi-teacher settings without the need for labels, thereby\nenhancing GNN reuse capabilities. Extending these concepts, this paper launches GRAMA, a new\nGNN reuse paradigm that further boosts resource ef\ufb01ciency by obviating the need for re-training.\nModel Merging. Recent years have seen a surge in interest in merging the weights of CNNs or\ntransformers into a single model [54, 43, 1, 19, 44, 57, 33, 59, 35, 62, 38, 2, 21]. Model merging is\ntypically categorised into two types: \ufb01ne-tuned model merging and variably initialised model merging\n[33, 44]. Merging models \ufb01ne-tuned from the same initialisation is generally straightforward as\nthese models often reside within the same error basin, allowing for simple weight interpolation [54].\nConversely, merging models from different initialisations is more challenging due to the randomness\nin network channels and components [33, 43, 1]. A key issue is aligning neurons between models\nto establish correspondences before weight interpolation. To address this issue, Git Re-basin [1]\nproposes to minimise the L2 distance between weight vectors. Singh and Jaggi [43] propose an\nOptimal Transport Fusion (OTFusion) method that uses the Wasserstein distance to align weight\nmatrices prior to performing parameter fusion. The subsequent work [19] extends the application of\nOTFusion to Transformer-based architectures, aiming to enhance ef\ufb01ciency and performance through\nfusion. Also, Liu et al. [35] approach the challenging task of model fusion as a graph matching\nproblem, incorporating second-order parameter similarities for improved fusion performance.\nAddressing the fusion of pre-trained models trained on disparate tasks, Stoica et al. [44] develop ZipIt!,\na novel method that utilises a \u201czip\u201d operation for layer-wise fusion based on feature redundancy,\ncreating a versatile multi-task model without additional training. More recently, Xu et al. [57]\npresent the Merging under Dual-Space Constraints (MuDSC) framework. This approach optimises\npermutation matrices by mitigating inconsistencies in unit matching across both weight and activation\nspaces, targeting effective model fusion in multi-task scenarios. Furthermore, Jordan et al. [28]\nintroduce REPAIR, a method that tackles the issue of variance collapse by rescaling the hidden units\nin the interpolated network to restore the statistical properties of the original networks.\nHowever, model merging has not yet been explored within the context of GNNs. This paper introduces\nGRAMA, the \ufb01rst formulation for weight space model merging tailored for the non-Euclidean domain.\nIt represents the initial investigation into merging GNNs, addressing the unique challenges that arise\nwith graph tasks.\n3\nMotivation and Problem De\ufb01nition\nIn this section, we begin by introducing Knowledge Amalgamation (KA), the sole existing task in\nmulti-GNN reuse, exploring its inherent limitations, and present our novel GRAMA paradigm for\nresource-ef\ufb01cient multi-model reuse with practical applications in real-world scenarios.\nTo the best of our knowledge, KA represents the only multi-model reuse task in the non-Euclidean\ndomain. KA is de\ufb01ned in the literature as follows: its objective is to learn a single, compact student\nGNN that integrates the diverse expertise of pre-trained teacher GNNs, without accessing human\nannotations [25]. Despite the promising performance, KA is inherently limited by its resource-\nintensive nature, requiring the re-training of a student GNN to amalgamate knowledge from existing\nGNNs. Furthermore, while KA ostensibly operates without ground-truth labels, it instead relies on\nsoft labels generated by teachers, making it susceptible to the misclassi\ufb01cation errors of teachers.\nTo mitigate these constraints of KA, this paper proposes:\nTask 3.1 (Deep Graph Mating). Deep Graph Mating (GRAMA) is a fully learning-free model reuse\ntask where a child GNN is derived from pre-trained parent GNNs without re-training or \ufb01ne-tuning,\nintegrating their expertise without requiring human-annotated labels.\nIn aggregate, GRAMA advances beyond KA by eliminating the need for any training or label\ndependency, paving the way for more widespread and versatile model reuse applications. Given the\n3\nnovelty and complexity of GRAMA, our initial investigation in this paper is con\ufb01ned to scenarios\nwhere pre-trained GNNs possess identical architectures yet are trained on separate datasets, termed\nas homogenous GRAMA. We reserve the exploration of more challenging heterogeneous GRAMA\nscenarios, where pre-trained parent models either have diverse architectures or are designed for\ndifferent domain tasks, as a topic for future studies, potentially incorporating solutions like partial\nGRAMA inspired by the work of Stoica et al. [44].\nThe applications of homogenous GRAMA are especially crucial in contexts where full data access\nfor training is restricted due to privacy concerns and regulatory requirements. This is common in\nsectors like healthcare or retail, where organisations operate across different regions, each gathering\ndata that cannot be centrally aggregated due to local privacy regulations. The proposed homogenous\nGRAMA paradigm enables the seamless integration of knowledge from these isolated datasets, thereby\nsafeguarding against the risks of privacy violations and the disclosure of sensitive data.\n4\nVanilla Methodologies and Challenge Pre-analysis\n4.1\nTwo Vanilla GRAMA Methods\nVanilla Parameter Interpolation (VPI). To achieve the ambitious goal of GRAMA outlined in\nSect. 3, the initial na\u00efve approach employs vanilla weight averaging [50, 54]. This method involves\na straightforward linear interpolation of weights Wa and Wb from two pre-trained GNNs: W (\u2113) =\n\u03b1W (\u2113)\na\n+ (1 \u2212\u03b1)W (\u2113)\nb , where \u03b1 represents the interpolation weight and W (\u2113) denotes the network\nweights at layer \u2113.\nHowever, the vanilla averaging approach requires the pre-trained models to share a portion of their\ntraining trajectory and remain suf\ufb01ciently close in the parameter space [33], typically achieved by\n\ufb01ne-tuning from the same initial model. This is not applicable in our GRAMA context, where parent\nGNNs are trained on distinct datasets. This mismatch leads to empirically poor performance for\nGRAMA, as observed in our experiments.\nVanilla Alignment Prior to Interpolation (VAPI). To address this issue in vanilla interpolation,\nprevious research in the Euclidean domain, based on the conjecture of the permutation invariance\nproperty of typical neural networks1, proposes aligning the neurons between pre-trained models\nby permuting parameter matrices before performing linear interpolation [1]. The alignment and\ninterpolation process can be formulated as:\nW (\u2113) = \u03b1W (\u2113)\na\n+ (1 \u2212\u03b1)P (\u2113)W (\u2113)\nb (P (\u2113\u22121))T ,\nP (\u2113) \u2208P\u2217,\n(1)\nwhere P\u2217=\n\u0002\nP (\u2113)\u0003\n\u2113\u2208[L] represents the set of all permutation matrices P (\u2113) for each layer \u2113of the\nGNN. Here, [L] refers to the set of indices corresponding to all layers in the GNN.\nHowever, to apply Eq. 1 to GNN-based GRAMA, it is essential to \ufb01rst discuss whether the permuta-\ntion invariance property extends to GNNs. This property has been extensively studied in existing\nliterature for various architectures [10, 5, 43, 1, 33, 19], including multi-layer perceptrons (MLPs),\nconvolutional neural networks (CNNs), and Transformers. Supported by these prior studies, and\ngiven that GNNs are fundamentally built upon MLPs [58, 30], we propose:\nConjecture 4.1 (Permutation Invariance in GNNs). Permutation invariance for parameters in GNNs\nexists if and only if there exists a set of permutation matrices P (\u2113) for each layer \u2113such that applying\nthese permutations to the parameters does not alter the outcome of graph-based learning task,\nregardless of the structure of the adjacency matrix.\nA key subsequent issue involves searching the optimal permutation matrix P\u2217for GNNs. One\npossible data-independent solution is to minimise the L2 distance between the weight vectors of the\npre-trained models by solving a sum of bilinear assignments problem, similar to weight matching\ntechniques described in [1]. This method, which does not consider data distributions, could be\nadapted as a baseline vanilla method for our GRAMA task in the non-Euclidean domain, which is\nevaluated in our experiments.\n1Neurons in each layer of neural networks can be permuted without altering network functionality [10, 1].\n4\n4.2\nChallenges Towards GRAMA\nHowever, we empirically observed that even the Vanilla Alignment Prior to Interpolation method\nyielded unfavourable results for our GRAMA task. To elucidate the underlying cause of this phe-\nnomenon, we theoretically demonstrated that GNNs typically exhibit greater sensitivity to parameter\nmismatches than neural networks in the Euclidean domain:\nLemma 4.1 (Ampli\ufb01ed Sensitivity of GNNs to Parameter Misalignment). GNNs exhibit greater\nsensitivity to mismatches in parameter alignment compared to CNNs, ampli\ufb01ed by the degree of\nconnectivity and heterogeneity of the node features in the graph topology.\nA complete theoretical proof of Lemma 4.1 is provided in Appendix G.1. Here, we present only\nthe \ufb01nal formulation of the approximated output changes resulting from weight perturbations due to\nmismatching, based on Taylor series approximation:\n\u2206Fi \u2248\u03c3\u2032\n\uf8eb\n\uf8edX\nj\u2208N(i)\nW \u00b7 Xj\n\uf8f6\n\uf8f8\u00b7\nX\nj\u2208N(i)\n\u03f5 \u00b7 Xj,\n(2)\nwhere \u2206Fi refers to the change in output at node i due to the perturbations \u03f5 in the weights W, and\n\u03c3\u2032 represents the derivative of the activation function. Xj denotes the features of the nodes within\nthe neighbourhood N(i) of node i. Detailed, model-speci\ufb01c formulations of Eq. 2 are provided in\nSect. B of the appendix.\nEq. 2 implies that the effect of \u03f5 can be exacerbated by the potentially large and diverse neighbour-\nhoods N(i), thereby making the output highly sensitive to changes in W. In other words, the effect of\nweight perturbation can vary dramatically based on the node\u2019s connectivity and the characteristics of\nits neighbours. Such variability leads to signi\ufb01cant and less predictable changes in output, illustrating\nthe particular vulnerability of GNNs to parameter mismatches.\nThe integration of Eq. 1 with Lemma 4.1 and Eq. 2 further gives rise to the following conjecture:\nConjecture 4.2 (Topology-dependent Complexity in GNNs). The identi\ufb01cation of optimal permu-\ntation matrices P\u2217for GNNs presents increased complexity compared to the Euclidean domain,\ncontingent upon the topological characteristics inherent to each graph.\nConjecture 4.2 highlights the essential need for developing GRAMA methods that are speci\ufb01cally\ntailored to accommodate the unique topologies of graphs, motivating the design of the proposed\napproach in Sect. 5.\n5\nProposed Approach: Dual-Message Coordinator and Calibrator\n5.1\nOverview\nMotivated by Conjecture 4.2, we introduce in this section the proposed Dual-Message Coordina-\ntion and Calibration (DuMCC) methodology, which is speci\ufb01cally designed to harness the unique\ntopological features of input graphs for achieving GRAMA without relying on human annotations.\nThe proposed DuMCC is composed of two strategic schemes. In particular, the \ufb01rst Parent Message\nCoordination (PMC) scheme effectively integrates topological information by deriving optimal\npermutation matrices from layer-speci\ufb01c aggregation results. However, both empirical and theoretical\nanalyses reveal a reduction in node feature variance in child GNNs, suggesting that models derived\nthrough this coordination are more susceptible to over-smoothing compared to their parent GNN\ncounterparts.\nTo address this issue, we further propose the Child Message Calibration (CMC) scheme as our second\nstrategic component. This scheme aims to maintain message variance consistency from the parent\nmodels, ensuring the retention of feature diversity essential for robust GNN performance. Further\nelaboration on each component is provided in subsequent sections.\n5.2\nParent Message Coordination Scheme\nMotivated by Conjecture 4.2, which highlights the signi\ufb01cance of incorporating topology information\nin the GRAMA process, we propose a Parent Message Coordination (PMC) scheme for identifying\n5\noptimal topology-aware permutation matrices P\u2217described in Eq. 1. Unlike the vanilla method\nof VAPI in Sect. 4 that minimises the distance between weight vectors without considering the\ninput graphs\u2019 topologies, PMC optimises P\u2217by leveraging the topology information embedded in\naggregated messages from two pre-trained parent GNN models.\nAssume we have two pre-trained parent GNNs, denoted by Ga and Gb, both sharing identical\narchitectures. Their corresponding weight matrices are denoted by Wa and Wb, respectively. To\nestablish correspondence between neurons in Wa and Wb as described in Eq. 1, our PMC optimises\nthe permutation matrix P (\u2113) at layer \u2113by aligning the aggregated messages of the two parent GNNs.\nTo align with the notations [1, 33] used in model merging within the Euclidean domain, the associated\noptimisation process can be formulated as follows:\nP (\u2113) = arg\nmin\nP (\u2113)\u2208P\u2217\nX\ni\n\r\r Agg\nj\u2208N(i)\n\u03c6\n\u0010\nW (\u2113\u22121)\na\n; X(\u2113\u22121)\nj,a\n, eij\n\u0011\n\u2212P (\u2113) \u00b7 Agg\nj\u2208N(i)\n\u03c6\n\u0010\nW (\u2113\u22121)\nb\n; X(\u2113\u22121)\nj,b\n, eij\n\u0011 \r\r2,\n(3)\nwhere \u03c6 denotes the message function that encodes both node features X and edge features e, and\nAgg represents the message aggregation function that accumulates incoming messages from \u03c6, acting\non each node i. N(i) denotes the set of neighbours of node i. The minimisation problem in the form\nof Eq. 3 can be typically transformed into a maximisation problem to maximise an inner product (as\nderived from expanding Eq. 3), thereby \ufb01tting it within the framework of a standard linear assignment\nproblem, as also done in the works of [1, 33, 35, 44].\nEq. 3 is based on the rationale that aggregated messages inherently encapsulate essential graph\ntopologies, and that structurally similar GNNs typically generate analogous aggregated messages\nwhen tasked with similar graph operations and topologies. As such, through Eq. 3, P\u2217can be\ndetermined in a topology-aware manner, matching aggregated messages to re\ufb02ect the topological\ncharacteristics of the graphs. Here, we clarify that while our approach involves passing the graph\ndata to the pre-trained model to capture the graph-speci\ufb01c topological characteristics by utilising\nX, it requires only a single forward pass of the unlabelled graph data to extract messages for\nalignment\u2014eliminating the need for iterative training or ground-truth labels. Subsequently, child\nGNNs can be derived through linear parameter interpolation.\nDespite the encouraging performance, we observe that the child GNN, derived from the proposed\nPMC, exhibits a reduction in the variance of node embeddings. We conjecture that this reduction\nstems from an averaging effect, which may smooth out the distinctive features captured by each\nparent model, particularly when these models have learned different structural aspects of the graph:\nLemma 5.1 (Variance Reduction in Interpolated Graph Embeddings). The variance of the graph\nembeddings in an interpolated child GNN is typically smaller than the variances of the embeddings\nfrom the individual pre-trained parent GNNs.\nThe full proof of Lemma 5.1 is detailed in Appendix G.2, providing evidence of feature homogeni-\nsation within child GNNs. In the context of GNNs, we further explore and establish the following\nproposition:\nProposition 5.1 (Increased Susceptibility to Over-Smoothing in Child GNNs). Interpolated child\nGNNs exhibit increased susceptibility to over-smoothing compared to their parent networks, as\nmeasured by Dirichlet energy.\nThe detailed proof of Proposition 5.1 is provided in Appendix G.3, utilising the quantitative over-\nsmoothing measurement based on Dirichlet energy [41], where a lower value indicates greater\nhomogeneity or smoothness among node features. In particular, our theoretical analysis in Ap-\npendix G.3 demonstrates that:\nE(X\u2113) \u2264max\n\u0000E(X\u2113\na), E(X\u2113\nb)\n\u0001\n,\n(4)\nwhere E(X\u2113) denotes the Dirichlet energy for the node features X\u2113at layer \u2113of the child GNN.\nIn GRAMA, the parameter \u03b1 is typically set to 0.5 to ensure unbiased knowledge integration from\nboth pre-trained models. This setting promotes a balanced contribution from each model and prevents\nany bias toward the characteristics of one over the other. As detailed in Appendix G.3, with \u03b1\nat this level, the Dirichlet energy of the interpolated child GNN, E(X\u2113), signi\ufb01cantly decreases\n6\n0.7\n0.8\n0.9\nParent GNN A\n0.70\n0.75\n0.80\nParent GNN B\n0.441\n0.428\n0.444\n0.416\n0.428\n0.476\n0.434\n0.444\n0.438\n0.452\n0.452\n0.444\n0.439\n0.446\n0.430\n0.441\n0.434\n0.429\n0.455\n0.452\n0.487\n0.444\n0.470\n0.441\n0.433\n0.445\n0.455\n0.425\n0.433\n0.422\n0.42\n0.44\n0.46\n0.48\nChild GNN (PMC)\nFigure 1: Comparison of Dirichlet en-\nergies between pre-trained parent GNNs\nand the corresponding child from PMC.\ncompared to the Dirichlet energies of the individual par-\nent models, indicating a higher susceptibility to over-\nsmoothing.\nCorresponding to this theoretical analysis, empirical ev-\nidence is presented in Fig. 1, where 60 parent models are\npre-trained on distinct partitions of the ogbn-products\ndataset with different random seeds. Additional imple-\nmentation details are provided in the appendix. Fig. 1 fur-\nther demonstrates the increased smoothing effect, which\ncan potentially diminish the model\u2019s expressive power\nand discriminative capability.\n5.3\nChild Message Calibration Scheme\nTo mitigate the over-smoothing issue identi\ufb01ed in Proposition 5.1, one potential solution involves\nleveraging established methods designed to address over-smoothing, such as PairNorm [65] and\nresidual connections [32]. However, to the best of our knowledge, all these existing solutions typically\nrequire re-training the model. This requirement contradicts the fundamental principle of our GRAMA\napproach, which aims for training-free model reuse.\nTo address this issue, we introduce a Child Message Calibration (CMC) scheme designed to re\ufb01ne the\nmessage statistics of the obtained child GNN without the need for re-training or ground-truth labels.\nCentral to this scheme is our Learning-Free Message Normalisation (LFNorm) layer speci\ufb01cally\ntailored for our GRAMA task, inspired by [28]. This layer is intended to enhance the discriminative\npower and representational capacity of the child GNN by promoting a more diverse node feature\ndistribution.\nAccording to Proposition 5.1, while the shift in mean node features is typically less problematic than\nvariance reduction, it is noted in [6] that mean statistics in GNNs also carry vital graph structural\ninformation. Therefore, our approach aims to simultaneously re\ufb01ne both mean and variance statistics\nin the child GNN, ensuring a comprehensive enhancement of topological representation.\nTowards this end, we \ufb01rst process the raw graph through the parent GNNs to compute the target\nmessage mean and variance intended for alignment in the child GNN. Subsequently, we integrate\nthe LFNorm layer into the child GNN to re\ufb01ne the message statistics using the derived mean and\nvariance from the parent models:\n\u02dcmij =\ns\nVar(ma\nij) + Var(mb\nij)\n2\n\u00b7\n\u0012mij \u2212\u00b5N(i)\n\u03c3N(i)\n\u0013\n+ E(ma\nij) + E(mb\nij)\n2\n,\n(5)\nwhere mij = \u03c6j\u2208N(i)(W (\u2113\u22121); X(\u2113\u22121)\nj\n, eij) represents the messages from node j to node i, as\nspeci\ufb01ed in Eq. 3. In this setup, \u02dcmij denotes the statistically calibrated message within the child\nGNN, while \u00b5N(i) and \u03c3N(i) represent the mean and standard deviation, respectively, of the messages\ndirected to node i in the child GNN.\nEq. 5 ensures that the normalised messages in the interpolated child model maintain a balanced\nrepresentation of central tendencies from both pre-trained parent models. This method effectively\nreduces the risk of over-smoothing in child GNNs by preserving essential topological statistics from\nthe parent models.\nA more detailed algorithmic procedure is outlined in Alg. 1. In practice, we \ufb01nd that incorporating\na single LFNorm layer and aligning the overall message mean and variance of the parent GNNs is\ntypically suf\ufb01cient to achieve favourable performance while minimising computational costs.\n6\nExperiments\nWe evaluate the performance of DuMCC across seven benchmarks spanning \ufb01ve GNN architectures.\nMore ablation studies and sensitivity analyses, additional results and implementation details, as well\nas more visualisations, are detailed in Secs. D and E of the appendix.\n7\nAlgorithm 1 The proposed Dual-Message Coordinator and Calibrator (DuMCC) for GRAMA.\nInput: Pre-trained Parent GNNs Ga and Gb, interpolation factor \u03b1.\nOutput: Child GNN G that integrates the expertise of Ga and Gb in a learning-free manner.\n// Parent Message Coordination\nforeach layer \u2113from 1 to L do\nExtract weights W (\u2113)\na\nfrom Ga; Extract weights W (\u2113)\nb\nfrom Gb\n// Compute permutation matrix for current layer with aggregated messages\nP (\u2113) \u2190arg\nmin\nP (\u2113)\u2208P\u2217\nX\ni \u2225Agg\nj\u2208N(i)\n\u03c6\n\u0010\nW (\u2113\u22121)\na\n; X(\u2113\u22121)\nj,a\n, eij\n\u0011\n\u2212P (\u2113) \u00b7 Agg\nj\u2208N(i)\n\u03c6\n\u0010\nW (\u2113\u22121)\nb\n; X(\u2113\u22121)\nj,b\n, eij\n\u0011\n\u22252\n// Interpolate weights for current layer\nW (\u2113) \u2190\u03b1W (\u2113)\na\n+ (1 \u2212\u03b1)P (\u2113)W (\u2113)\nb (P (\u2113\u22121))T\nend\nG \u2190{W (\u2113)}L\n\u2113=1\n// Child Message Calibration\nforeach layer \u2113from (L \u2212n) to L do\nforeach edge (i, j) in the graph do\n// Compute the messages in Ga and Gb\nma\nij \u2190\u03c6j\u2208N(i)(W (\u2113\u22121)\na\n; X(\u2113\u22121)\nj,a\n, eij); mb\nij \u2190\u03c6j\u2208N(i)(W (\u2113\u22121)\nb\n; X(\u2113\u22121)\nj,b\n, eij)\n// Compute the message for G\nmij \u2190\u03c6j\u2208N(i)(W (\u2113\u22121); X(\u2113\u22121)\nj\n, eij)\n// Compute the scale and shift parameters\n\u03b2 \u2190\nq\u0000Var(ma\nij) + Var(mb\nij)\n\u0001\n/2; \u03b3 \u2190\n\u0000E(ma\nij) + E(mb\nij)\n\u0001\n/2\n// Learning-free message calibration for G\n\u02dcmij \u2190\u03b2\n\u0000(mij \u2212\u00b5N(i))/\u03c3N(i)\n\u0001\n+ \u03b3\nend\nforeach node i in the graph do\n// Aggregate the calibrated messages and update features\nX\u2113\ni \u2190Aggj\u2208N(i) \u02dcmij\nend\nend\nTable 2: Multi-class molecule property prediction results for\nparent GNNs, each pre-trained on disjoint partitions of the\nogbn-arxiv and ogbn-products datasets [18].\nMethods\nRe-\ntrain?\nogbn-arxiv\nogbn-products\nDataset A Dataset B Dataset C Dataset D\nParent GCN A [30]\n-\n0.7193\n0.5516\nN/A\nN/A\nParent GCN B [30]\n-\n0.6564\n0.7464\nN/A\nN/A\nParent GraphSAGE C [13]\n-\nN/A\nN/A\n0.7982\n0.7308\nParent GraphSAGE D [13]\n-\nN/A\nN/A\n0.7626\n0.7904\nKA [25] (Section 3)\n\u221a\n0.7150\n0.6687\n0.7973\n0.7775\nVPI [54] (Section 4)\n\u00d7\n0.3486\n0.4361\n0.6568\n0.6546\nVAPI [1] (Section 4)\n\u00d7\n0.6140\n0.5752\n0.5425\n0.5779\nOurs (w/o CMC)\n\u00d7\n0.6531\n0.5957\n0.7374\n0.7414\nOurs (w/ CMC)\n\u00d7\n0.6645\n0.6382\n0.7647\n0.7515\nImplementation Details.\nDetailed\ndataset descriptions and statistics are\nprovided in Appendix C. For multi-\nclass classi\ufb01cation tasks on ogbn-\narxiv [17], ogbn-products [4], and\nModelNet40 [55],\nwe adopt the\ndataset partition strategy widely used\nin model merging within the Eu-\nclidean domain [1, 28]. Speci\ufb01cally,\neach dataset is randomly split into two\ndisjoint subsets: the \ufb01rst subset com-\nprises 20% of the data with odd labels\nand 80% with even labels, while the\nsecond subset is arranged vice versa.\nFor the semantic segmentation task on S3DIS [3], we train the two parent models using Areas 1, 2,\n3 and Areas 2, 3, 4, 6, respectively, with Area 5 designated for testing, as also done in [32]. In the\nmulti-label classi\ufb01cation task on ogbn-proteins [17], one parent model is trained on nodes with odd\nlabels and the other on nodes with even labels. Implementation follows the of\ufb01cial codes provided\nby the Deep Graph Library (DGL) [52] and the original authors, including detailed architectures\nand hyperparameter settings. We set the interpolation factor \u03b1 in Eq. 1 to 0.5 for all experiments,\nwith a sensitivity analysis provided in Sect. D of the appendix. For models originally equipped with\n8\n(a) KA [25]\n(b) VPI [54]\n(c) VAPI [1]\n(d) Ours (w/o CMC)\n(e) Ours (w/ CMC)\nFigure 2: The t-SNE visualisations of various methods on a subset comprising the \ufb01rst 10 classes of\nogbn-arxiv. Additional visualisations for the remaining classes are available in Appendix E.\nMethods\nRe-train? Dataset I Dataset J\nParent DGCNN I [53]\n-\n0.9159\n0.8151\nParent DGCNN J [53]\n-\n0.8862\n0.9275\nKA [25] (Section 3)\n\u221a\n0.9250\n0.9283\nVPI [54] (Section 4)\n\u00d7\n0.4518\n0.4096\nVAPI [1] (Section 4)\n\u00d7\n0.6538\n0.5482\nOurs (w/o CMC)\n\u00d7\n0.8326\n0.8088\nOurs (w/ CMC)\n\u00d7\n0.8920\n0.8574\nTable 4: Results of the point cloud classi\ufb01cation\ntask on ModelNet40 [55] using DGCNN, with\ntwo parent models trained on disjoint partitions.\nNear\nFar\nKA\nVPI\nVAPI\nOurs\nFigure 3: Visualisations of feature space struc-\ntures, depicted by the distances between the red\npoint and all other points.\nnormalisation layers, we recompute the running mean and running variance for the student GNN. In\nparticular, the recomputation of statistics is performed concurrently with that of the LFNorm layer\nfor the child GNN in CMC. More implementation details are elaborated in Sect. E of the appendix.\nComparison Methods. Considering the limited exploration of multi-GNN reuse in existing literature,\nwe focus our comparison of the proposed DuMCC approach on the training-dependent KA [25] and\ntraining-free VPI [54] and VAPI [1] methods, as introduced in Sect. 4. Furthermore, in Sect. E of the\nappendix, we also provide the results of retraining a child model using combined parent datasets with\nground-truth labels, which establish an upper bound for GRAMA\u2019s performance.\nNode Property Prediction. Tab. 2 presents the results for the multi-class node classi\ufb01cation task.\nThe proposed DuMCC framework, as shown in the table, achieves a more balanced performance\nacross all datasets. It notably outperforms the parent models on datasets that were not used for their\ntraining. Additionally, the last two lines of Tab. 2 detail an ablation study on the proposed CMC\nscheme, demonstrating its ability to enhance performance beyond PMC. While our method slightly\nlags behind KA in performance, KA involves a complex re-training process, whereas our approach is\ncompletely training-free. A sample t-SNE visualisation of the results is provided in Fig. 2.\nWe further show in Tab. 3 the results of multi-label molecule property prediction. Notably, our\napproach slightly outperforms KA in Tab. 3, underscoring the limitations of KA discussed in Sect. 3.\nThese limitations stem from KA\u2019s reliance on soft labels produced by the teacher GNNs, making it\nvulnerable to their misclassi\ufb01cation errors. Furthermore, we explore the potential of multi-model\nGRAMA by concurrently reusing three pre-trained node classi\ufb01cation GNNs, as discussed in Sect. F\nof the appendix.\nGraph Property Prediction. Tab. 3 also presents results for the graph classi\ufb01cation task using GAT\narchitectures. The proposed DuMCC demonstrates enhanced equilibrium in performance relative to\nthe parent models. Notably, DuMCC slightly outperforms KA on ogbg-molbace, further illustrating\nTable 3: Results for multi-label node classi\ufb01cation and graph classi\ufb01cation, indicating KA\u2019s vulnera-\nbility to misclassi\ufb01cation errors from pre-trained models.\nArchitectures\nGIN [56]\nArchitectures\nGAT [48]\nMethods\nParent E Parent F\nKA\nOurs\nMethods\nParent G Parent H\nKA\nOurs\nogbn-proteins\n0.7478\n0.7222 0.7215 0.7341 ogbg-molbace\n0.7247\n0.4067\n0.6135 0.6296\nogbg-molbbbp\n0.4681\n0.6366\n0.6446 0.5275\n9\nTable 5: Results of the 3D semantic segmentation task on the S3DIS dataset [3], with detailed\nper-class results provided. Architecture details can be found in Sect. E of the appendix.\nMethods\nRe-\ntrain?\nStructural Elements\nceiling\n\ufb02oor\nwall\nbeam\ncolumn\nwindow\ndoor\nmean\nParent DGCNN K [53]\n-\n0.9655\n0.9947\n0.9355\n0.0079\n0.0557\n0.4529\n0.1430\n0.5079\nParent DGCNN L [53]\n-\n0.9529\n0.9927\n0.9546\n0.0573\n0.0661\n0.3555\n0.1335\n0.5018\nKA [25] (Section 3)\n\u221a\n0.9580\n0.9943\n0.9003\n0.0681\n0.1835\n0.5154\n0.7048\n0.6178\nVPI [54] (Section 4)\n\u00d7\n0.6909\n0.9871\n0.3612\n0.0000\n0.0044\n0.0000\n0.0037\n0.2925\nVAPI [1] (Section 4)\n\u00d7\n0.5338\n0.8766\n0.6825\n0.0382\n0.0046\n0.0284\n0.0019\n0.3094\nOurs (w/o CMC)\n\u00d7\n0.9804\n0.9967\n0.9186\n0.0302\n0.0000\n0.0008\n0.0752\n0.4289\nOurs (w/ CMC)\n\u00d7\n0.9695\n0.9962\n0.9290\n0.1466\n0.0004\n0.0344\n0.1047\n0.4544\nMethods\nRe-\ntrain?\nFurniture\nOthers\nOverall\ntable\nchair\nsofa\nbookcase\nboard\nmean\nclutter\nmean\nParent DGCNN K [53]\n-\n0.7182\n0.7746\n0.0216\n0.4980\n0.4022\n0.4829\n0.6515\n0.8181\nParent DGCNN L [53]\n-\n0.7447\n0.9255\n0.1301\n0.5267\n0.1774\n0.5009\n0.6038\n0.8174\nKA [25] (Section 3)\n\u221a\n0.7358\n0.8649\n0.0295\n0.5377\n0.4211\n0.5178\n0.6844\n0.8382\nVPI [54] (Section 4)\n\u00d7\n0.0159\n0.2802\n0.0019\n0.0135\n0.0001\n0.0623\n0.4188\n0.4791\nVAPI [1] (Section 4)\n\u00d7\n0.0117\n0.1984\n0.0142\n0.1235\n0.0025\n0.0701\n0.2326\n0.5060\nOurs (w/o CMC)\n\u00d7\n0.5953\n0.8485\n0.0015\n0.1160\n0.0009\n0.3125\n0.5279\n0.7497\nOurs (w/ CMC)\n\u00d7\n0.6351\n0.8089\n0.0086\n0.2867\n0.0182\n0.3515\n0.5274\n0.7676\nthat KA is vulnerable to the errors of pre-trained models. In contrast, our approach does not rely on\nsoft labels, thus avoiding this limitation.\n3D Object Recognition and Semantic Parsing. Tab. 4 and Fig. 3 illustrate the quantitative results\nand qualitative visualisations for the point cloud classi\ufb01cation task, respectively. The proposed\nDuMCC outperforms model I on dataset J and model J on Dataset I without requiring re-training.\nMoreover, Tab. 4 demonstrates that our approach signi\ufb01cantly outperforms two vanilla methods.\nFig. 3 further illustrates the structure of the feature space, revealing that our method produces\nsemantically similar structures to those achieved by KA with re-training. We also show in Tab. 5 the\nresults for the large-scale indoor semantic segmentation task. Our method notably surpasses other\nlearning-free GNN reuse methods VPI and VAPI. Further qualitative and quantitative results across\nvarious dataset splits and network architectures are provided in Sect. E of the appendix.\n7\nConclusions and Limitations\nIn this paper, we explore a novel GRAMA task for learning-free GNN reuse. The child model from\nGRAMA is expected to functionally merge knowledge from pre-trained parent models. Uniquely,\nGRAMA establishes the \ufb01rst paradigm in GNN reuse that operates entirely without re-training or \ufb01ne-\ntuning, while also eliminating the need for ground-truth labels. To this end, we start by developing two\nvanilla GRAMA approaches, which reveal speci\ufb01c challenges inherent to GRAMA. These challenges\nmotivate us to develop a DuMCC framework for topology-aware model reuse, leveraging a parent\nmessage coordination scheme followed by child message calibration. Experiments on node- and\ngraph-level tasks across various domains demonstrate the effectiveness of the proposed approach for\nannotation-free knowledge transfer without additional learning.\nDespite its strengths, the proposed DuMCC is primarily designed for homogeneous GRAMA, as\ndiscussed in Sect. 3. Currently, the framework does not support cross-architecture heterogeneous\nGRAMA, where parent models have different architectures, such as a combination of GCN and\nGraphSAGE. Additionally, it does not handle scenarios where parent models address tasks at differ-\nent levels, such as node-level versus graph-level tasks\u2014another aspect of heterogeneous GRAMA.\nThese limitations primarily arise from the absence of direct correspondence between the differing\narchitectural layers of the parent models, an issue we plan to explore in our future work. We will\nalso explore the possibility of a fully data-independent GRAMA scheme and investigate broader\napplications beyond training-free model reuse, such as its use as a pre-processing step to facilitate\ngraph-based knowledge amalgamation. Further discussions on limitations and potential solutions are\nprovided in Sect. H of the appendix.\n10\nAcknowledgement\nThis research / project is supported by the National Research Foundation, Singapore, and Cyber\nSecurity Agency of Singapore under its National Cybersecurity R&D Programme and CyberSG\nR&D Cyber Research Programme Of\ufb01ce, as well as Australian Research Council Discovery Project\nDP190103301.\nAny opinions, \ufb01ndings and conclusions or recommendations expressed in these materials are those\nof the author(s) and do not re\ufb02ect the views of National Research Foundation, Singapore, Cyber\nSecurity Agency of Singapore as well as CyberSG R&D Programme Of\ufb01ce, Singapore.\nReferences\n[1] Samuel Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. Git re-basin: Merging models\nmodulo permutation symmetries. In ICLR, 2023.\n[2] Takuya Akiba, Makoto Shing, Yujin Tang, Qi Sun, and David Ha. Evolutionary optimization of\nmodel merging recipes. arXiv preprint arXiv:2403.13187, 2024.\n[3] Iro Armeni, Ozan Sener, Amir R Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and\nSilvio Savarese. 3d semantic parsing of large-scale indoor spaces. In CVPR, 2016.\n[4] K. Bhatia, K. Dahiya, H. Jain, P. Kar, A. Mittal, Y. Prabhu, and M. Varma. The extreme\nclassi\ufb01cation repository: Multi-label datasets and code, 2016.\n[5] Johanni Brea, Ber\ufb01n Simsek, Bernd Illing, and Wulfram Gerstner. Weight-space symmetry in\ndeep networks gives rise to permutation saddles, connected by equal-loss valleys across the loss\nlandscape. arXiv preprint arXiv:1907.02911, 2019.\n[6] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-yan Liu, and Liwei Wang. Graphnorm: A\nprincipled approach to accelerating graph neural network training. In ICML, 2021.\n[7] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph\nconvolutional networks. In ICML, 2020.\n[8] Avery Ching, Sergey Edunov, Maja Kabiljo, Dionysios Logothetis, and Sambavi Muthukrishnan.\nOne trillion edges: Graph processing at facebook-scale. Proceedings of the VLDB Endowment,\n2015.\n[9] Xiang Deng and Zhongfei Zhang. Graph-free knowledge distillation for graph neural networks.\nIn IJCAI, 2021.\n[10] Rahim Entezari, Hanie Sedghi, Olga Saukh, and Behnam Neyshabur. The role of permutation\ninvariance in linear mode connectivity of neural networks. In ICLR, 2022.\n[11] Kaituo Feng, Changsheng Li, Ye Yuan, and Guoren Wang. Freekd: Free-direction knowledge\ndistillation for graph neural networks. In KDD, 2022.\n[12] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A\nsurvey. IJCV, 2021.\n[13] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large\ngraphs. In NeurIPS, 2017.\n[14] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang,\nAn Xiao, Chunjing Xu, Yixing Xu, et al. A survey on vision transformer. TPAMI, 2022.\n[15] Yunzhi Hao, Yu Wang, Shunyu Liu, Tongya Zheng, Xingen Wang, Xinyu Wang, Mingli Song,\nWenqi Huang, and Chun Chen. Attribution guided layerwise knowledge amalgamation from\ngraph neural networks. In ICONIP, 2023.\n[16] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. In\nNIPS Deep Learning and Representation Learning Workshop, 2015.\n11\n[17] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec.\nOgb-lsc: A large-scale challenge for machine learning on graphs. In NeurIPS Datasets and\nBenchmarks, 2021.\n[18] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele\nCatasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.\narXiv preprint arXiv:2005.00687, 2020.\n[19] Moritz Imfeld, Jacopo Graldi, Marco Giordano, Thomas Hofmann, Sotiris Anagnostidis, and\nSidak Pal Singh. Transformer fusion with optimal transport. In ICLR, 2024.\n[20] Wei Jin, Lingxiao Zhao, Shichang Zhang, Yozen Liu, Jiliang Tang, and Neil Shah. Graph\ncondensation for graph neural networks. In ICLR, 2022.\n[21] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge fusion\nby merging weights of language models. In ICLR, 2023.\n[22] Yongcheng Jing. Ef\ufb01cient representation learning with graph neural networks. PhD thesis,\n2023.\n[23] Yongcheng Jing, Yining Mao, Yiding Yang, Yibing Zhan, Mingli Song, Xinchao Wang, and\nDacheng Tao. Learning graph neural networks for image style transfer. In ECCV, 2022.\n[24] Yongcheng Jing, Xinchao Wang, and Dacheng Tao. Segment anything in non-euclidean domains:\nChallenges and opportunities. arXiv preprint arXiv:2304.11595, 2023.\n[25] Yongcheng Jing, Yiding Yang, Xinchao Wang, Mingli Song, and Dacheng Tao. Amalgamating\nknowledge from heterogeneous graph neural networks. In CVPR, 2021.\n[26] Yongcheng Jing, Yiding Yang, Xinchao Wang, Mingli Song, and Dacheng Tao. Meta-aggregator:\nLearning to aggregate for 1-bit graph neural networks. In ICCV, 2021.\n[27] Yongcheng Jing, Chongbin Yuan, Li Ju, Yiding Yang, Xinchao Wang, and Dacheng Tao. Deep\ngraph reprogramming. In CVPR, 2023.\n[28] Keller Jordan, Hanie Sedghi, Olga Saukh, Rahim Entezari, and Behnam Neyshabur. Repair:\nRenormalizing permuted activations for interpolation repair. In ICLR, 2023.\n[29] Chaitanya K Joshi, Fayao Liu, Xu Xun, Jie Lin, and Chuan-Sheng Foo. On representation\nknowledge distillation for graph neural networks. arXiv preprint arXiv:2111.04964, 2021.\n[30] Thomas N Kipf and Max Welling. Semi-supervised classi\ufb01cation with graph convolutional\nnetworks. In ICLR, 2017.\n[31] Guohao Li, Matthias M\u00fcller, Guocheng Qian, Itzel C Delgadillo, Abdulellah Abualshour, Ali\nThabet, and Bernard Ghanem. Deepgcns: Making gcns go as deep as cnns. TPAMI, 2021.\n[32] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. Deepgcns: Can gcns go as deep\nas cnns? In ICCV, 2019.\n[33] Weishi Li, Yong Peng, Miao Zhang, Liang Ding, Han Hu, and Li Shen. Deep model fusion: A\nsurvey. arXiv preprint arXiv:2309.15698, 2023.\n[34] Sihao Lin, Hongwei Xie, Bing Wang, Kaicheng Yu, Xiaojun Chang, Xiaodan Liang, and Gang\nWang. Knowledge distillation via the target-aware transformer. In CVPR, 2022.\n[35] Chang Liu, Chenfei Lou, Runzhong Wang, Alan Yuhan Xi, Li Shen, and Junchi Yan. Deep\nneural network fusion via graph matching with applications to model ensemble and federated\nlearning. In ICML, 2022.\n[36] Haibo Liu, Di Zhang, Liang Wang, and Xin Song. Multi-teacher local semantic distillation\nfrom graph neural networks. In ADMA, 2023.\n[37] Xin Liu, Mingyu Yan, Lei Deng, Guoqi Li, Xiaochun Ye, Dongrui Fan, Shirui Pan, and Yuan\nXie. Survey on graph neural network acceleration: An algorithmic perspective. In IJCAI, 2022.\n12\n[38] Michael S Matena and Colin A Raffel. Merging models with \ufb01sher-weighted averaging. In\nNeurIPS, 2022.\n[39] Yu Pan, Ye Yuan, Yichun Yin, Zenglin Xu, Lifeng Shang, Xin Jiang, and Qun Liu. Reusing\npretrained models by multi-linear operators for ef\ufb01cient training. In NeurIPS, 2023.\n[40] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and\nYoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014.\n[41] T Konstantin Rusch, Michael Bronstein, and Siddhartha Mishra. A survey on oversmoothing in\ngraph neural networks. SAM Research Report, 2023.\n[42] Andrei A Rusu, Neil C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,\nKoray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. arXiv\npreprint arXiv:1606.04671, 2016.\n[43] Sidak Pal Singh and Martin Jaggi. Model fusion via optimal transport. In NeurIPS, 2020.\n[44] George Stoica, Daniel Bolya, Jakob Brandt Bjorner, Pratik Ramesh, Taylor Hearn, and Judy\nHoffman. Zipit! merging models from different tasks without training. In ICLR, 2024.\n[45] Xiu Su, Shan You, Fei Wang, Chen Qian, Changshui Zhang, and Chang Xu. Bcnet: Searching\nfor network width with bilaterally coupled network. In CVPR, 2021.\n[46] Xiu Su, Shan You, Jiyang Xie, Fei Wang, Chen Qian, Changshui Zhang, and Chang Xu.\nSearching for network width with bilaterally coupled network. TPAMI, 2022.\n[47] Xiu Su, Shan You, Jiyang Xie, Mingkai Zheng, Fei Wang, Chen Qian, Changshui Zhang,\nXiaogang Wang, and Chang Xu. Vitas: Vision transformer architecture search. In ECCV, 2022.\n[48] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\nBengio. Graph attention networks. In ICLR, 2018.\n[49] Can Wang, Zhe Wang, Defang Chen, Sheng Zhou, Yan Feng, and Chun Chen. Online adversarial\ndistillation for graph neural networks. arXiv preprint arXiv:2112.13966, 2021.\n[50] Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and Yasaman Khazaeni.\nFederated learning with matched averaging. In ICLR, 2020.\n[51] Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learning for visual\nintelligence: A review and new outlooks. TPAMI, 2021.\n[52] Minjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou,\nQi Huang, Chao Ma, et al. Deep graph library: Towards ef\ufb01cient and scalable deep learning on\ngraphs. In ICLR Workshop, 2019.\n[53] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M.\nSolomon. Dynamic graph cnn for learning on point clouds. TOG, 2019.\n[54] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\nAri S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model\nsoups: averaging weights of multiple \ufb01ne-tuned models improves accuracy without increasing\ninference time. In ICML, 2022.\n[55] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and\nJianxiong Xiao. 3d shapenets: A deep representation for volumetric shapes. In CVPR, 2015.\n[56] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks? In ICLR, 2019.\n[57] Zhengqi Xu, Ke Yuan, Huiqiong Wang, Yong Wang, Mingli Song, and Jie Song. Training-free\npretrained model merging. In CVPR, 2024.\n[58] Chenxiao Yang, Qitian Wu, Jiahua Wang, and Junchi Yan. Graph neural networks are inherently\ngood generalizers: Insights by bridging gnns and mlps. In ICLR, 2023.\n13\n[59] Enneng Yang, Zhenyi Wang, Li Shen, Shiwei Liu, Guibing Guo, Xingwei Wang, and Dacheng\nTao. Adamerging: Adaptive model merging for multi-task learning. In ICLR, 2024.\n[60] Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang. Distilling knowledge\nfrom graph convolutional networks. In CVPR, 2020.\n[61] Jingwen Ye, Zunlei Feng, and Xinchao Wang. Flocking birds of a feather together: Dual-step\ngan distillation via realer-fake samples. In VCIP, 2022.\n[62] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario:\nAbsorbing abilities from homologous models as a free lunch. arXiv preprint arXiv:2311.03099,\n2023.\n[63] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving\nthe performance of convolutional neural networks via attention transfer.\narXiv preprint\narXiv:1612.03928, 2016.\n[64] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio\nSavarese. Taskonomy: Disentangling task transfer learning. In CVPR, 2018.\n[65] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In ICLR, 2020.\n[66] Sheng Zhou, Yucheng Wang, Defang Chen, Jiawei Chen, Xin Wang, Can Wang, and Jiajun Bu.\nDistilling holistic knowledge with graph neural networks. In ICCV, 2021.\n[67] Xiatian Zhu, Shaogang Gong, et al. Knowledge distillation by on-the-\ufb02y native ensemble. In\nNeurIPS, 2018.\n14\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately re\ufb02ect the\npaper\u2019s contributions and scope?\nAnswer: [Yes]\nJusti\ufb01cation: The main claims in the abstract and introduction accurately re\ufb02ect the paper\u2019s\ncontributions and scope.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and re\ufb02ect how\nmuch the results can be expected to generalize to other settings.\n\u2022 It is \ufb01ne to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJusti\ufb01cation: Limitation discussions are provided in Sect. 7.\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-speci\ufb01cation, asymptotic approximations only holding locally). The authors\nshould re\ufb02ect on how these assumptions might be violated in practice and what the\nimplications would be.\n\u2022 The authors should re\ufb02ect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n\u2022 The authors should re\ufb02ect on the factors that in\ufb02uence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n\u2022 The authors should discuss the computational ef\ufb01ciency of the proposed algorithms\nand how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n\u2022 While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren\u2019t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be speci\ufb01cally instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [Yes]\n15\nJusti\ufb01cation: The full set of assumptions and a complete proof are provided in Appendix G.\nGuidelines:\n\u2022 The answer NA means that the paper does not include theoretical results.\n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.\n\u2022 The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJusti\ufb01cation: The information to reproduce the experiments is provided in Appendix E.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or veri\ufb01able.\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suf\ufb01ce, or if the contribution is a speci\ufb01c model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n\u2022 While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with suf\ufb01cient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\n16\nAnswer: [Yes]\nJusti\ufb01cation: The code and model are provided in the supplementary material.\nGuidelines:\n\u2022 The answer NA means that paper does not include experiments requiring code.\n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n\u2022 While we encourage the release of code and data, we understand that this might not be\npossible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n\u2022 The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\u2022 The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n\u2022 The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n\u2022 At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n\u2022 Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJusti\ufb01cation: Implementation details are elaborated in Sect. E of the appendix.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n\u2022 The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment Statistical Signi\ufb01cance\nQuestion: Does the paper report error bars suitably and correctly de\ufb01ned or other appropriate\ninformation about the statistical signi\ufb01cance of the experiments?\nAnswer: [Yes]\nJusti\ufb01cation: The statistical analysis is provided in Appendix E.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, con\ufb01-\ndence intervals, or statistical signi\ufb01cance tests, at least for the experiments that support\nthe main claims of the paper.\n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n\u2022 The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n\u2022 The assumptions made should be given (e.g., Normally distributed errors).\n\u2022 It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n17\n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not veri\ufb01ed.\n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or\n\ufb01gures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding \ufb01gures or tables in the text.\n8. Experiments Compute Resources\nQuestion: For each experiment, does the paper provide suf\ufb01cient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJusti\ufb01cation: Computer resources needed are detailed in Sect. E of the appendix.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n\u2022 The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n\u2022 The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn\u2019t make it into the paper).\n9. Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJusti\ufb01cation: The research conducted in the paper conform with the NeurIPS Code of Ethics.\nGuidelines:\n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\u2022 If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes]\nJusti\ufb01cation: Potential societal impacts of the work are discussed in Appendix I.\nGuidelines:\n\u2022 The answer NA means that there is no societal impact of the work performed.\n\u2022 If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n\u2022 Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake pro\ufb01les, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact speci\ufb01c\ngroups), privacy considerations, and security considerations.\n\u2022 The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\n18\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n\u2022 The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the ef\ufb01ciency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJusti\ufb01cation: The paper studies GNN reuse for resource-ef\ufb01cient graph representation\nlearning and thereby, to our knowledge, poses no such risk.\nGuidelines:\n\u2022 The answer NA means that the paper poses no such risks.\n\u2022 Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety \ufb01lters.\n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n\u2022 We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJusti\ufb01cation: The existing assets are cited and properly respected with the discussions\nprovided in Appendix C.\nGuidelines:\n\u2022 The answer NA means that the paper does not use existing assets.\n\u2022 The authors should cite the original paper that produced the code package or dataset.\n\u2022 The authors should state which version of the asset is used and, if possible, include a\nURL.\n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n\u2022 If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n\u2022 For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n\u2022 If this information is not available online, the authors are encouraged to reach out to\nthe asset\u2019s creators.\n13. New Assets\n19\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJusti\ufb01cation: Details of the code and model are provided in the supplementary material.\nGuidelines:\n\u2022 The answer NA means that the paper does not release new assets.\n\u2022 Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n\u2022 The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip \ufb01le.\n14. Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJusti\ufb01cation: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022 Including this information in the supplemental material is \ufb01ne, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJusti\ufb01cation: The paper does not involve crowdsourcing nor research with human subjects.\nGuidelines:\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n\u2022 We recognize that the procedures for this may vary signi\ufb01cantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n\u2022 For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n20"
    },
    {
        "paper": "Foundation Inference Models for Markov Jump Processes",
        "input": "Foundation Inference Models for\nMarkov Jump Processes\nDavid Berghaus1, 2, Kostadin Cvejoski1, 2, Patrick Seifner1, 3\nC\u00e9sar Ojeda4 & Rams\u00e9s J. S\u00e1nchez1, 2, 3\nLamarr Institute1, Fraunhofer IAIS2, University of Bonn3 & University of Potsdam4\n{david.berghaus, kostadin.cvejoski}@iais.fraunhofer.de\nseifner@cs.uni-bonn.de, ojedamarin@uni-potsdam.de, sanchez@cs.uni-bonn.de\nAbstract\nMarkov jump processes are continuous-time stochastic processes which describe\ndynamical systems evolving in discrete state spaces. These processes find wide\napplication in the natural sciences and machine learning, but their inference is\nknown to be far from trivial. In this work we introduce a methodology for zero-\nshot inference of Markov jump processes (MJPs), on bounded state spaces, from\nnoisy and sparse observations, which consists of two components. First, a broad\nprobability distribution over families of MJPs, as well as over possible observation\ntimes and noise mechanisms, with which we simulate a synthetic dataset of hidden\nMJPs and their noisy observations. Second, a neural recognition model that\nprocesses subsets of the simulated observations, and that is trained to output\nthe initial condition and rate matrix of the target MJP in a supervised way. We\nempirically demonstrate that one and the same (pretrained) recognition model can\ninfer, in a zero-shot fashion, hidden MJPs evolving in state spaces of different\ndimensionalities. Specifically, we infer MJPs which describe (i) discrete flashing\nratchet systems, which are a type of Brownian motors, and the conformational\ndynamics in (ii) molecular simulations, (iii) experimental ion channel data and (iv)\nsimple protein folding models. What is more, we show that our model performs on\npar with state-of-the-art models which are trained on the target datasets.\nOur pretrained model, repository and tutorials are available online1.\n1\nIntroduction\nVery often one encounters dynamic phenomena of wildly different nature, that display features which\ncan be reasonably described in terms of a macroscopic variable that jumps among a finite set of\nlong-lived, metastable discrete states. Think, for example, of the changes in economic activity of\na country, which exhibit jumps between recession and expansion states (Hamilton, 1989), or the\ninternal motion in proteins or enzymes, which feature jumps between different conformational states\n(Elber and Karplus, 1987). The states in these phenomena are said to be long-lived, inasmuch as\nevery jump event among them is rare, at least as compared to every other event (or subprocess, or\nfluctuation) that composes the phenomenon and that occurs, by construction, within the metastable\nstates. Such a description in terms of macroscopic variables effectively decouples the fast, intra-state\nevents from the slow, inter-state ones, and allows for a simple probabilistic treatment of the jumping\nsequences as Markov stochastic processes: the Markov Jump Processes (MJPs). In this work we are\ninterested in the general problem of inferring the MJPs that best describe empirical (time series) data,\nrecorded from dynamic phenomena of very different kinds.\n1https://fim4science.github.io/OpenFIM/intro.html\n38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n0\n2\n4\n6\n8\n10\nTime [a.u.]\n0\n1\n2\nState\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nTime [s]\n0\n5\nCurrent [pA]\nFigure 1: Processes of very different nature (seem to) feature similar jump processes. Left: State\nvalues (blue circles) recorded from the discrete flashing ratchet process (black line). Right: Current\nsignal (blue line) recorded from the viral potassium channel KcvMT35, together with one possible\ncoarse-grained representation (black line).\nTo set the stage, let us assume that we want to study some D-dimensional empirical process\nz(t) : R+ \u2192RD, which features long-lived dynamic modes, trapped in some discrete set of\nmetastable states. Let us call this set X. Let us also assume that we can obtain a macroscopic,\ncoarse-grained representation from z(t) \u2014 say, with a clustering algorithm \u2014 in which the fast,\nintra-state events have been integrated out (i.e. marginalized). Let us call this macroscopic variable\nX(t) : R+ \u2192X. If we now make the Markov assumption and define the quantity f(x|x\u2032)\u2206t as the\ninfinitesimal probability of observing one jump from state x\u2032 (at some time t), into a different state x\n(at time t + \u2206t), we can immediately write down, following standard arguments (Gardiner, 2009),\na differential equation that describes the probability distribution pMJP(x, t), over the discrete set of\nmetastable states X, which encapsulates the state of the process X(t) as time evolves, that is\ndpMJP(x, t)\ndt\n=\nX\nx\u2032\u0338=x\n\u0010\nf(x|x\u2032)pMJP(x\u2032, t) \u2212f(x\u2032|x)pMJP(x, t)\n\u0011\n.\n(1)\nEquation 1 is the so-called master equation of the MJP whose solutions are completely characterized\nby an initial condition pMJP(x, t = 0) and the transition rates f : X \u00d7 X \u2192R+.\nWith these preliminaries in mind, we shall say that to infer an MJP from a set of (noisy) observations\nz(\u03c41), . . . , z(\u03c4l) on the process z(t), recorded at some observation times \u03c41, . . . , \u03c4l, means to infer\nboth the transition rates and the initial condition determining the hidden MJP X(t) that best explains\nthe observations. In practice, statisticians typically assume that they directly observe the coarse-\ngrained process X(t). That is, they assume they have access to the (possibly noisy) values x1, . . . , xl,\ntaken by X(t) at the observation times \u03c41, . . . , \u03c4l (see Section 2). We shall start from the same\nassumptions. Statisticians then tackle the inference problem by (i) defining some (typically complex)\nmodel that encodes, in one way or the other, equation 1 above; (ii) parameterizing the model with\nsome trainable parameter set \u03b8; and (iii) updating \u03b8 to fit the empirical dataset.\nOne issue with this approach is that it turns the inference of hidden MJPs into an instance of an\nunsupervised learning problem, which, as history shows, is far from trivial (see Section 2). Another\nmajor issue is that, if one happens to succeed in training said model, the trained parameter set \u03b8\u2217\nwill usually be overly specific to the training set {(x1, \u03c41), . . . , (xl, \u03c4l)}, which means it will likely\nstruggle to handle a second empirical process, even if the latter can be described by a similar MJP.\nFigure 1 contains snapshots from two empirical processes of very different nature. The figure on\nthe left shows a set of observations (blue circles) recorded from the discrete flashing ratchet process\n(black line). The figure on the right shows the ion flow across a cell membrane, which jumps between\ndifferent activity levels (blue line). Despite the vast differences between the physical mechanisms\nunderlying each of these processes, the coarse-grained representations of the second one (black\nline) is abstract enough to be strikingly similar to the first one. Now, we expect that \u2014 at this\nlevel of representation \u2014 one could train a single inference model to fit each process (separately).\nUnfortunately, we also expect that an inference model trained to fit only one of these (coarse-grained)\nprocesses, will have a hard time describing the second one.\nIn this paper we will argue that the notion of an MJP description (in coarse-grained space) is\nsimple enough, that it can be encoded into the weights of a single neural network model. Indeed,\ninstead of training, in an unsupervised manner, a complex model (which somehow encodes the\n2\nFigure 2: Foundation Inference Model (FIM) for MJP. Left: Graphical model of the FIM (synthetic)\ndata generation mechanism. Filled (empty) circles represent observed (unobserved) random variables.\nThe light-blue rectangle represents the continuous-time MJP trajectory, which is observed discretely\nin time. See main text for details regarding notation. Right: Inference model. The network \u03c81 is called\nK times to process K different time series. Their outputs is first processed by the attention network\n\u21261 and then by the FNNs \u03d51, \u03d52 and \u03d53 to obtain the estimates \u02c6F, log Var \u02c6F and \u02c6\u03c00, respectively.\nmaster equation) on a single empirical process; we will train, in a supervised manner, a simple\nneural network model on a synthetic dataset that is composed of many different MJPs, and hence\nimplicitly encodes the master equation. This procedure can be understood as an amortization of\nthe probabilistic inference process through a single recognition model, and is therefore akin to\nthe works of Stuhlm\u00fcller et al. (2013), Heess et al. (2013) and Paige and Wood (2016). Rather\nthan treating, as these previous works do, our (pretrained) recognition model as auxiliary to Monte\nCarlo or expectation propagation methods, we employ it to directly infer hidden MJPs from various\nsynthetic, simulation and experimental datasets, without any parameter fine-tuning. We thus adopt the\n\u201czero-shot\u201d terminology introduced by Larochelle et al. (2008), by which we mean that our procedure\naims to recognize objects (i.e. MJPs) whose instances (i.e. noisy and sparse series of observations on\nthem) may have not been seen during training. We have recently shown that such an amortization can\nbe used to train a recognition model to perform zero-shot imputation of time series data (Seifner et al.,\n2024). Below we demonstrate that it can also be used to train a model of minimal inductive biases,\nto perform zero-shot inference of hidden MJPs from empirical processes of very different kinds,\nwhich take values in state spaces of different sizes. We shall call this recognition model Foundation\nInference Model2 (FIM) for Markov jump processes.\nIn what follows, we first review both classical and recent solutions to the MJP inference problem in\nSection 2. We then introduce the FIM methodology in Section 3, which consists of a synthetic data\ngeneration model and a neural recognition model. In Section 4 we empirically demonstrate that our\nmethodology is able to infer MJPs from a discrete flashing ratchet process, as well as from molecular\ndynamics simulations and experimental ion channel data, all in a zero-shot fashion, while performing\non par with state-of-the-art models which are trained on the target datasets. Finally, Section 5 closes\nthe paper with some concluding remarks about future work, while Section 6 comments on the main\nlimitations of our methodology.\n2\nRelated Work\nThe inference of MJP from noisy and sparse observations (in coarse-grained space) is by now a\nclassical problem in machine learning. There are three main lines of research. The first (and earliest)\none attempts to directly optimize the MJP transition rates, to maximize the likelihood of the discretely\nobserved MJP via expectation maximization (Asmussen et al., 1996; Bladt and S\u00f8rensen, 2005;\nMetzner et al., 2007). Thus, these works encode the MJP inductive bias directly into their architecture.\nThe second line of research leverages a Bayesian framework to infer the posterior distribution over\nthe transition rates, through various Markov chain Monte Carlo (MCMC) algorithms (Boys et al.,\n2008; Fearnhead and Sherlock, 2006; Rao and Teg, 2013; Hajiaghayi et al., 2014). Accordingly, these\n2We name our model foundation model because it aligns with the definition proposed by Bommasani et al.\n(2021). Indeed, they define foundation models as any model that is trained on broad data (generally using\nself-supervision at scale) that can be adapted to a wide range of downstream tasks.\n3\nsimulation-based approaches encode the MJP inductive bias directly into their trainable sampling\ndistributions. The third one, also Bayesian in character, involves variational inference. Within\nit, one finds again MCMC (Zhang et al., 2017), as well as expectation maximization (Opper and\nSanguinetti, 2007) and moment-based (Wildner and Koeppl, 2019) approaches. More recently,\nSeifner and S\u00e1nchez (2023) used neural variational inference (Kingma and Welling, 2013) and neural\nODEs (Chen et al., 2018) to infer an implicit distribution over the MJP transition rates. All these\nvariational methods encode the MJP inductive bias into their training objective and, in some cases,\ninto their architecture too.\nBesides the model of Seifner and S\u00e1nchez (2023), which automatically infers the coarse-grained\nrepresentation X(t) from D-dimensional, countinuous signals, all the solutions above tackle the MJP\ninference problem directly in coarse-grained space. Yet below, we also investigate the conformational\ndynamics of physical systems for which the recorded data lies in a continuous space. To approach\nsuch type of problems, we will first need to define a coarse-grained representation of the state space\nof interest. Fortunately for us, there is a large body of works, within the molecular simulation\ncommunity, precisely dealing with different methods to obtain such representations, and we refer the\nreader to e.g. No\u00e9 et al. (2020) for a review. McGibbon and Pande (2015), for example, leveraged\none such method to infer the MJP transition rates describing a molecular dynamics simulation via\nmaximum likelihood. Alternatively, researchers have also treated the conformational states in these\nsystems as core sets, and inferred phenomenological MJP rates from them (Sch\u00fctte et al., 2011), or\nmodelled the fast intra-state events as diffusion processes, indexed by a hidden MJP, and inferred the\nlatter either via MCMC (Kilic et al., 2021; K\u00f6hs et al., 2022) or variational (Horenko et al., 2006;\nK\u00f6hs et al., 2021) methods.\nIn this work we tackle the classical MJP inference problem on coarse-grained space and present, to\nthe best of our knowledge, its first zero-shot solution.\n3\nFoundation Inference Models\nIn this section we introduce a novel methodology for zero-shot inference of Markov jump processes\nwhich frames the inference task as a supervised learning problem. Our main assumption is that the\nspace of realizable MJPs3, which take values on bounded state spaces that are not too large, is simple\nenough to be covered by a heuristically constructed synthetic distribution over noisy and discretely\nobserved MJPs. If this assumption were to hold, a model trained to infer the hidden MJPs within a\nsynthetic dataset sampled from this distribution would automatically perform zero-shot inference on\nany unseen sequence of empirical observations. We do not intend to formally prove this assumption.\nRather, we will empirically demonstrate that a model trained in such a way can indeed perform\nzero-shot inference of MJPs in a variety of cases.\nOur methodology has two components. First, a data generation model that encodes our believes about\nthe class of realizable MJPs we aim to model. Second, a neural recognition model that maps subsets\nof the simulated MJP observations onto the initial condition and rate matrix of their target MJPs. We\nwill explore the details of these two components in the following sections.\n3.1\nSynthetic Data Generation Model\nIn this subsection we define a broad distribution over possible MJPs, observation times and noise\nmechanisms, with which we simulate an ensemble of noisy, discretely observed MJPs. Before we\nstart, let us remark that we will slightly abuse notation and denote both probability distributions and\ntheir densities with the same symbols. Similarly, we will also denote both random variables and their\nvalues with the same symbols.\nLet us denote the size of the largest state space we include in our ensemble with C, and arrange all\ntransition rates, for every MJPs within the ensemble, into C \u00d7 C rate matrices. Let us label these\nmatrices with F. We define the probability of recording the noisy sequence x\u2032\n1, . . . , x\u2032\nl \u2208X, at the\n3By realizable MJPs we mean here MJPs that can be inferred from physical processes, given the typical\nexperimental constraints, like e.g. temporal or spatial resolution.\n4\nobservation times 0 < \u03c41 < \u00b7 \u00b7 \u00b7 < \u03c4l < T, with T the observation time horizon, as follows\nlY\ni=1\npnoise(x\u2032\ni|xi, \u03c1x)pMJP(xi|\u03c4i, F, \u03c00)pgrid(\u03c41, . . . , \u03c4l|\u03c1\u03c4)prates(F|A, \u03c1f)p(A, \u03c1f)p(\u03c00|\u03c10).\n(2)\nNext, we specify the different components of Eq. 2, starting from the right.\nDistribution over initial conditions. The distribution p(\u03c00|\u03c10), with hyperparameter \u03c10, is defined\nover the C-simplex, and encodes our beliefs about the initial state (i.e. the preparation) of the system.\nIt enters the master equation as the class probabilities of the categorical distribution over the states\nof the system, at the start of the process. That is pMJP(x, t = 0) = Cat(\u03c00). We either choose \u03c00 to\nbe the class probabilities of the stationary distribution of the process, or sample it from a Dirichlet\ndistribution. Appendix B provides the specifics.\nDistribution over rate matrices. The distribution prates(F|A, \u03c1f) over the rate matrices encodes\nour beliefs about the class of MJPs we expect to find in practice. We define it to cover MJPs with\nstate spaces whose sizes range from 2 until C, because we want our FIM to be able to handle\nprocesses taking values in all those spaces. The distribution is conditioned on the adjacency matrix\nA, which encodes only connected state spaces (i.e. irreducible embedded Markov chains only), and\na hyperparameter \u03c1f which encodes the range of rate values within the ensemble. Specifically, we\ndefine the transition rates as Fij = aijfij, where aij is the corresponding entry of A and fij is\nsampled from a set of Beta distributions, with different hyperparameters \u03c1f. Note that these choices\nrestrict the values of the transition rates within the ensemble to the interval (0, 1) and hence, they\nrestrict the number of resolvable transitions within the time horizon T of the simulation. We refer the\nreader to Appendix B, where we specify the prior p(A, \u03c1f) = p(A)p(\u03c1f) and its consequences, as\nwell as give details about the sampling procedure. We also discuss the main limitations of choosing a\nBeta prior over the transition rates in Section 6.\nDistribution over observation grids. The distribution pgrid(\u03c41, . . . , \u03c4l|\u03c1\u03c4), with hyperparameter \u03c1\u03c4,\ngives the probability of observing the MJP at the times \u03c41, . . . , \u03c4l, and thus encodes our uncertainty\nabout the recording process. Given that we do not know a priori whether the data will be recorded\nregularly or irregularly in time, nor we know its recording frequency, we define this distribution to\ncover both regular and irregular cases, as well as various recording frequencies. Note that the number\nof observation points on the grid is variable. Please see Appendix B for details.\nDistribution over noise process. Just as the (instantaneous) solution of the master equation\npMJP(x|t, F, \u03c00), the noise distribution pnoise(x\u2032|x, \u03c1x), with hyperparameter \u03c1x, is defined over the set\nof metastable states X. Recall that FIM solves the MJP inference problem directly in coarse-grained\nspace. The noise distributions then encodes both, possible measurement errors that propagate through\nthe coarse-grained representation, or noise in the coarse-grained representation itself. We provide\ndetails of its implementation in Appendix B.\nWe use the generative model, Eq. 2 above, to generate N MJPs, taking values on state spaces with\nsizes ranging from 2 to C. We then sample K paths per MJP, with probability p(K), on the interval\n[0, T]. The jth instance of the dataset thus consists of K paths and is given by\nFj \u223cprates(F|Aj, \u03c1fj), and \u03c00j \u223cp(\u03c00|\u03c10), with (Aj, \u03c1fj) \u223cp(A, \u03c1f),\nso that\nn\nXjk(t)\noK\nk=1 \u223cGillespie(Fj, \u03c00j),\n(3)\nand\nn\nx\u2032\njki \u223cpnoise(x\u2032|Xjk(\u03c4jki))\no(K,l)\n(k,i)=(1,1), with\nn\n\u03c4jk1, . . . , \u03c4jkl\noK\nk=1 \u223cpgrid(\u03c41, . . . , \u03c4l|\u03c1\u03c4),\nwhere Gillespie denotes the Gillespie algorithm we use to sample the MJP paths (see Algorithm 1).\nNote that we make the number of paths (K above) per MJP random, because we do not know a priori\nhow many realizations (i.e. experiments), from the empirical process of interest, will be available at\nthe inference time. We refer the reader to Appendix B for additional details.\nFigure 2 illustrates the complete data generation process.\n3.2\nSupervised Recognition Model\nIn this subsection we introduce a neural recognition model that processes a set of K time series of the\nform {(x\u2032\nk1, \u03c4k1), . . . , (x\u2032\nkl, \u03c4kl)}K\nk=1, as generated by the procedure in Eq. 3 above, and estimates\n5\nthe intensity rate matrix F and initial distribution \u03c00 of the hidden MJP. Practically speaking, we\nwould like the model to be able to infer MJPs from time series with observation times on any scale.\nTo ensure this, we first normalize all observation times to lie on the unit interval, by dividing them\nby the maximum observation time \u03c4max = max{\u03c4k1, . . . , \u03c4kl}K\nk=1, and then rescale the output of the\nmodel accordingly (see Appendix C for details).\nLet us use \u03d5, \u03c8 and \u2126to denote feed-forward, sequence processing networks, and attention networks,\nrespectively. Thus \u03c8 can denote e.g. LSTM or Transformer networks, while \u2126can denote e.g. a\nself-attention mechanism. Let us also denote the networks\u2019 parameters with \u03b8.\nWe first process each time series with a network \u03c81 to get a set of K embeddings, which we then\nsummarize into a global representation h\u03b8 through the attention network \u21261. In equations, we write\nh\u03b8 = \u21261(h1\u03b8, . . . , hK\u03b8, \u03b8) with hk\u03b8 = \u03c81(x\u2032\nk1, \u03c4k1, . . . , x\u2032\nkl, \u03c4kl, \u03b8) and k = 1, . . . , K.\n(4)\nNext we use the global representation to get an estimate of the intensity rate matrix, which we\nartificially model as a Gaussian variable with positive mean, and the initial distribution of the hidden\nMJP as follows\n\u02c6F = exp(\u03d51(h\u03b8, \u03b8)),\nVar \u02c6F = exp(\u03d52(h\u03b8, \u03b8)) and\n\u02c6\u03c00 = \u03d53(h\u03b8, \u03b8),\n(5)\nwhere the exponential function ensures the positivity of our estimates, and the variance is used to\nrepresent the model\u2019s uncertainty in the estimation of the rates (Seifner et al., 2024). The right panel\nof Figure 2 summarizes the recognition model, and Appendix C provides additional information\nabout the inputs to, outputs of and rescalings done by the model.\nTraining objective. We train the model to maximize the likelihood of its predictions, taking care of\nthe exact zeros (i.e. the missing links) in the data. To wit\nL\n=\n\u2212\nE\nF,A\u223cprates\nn\nC\nX\nij=1\naij\nh(fij \u2212\u02c6fij)2\n2Var \u02c6fij\n+ 1\n2 log Var \u02c6fij\ni\n\u2212\u03bb(1 \u2212aij)\nh\n\u02c6f 2\nij + Var \u02c6fij\nio\n\u2212E\n\u03c00\u223cp\nn\nC\nX\ni=1\n\u03c0i0 log \u02c6\u03c0i0\no\n,\n(6)\nwhere the second term is nothing but the mean-squared error of the predicted rates \u02c6fij (and its\nstandard deviation) when the corresponding link is missing, and can be understood as a regularizer\nwith weight \u03bb. The latter is a hyperparameter.\nFIM context number. During training, FIM processes a variable number K of time series, which\nlies on the interval [Kmin, Kmax]. Similarly, each one of these time series has a variable number l of\nobservation points, which lies on the interval [lmin, lmax]. We shall say that FIM needs a bare minimum\nof Kminlmin input data points to function. Perhaps unsurprisingly, we have empirically seen that FIM\nperform bests when processing Kmaxlmax data points. Going significantly beyond this number seems\nnevertheless to decrease the performance of FIM. We invite the reader to check Appendix D for\ndetails.\nLet us define then, for the sake of convenience, the FIM context number c(K, l) = Kl as the number\nof input points4 FIM makes use of to estimate F and \u03c00.\n4\nExperiments\nIn this section we test our methodology on five datasets of varying complexity, and corrupted by\nnoise signals of very different nature, whose hidden MJPs are known to take values in state spaces of\ndifferent sizes. In what follows we use one and the same (pretrained) FIM to infer hidden MJPs from\nall these datasets, without any parameter fine-tuning. Our FIM was (pre)trained on a dataset of 45K\nMJPs, defined over state spaces whose sizes range from 2 to 6. A maximum of (K =)300 realizations\n(paths) per MJP were observed during training, everyone of which spanned a time-horizon T = 10,\nrecorded at a maximum of 100 time points, 1% of which were mislabeled. Given these specifications,\nFIM is expected to perform best for the context number c(300, 100) during evaluation. Additional\n4We can think about it as the context length in large language models.\n6\nFigure 3: Illustration of the six-state dis-\ncrete flashing ratchet model. The potential\nV is switched on and off at rate r. The\ntransition rates f on\nij, f off\nij allow the particle\nto propagate through the ring.\nV\nr\nb\nGROUND TRUTH\n1.00\n1.00\n1.00\nNEURALMJP\n1.06\n1.17\n1.14\nFIM\n1.11(7)\n0.99(8)\n0.98(5)\nTable 1: Inference of the discrete flashing ratchet\nprocess. The FIM results correspond to FIM evalu-\nations with context number c(300, 50), averaged over\n15 batches.\ninformation regarding model architecture, hyperparameter selection and other training details can be\nfound in Appendix D.\nBaselines: Depending on the dataset, we compare our findings against the NeuralMJP model of\nSeifner and S\u00e1nchez (2023), the switching diffusion model (SDiff) of K\u00f6hs et al. (2021), and the\ndiscrete-time Markov model (VampNets) of Mardt et al. (2017).\nAll these baselines are trained on the target datasets.\n4.1\nThe Discrete Flashing Ratchet (DFR): A Proof of Concept\nIn statistical physics, the ratchet effect refers to the rectification of thermal fluctuations into directed\nmotion to produce work, and goes all the way back to Feynman (Feynman et al., 1965). Here we\nconsider a simple example thereof, in which a Brownian particle, immersed in a thermal bath at unit\ntemperature, moves on a one-dimensional lattice. The particle is subject to a linear, periodic and\nasymmetric potential of maximum height 2V that is switched on and off at a constant rate r. The\npotential has three possible values when is switched on, which correspond to three of the states of the\nsystem. The particle jumps among them with rate f on\nij. When the potential is switched off, the particle\njumps freely with rate f off\nij . We can therefore think of the system as a six-state system, as illustrated\nin Figure 3. Similar to Rold\u00e1n and Parrondo (2010), we now define the transition rates as\nf on\nij = exp\n\u0012\n\u2212V\n2 (j \u2212i)\n\u0013\n, for i, j \u2208(0, 1, 2);\nf off\nij = b, for i, j \u2208(3, 4, 5).\n(7)\nGiven these specifics, we consider the parameter set (V, r, B) = (1, 1, 1) together with the dataset\nsimulated by Seifner and S\u00e1nchez (2023), which consists of 5000 paths (in coarse-grained space)\nrecorded on an irregular grid of 50 time points. The task is to infer (V, r, B) from these time series.\nNeuralMJP infers a global distribution over the rate matrices and hence relies on their entire train set,\nwhich amounts to about 4500 time series. We therefore report FIM evaluations with context number\nc(300, 50) on that same train set, averaged over 15 (non-overlapping) batches in Table 1.\nThe results show that FIM performs on par with (or even better than) NeuralMJP, despite not having\nbeen trained on the data. Note in particular that our results are sharply peaked around their mean,\nindicating that a context of c(300, 50) points only contains enough information to describe the\ndata well. What is more, Table 16 in the Appendix demonstrates that FIM can infer vanishing\ntransition rates as well (see Eq. 6). Now, being able to infer the rate matrix in zero-shot mode allows\nus to immediately estimate a number of observables of interest without any training. Stationary\ndistributions, relaxation times and mean first-passage times (see Appendix A for their definition), as\nwell as time-dependent moments, can all be computed zero-shot via FIM. For example, we report\non the left block of Figure 4 the time-dependent class probabilities (i.e. the master eq. solutions)\ncomputed with the FIM-inferred rate matrix (black), against the ground-truth solution (blue). The\nagreement is very good.\nZero-shot estimation of entropy production. The DFR model is interesting because the random\nswitching combined with the asymmetry in the potential make it more likely for the particle to jump\ntowards the right (see Figure 4). Indeed, that is the ratchet effect. As a consequence, the system\nfeatures a stationary distribution with a net current \u2014 the so-called non-equilibrium steady state\n7\n0.0\n0.5\n1.0\n1.5\n2.0\nTime\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nProbability\np0\np1\np2\np3\np4\np5\n0\n1\n2\n3\nV\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nEntropy Production\nPrediction\nGround Truth\nFigure 4: Zero-shot inference of DFR process. Left: master eq. solution pMJP(x, t) as time evolves,\nwrt. the (averaged) FIM-inferred rate matrix is shown in black. The ground-truth solution is shown in\nblue. Right: Total entropy production computed from FIM (over a time-horizon T = 2.5 [a.u.]). The\nmodel works remarkably well for a continuous range of potential values.\n(Ajdari and Prost, 1992), which is characterized by a non-vanishing (stochastic) entropy production.\nThe development of (neural) estimators of entropy production is a very active topic of current research\n(see e.g. Kim et al. (2020) and Otsubo et al. (2022)). Given that the entropy production can be written\ndown in closed form as a function of both the rate matrix and the master eq. solution (see e.g. Seifert\n(2012)), we can readily use FIM to estimate it.\nFigure 4 displays the total entropy production computed with FIM for a set of different potentials. The\nresults are averaged over 15 FIM evaluations with c(300, 50) and are again in very good agreement\nwith the ground truth. It is noteworthy that FIM, trained on our heuristically constructed dataset,\ncaptures well a continuous set of MJPs. That is, we evaluate one and the same FIM over different\ndatasets, each sampled from a DFR model with a different potential value. In sharp contrast, state-of-\nthe-art models need to be retrained for every new potential value (Kim et al., 2020).\nZero-shot simulation of the DFR process. Inferring the rate matrix and initial condition of a MJP\nprocess entails that one can also sample from it. Our FIM can thus be used as a zero-shot generative\nmodel for MJPs. However, to test the quality of said MJP realizations wrt. some target MJP, we need\na distance between the two. Here we propose to use the Hellinger distance (Le Cam and Yang, 2000)\nto first estimate the divergence between a sequence of (local) histogram pairs, recorded at a given\nset of observation times, and then average the local estimates along time. Appendix F.1 empirically\ndemonstrates that this pragmatically defined MJP distance is sensible.\nTable 2 reports the time-averaged Hellinger distance between 1000 (ground-truth) DFR paths and\n1000 paths sampled from (the MJPs inferred by) NeuralMJP and FIM. We repeat this calculation\n100 times, for 1000 newly sampled paths from NeuralMJP and FIM, but the same 1000 target paths,\nto compute the mean values and error bars in the Table. The results show that the zero-shot DFR\nsimulation obtained through FIM is on par with the NeuralMJP-based simulation, wrt. the ground\ntruth.\n4.2\nSwitching Ion Channel (IonCh): Zero-Shot Inference of Three-State MJP\nIn this section we study the conformational dynamics of the viral ion channel KcvMT325, which\nexhibits three metastable states (Gazzarrini et al., 2006). Specifically, we analyse the ion flow across\nthe membrane as the system jumps between its metastable configurations. This ion flow was recorded\nat a frequency of 5kHz over one second. Figure 1 shows one snapshot of these recordings, which\nwere made available to us via private communication (see the Acknowledgements). Our goal is to\ninfer physical observables \u2014 like the stationary distribution and mean first-passage times \u2014 of the\nconformational dynamics, and to compare our findings against the SDiff model of K\u00f6hs et al. (2021)\nand NeuralMJP.\nThe recordings live in real space, which means that we first need to obtain a coarse-grained represen-\ntation (CGR) from them, before we can apply FIM. Here we consider two CGRs: the CGR inferred\n8\nDataset\nNEURALMJP\nFIM\nDFR\n0.30(0.06)\n0.27(0.06)\nIONCH\n0.48(0.02)\n0.41(0.02)\nADP\n1.38(0.52)\n1.39(0.47)\nPFOLD\n0.015(0.015)\n0.014(0.014)\nTable 2: Time-averaged Hellinger distances\nbetween empirical processes and samples\nfrom either NeuralMJP or FIM [in a 1e-2\nscale] (lower is better). Mean and std. are\ncomputed from a set of 100 histograms\nBOTTOM\nMIDDLE\nTOP\nSDIFF\n0.17961\n0.14987\n0.67052\nNEURALMJP\n0.17672\n0.09472\n0.72856\nFIM-NMJP\n0.18224\n0.10156\n0.71621\nFIM-GMM\n0.19330\n0.08124\n0.72546\nTable 3: Stationary distribution inferred from the\nswitching ion channel experiment. FIM-NMJP and\nFIM-GMM correspond to our inference from differ-\nent coarse-grained representations. The results agree\nwell.\nby NeuralMJP and a naive CGR obtained with a Gaussian Mixture Model (GMM). Given that we\nonly have 5000 observations available, we make use of a single FIM evaluation with context number\nc(50, 100). We infer two FIM rate matrices, one per each CGR, which we label as FIM-NMJP and\nFIM-GMM.\nTable 3 contains the inferred stationary distributions from all models and evidences that a single\nFIM evaluation is enough to unveil the long-time asymptotics of the process. Similarly, Table 15 in\nthe Appendix, which contains the inferred mean-first passage times, demonstrates that FIM makes\nthe same inference about the short-term dynamics of the process as do SDiff and NeuralMJP. See\nAppendix F for additional results.\nZero-shot simulation of switching ion channel process. Just as we did with the DFR process, we\ncan use FIM to simulate the switching ion channel process in coarse-grained space. Since only paths\non the same CG space can be compared, we evaluate NeuralMJP against FIM-NMJP. To construct\nthe target distribution, we leverage another 30 seconds of measurements, which amount to 150K\nobservations that have not been seen by any of the models. The results in Table 2 indicate that our\nzero-shot simulations is statistically closer to the ground-truth process than the NeuralMJP simulation.\n4.3\nAlanine Dipeptide (ADP): Zero-Shot Inference of Six-State MJP\nAlanine dipeptide is 22-atom molecule widely used as benchmark in molecular dynamics simulation\nstudies. Its popularity stems from the fact that the heavy-atom dynamics, which jumps between six\nmetastable states, can be fully described in terms of the dihedral (torsional) angles \u03c8 and \u03d5 (see\ne.g. Mironov et al. (2019) for details).\nWe examine an all-atom ADP simulation of 1 microsecond, which was made available to us via\nprivate communication (see the Acknowledgements below), and compare against both, the VampNets\nmodel of Mardt et al. (2017) and NeuralMJP. The data consists of the values taken by the dihedral\nangles as time evolves and thus needs to be mapped onto some coarse-grained space. We again make\nuse of NeuralMJP to obtain a CGR. We then use FIM with context number c(300, 100) to process 32\n100-point time windows of the simulation and compute an average rate matrix. Note that this is the\noptimal context number of our pretrained model. Table 4 (and Appendix F.2) confirms that, once\nagain, FIM can infer the same physical properties from the ADP simulation as the baselines.\nZero-shot simulation of the alanine dipeptide. Simulations in coarse-grained space for molecular\ndynamics is a high-interest research direction (Husic et al., 2020). Here we demonstrate that FIM can\nbe used to simulate the ADP process in zero-shot mode. Indeed, Table 2 reports the distance from\nboth NeuralMJP and FIM to a target ADP process, computed from 200 paths with 100 observations\neach. Once more, FIM performs comparable to NeuralMJP.\n4.4\nZero-Shot Inference of Two-State MJPs\nFinally, we consider two additional systems that feature jumps between two metastable states: a\nsimple protein folding model and a two-mode switching system. We invite the reader to check out\nAppendix F.5 and F.6 for the details. That being said, Table 8 reports the distance of both NeuralMJP\nand FIM wrt. the empirical protein folding process (PFold). The high variance indicates that the\ndistance cannot resolve any difference between the processes given the available number of samples.\n9\nPROBABILITY PER STATE\nRELAXATION TIME SCALES (IN ns)\nI\nII\nIII\nIV\nV\nVI\nVAMPNETS\n0.30\n0.24\n0.20\n0.15\n0.11\n0.01\n0.008\n0.009\n0.055\n0.065\n1.920\nNEURALMJP\n0.30\n0.31\n0.23\n0.10\n0.05\n0.01\n0.009\n0.009\n0.043\n0.069\n0.774\nFIM\n0.28\n0.28\n0.24\n0.07\n0.10\n0.03\n0.008\n0.009\n0.079\n0.118\n0.611\nTable 4: Left: stationary distribution of the ADP process. The states are ordered in such a way that\nthe ADP conformations associated with a given state are comparable between the VampNets and\nNeuralMJP CGRs. Right: relaxation time scales to stationarity. FIM agrees well with both baselines.\n5\nConclusions\nIn this work we introduced a novel methodology for zero-shot inference of Markov jump processes\nand its Foundation Inference Model (FIM). We empirically demonstrated that one and the same\nFIM can be used to estimate stationary distributions, relaxation times, mean first-passage times,\ntime-dependent moments and thermodynamic quantities (i.e. the entropy production) from noisy and\ndiscretely observed MJPs, taking values in state spaces of different dimensionalities, all in zero-shot\nmode. To the best of our knowledge, FIM is also the first zero-shot generative model for MJPs.\nFuture work shall involve extending our methodology to Birth and Death processes, as well as\nconsidering more complex (prior) transition rate distributions. See our discussion on Limitations in\nthe next section, for details.\n6\nLimitations\nThe main limitations of our methodology clearly involve our synthetic distribution. Evaluating FIM\non empirical datasets whose distribution significantly deviates from our synthetic distribution will,\ninevitably, yield poor estimates. Consider Figure 4 (right), for example. The performance of FIM\nquickly deteriorates for V \u22653, for which the ratio between the largest and smallest rates gets larger\nthan about three orders of magnitude. These cases are unlikely under our prior Beta distributions, and\nhence effectively lie outside of our synthetic distribution.\nMore generally, the MJP dynamics underlying phenomena that feature long-lived, metastable states,\nultimately depends on the shape of the energy landscape characterizing the set X, inasmuch as the\ntransition rates between metastable states i and j (fij in our notation) are characterized by the depth\nof the energy traps (that is, the height of the barrier between them).\nIn equations, we write\nfij = exp\n\u0012\u2212Ej\nT\n\u0013\n,\n(8)\nwhere Ej is the jth trap depth, and T is the temperature of the system. Therefore, the distribution\nover energy traps determines the distribution over transition rates.\nJust to give an example, if we studied systems with exponentially distributed energy traps \u2014 as\ne.g. in the classical Trap model of glassy systems of Bouchaud (1992) \u2014 we would immediately find\np(f) \u221dTf T \u22121. Transition rates sampled from such power-law distributions clearly lie outside our\nensemble of Beta distributions, even if we use our rescaling trick. Future work shall explore training\nFIM on synthetic MJPs featuring power-law-distributed transition rates.\nAcknowledgements\nThis research has been funded by the Federal Ministry of Education and Research of Germany and the\nstate of North-Rhine Westphalia as part of the Lamarr Institute for Machine Learning and Artificial\nIntelligence. Additionally, C\u00e9sar Ojeda was supported by Deutsche Forschungsgemeinschaft (DFG) \u2013\nProject-ID 318763901 \u2013 SFB1294.\nWe would like to thank Lukas K\u00f6hs for sharing the experimental ion channel data with us. The actual\nexperiment was carried out by Kerri Kukovetz and Oliver Rauh while working in the lab of Gerhard\n10\nThiel of TU Darmstadt. Similarly, we would like to thank Nick Charron and Cecilia Clementi, from\nthe Theoretical and Computational Biophysics group of the Freie Universit\u00e4t Berlin, for sharing the\nall-atom alanine dipeptide simulation data with us. The simulation was carried out by Christoph\nWehmeyer while working in the research group of Frank No\u00e9 of the Freie Universit\u00e4t Berlin.\nReferences\nArmand Ajdari and Jaxques Prost. Mouvement induit par un potentiel p\u00e9riodique de basse sym\u00e9trie:\ndi\u00e9lectrophorese puls\u00e9e. Comptes rendus de l\u2019Acad\u00e9mie des sciences. S\u00e9rie 2, M\u00e9canique, Physique,\nChimie, Sciences de l\u2019univers, Sciences de la Terre, 315(13):1635\u20131639, 1992.\nS\u00f8ren Asmussen, Olle Nerman, and Marita Olsson. Fitting phase-type distributions via the em\nalgorithm. Scandinavian Journal of Statistics, pages 419\u2013441, 1996.\nMogens Bladt and Michael S\u00f8rensen. Statistical inference for discretely observed markov jump\nprocesses. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(3):\n395\u2013410, 2005.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-\nties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nJean-Philippe Bouchaud. Weak ergodicity breaking and aging in disordered systems. Journal de\nPhysique I, 2(9):1705\u20131713, 1992.\nRichard J Boys, Darren J Wilkinson, and Thomas BL Kirkwood. Bayesian inference for a discretely\nobserved stochastic kinetic model. Statistics and Computing, 18(2):125\u2013135, 2008.\nTian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David Kristjanson Duvenaud. Neural ordinary\ndifferential equations. In Neural Information Processing Systems, 2018.\nR Elber and Martin Karplus. Multiple conformational states of proteins: a molecular dynamics\nanalysis of myoglobin. Science, 235(4786):318\u2013321, 1987.\nP Erd\u00f6s and A R\u00e9nyi. On random graphs i. Publ. math. debrecen, 6(290-297):18, 1959.\nPaul Fearnhead and Chris Sherlock. An exact gibbs sampler for the markov-modulated poisson\nprocess. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 68(5):\n767\u2013784, 2006.\nRichard P Feynman, Robert B Leighton, Matthew Sands, and Everett M Hafner. The feynman\nlectures on physics; vol. i. American Journal of Physics, 33(9):750\u2013752, 1965.\nCrispin W. Gardiner. Stochastic methods: A handbook for the natural and social sciences. 2009.\nSabrina Gazzarrini, Ming Kang, Svetlana Epimashko, James L Van Etten, Jack Dainty, Gerhard Thiel,\nand Anna Moroni. Chlorella virus mt325 encodes water and potassium channels that interact\nsynergistically. Proceedings of the National Academy of Sciences, 103(14):5355\u20135360, 2006.\nDaniel T. Gillespie. Exact stochastic simulation of coupled chemical reactions. The Journal of\nPhysical Chemistry, 81:2340\u20132361, 1977.\nMonir Hajiaghayi, Bonnie Kirkpatrick, Liangliang Wang, and Alexandre Bouchard-C\u00f4t\u00e9. Efficient\ncontinuous-time markov chain estimation. In International Conference on Machine Learning,\npages 638\u2013646. PMLR, 2014.\nJames D Hamilton. A new approach to the economic analysis of nonstationary time series and the\nbusiness cycle. Econometrica: Journal of the econometric society, pages 357\u2013384, 1989.\nNicolas Heess, Daniel Tarlow, and John Winn. Learning to pass expectation propagation messages.\nAdvances in Neural Information Processing Systems, 26, 2013.\nSepp Hochreiter and J\u00fcrgen Schmidhuber. Long Short-Term Memory. Neural Computation, 9(8):\n1735\u20131780, 1997.\n11\nIllia Horenko, Evelyn Dittmer, Alexander Fischer, and Christof Sch\u00fctte. Automated model reduction\nfor complex systems exhibiting metastability. Multiscale Modeling & Simulation, 5(3):802\u2013827,\n2006.\nBrooke E. Husic, Nicholas E. Charron, Dominik Lemm, Jiang Wang, Adri\u00e0 P\u00e9rez, Maciej Majewski,\nAndreas Kr\u00e4mer, Yaoyi Chen, Simon Olsson, Gianni de Fabritiis, Frank No\u00e9, and Cecilia Clementi.\nCoarse graining molecular dynamics with graph neural networks. The Journal of Chemical Physics,\n153(19):194101, 2020.\nZeliha Kilic, Ioannis Sgouralis, and Steve Press\u00e9. Generalizing hmms to continuous time for fast\nkinetics: Hidden markov jump processes. Biophysical journal, 120(3):409\u2013423, 2021.\nDong-Kyum Kim, Youngkyoung Bae, Sangyun Lee, and Hawoong Jeong. Learning entropy produc-\ntion via neural networks. Physical Review Letters, 125(14):140604, 2020.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nLukas K\u00f6hs, Bastian Alt, and Heinz Koeppl. Variational inference for continuous-time switching\ndynamical systems. In Advances in Neural Information Processing Systems, volume 34, pages\n20545\u201320557, 2021.\nLukas K\u00f6hs, Bastian Alt, and Heinz Koeppl. Markov chain monte carlo for continuous-time switching\ndynamical systems. In International Conference on Machine Learning, pages 11430\u201311454. PMLR,\n2022.\nHugo Larochelle, Dumitru Erhan, and Yoshua Bengio. Zero-data learning of new tasks. In AAAI,\nvolume 1, page 3, 2008.\nLucien Marie Le Cam and Grace Lo Yang. Asymptotics in statistics: some basic concepts. Springer\nScience & Business Media, 2000.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nAndreas Mardt, Luca Pasquali, Hao Wu, and Frank No\u00e9. Vampnets for deep learning of molecular\nkinetics. Nature Communications, 9, 2017.\nRobert T McGibbon and Vijay S Pande. Efficient maximum likelihood parameterization of continuous-\ntime markov processes. The Journal of chemical physics, 143(3):034109, 2015.\nPhilipp Metzner, Illia Horenko, and Christof Sch\u00fctte. Generator estimation of markov jump processes\nbased on incomplete observations nonequidistant in time. Phys. Rev. E, 76:066702, Dec 2007.\nVladimir Mironov, Yuri Alexeev, Vikram Khipple Mulligan, and Dmitri G. Fedorov. A systematic\nstudy of minima in alanine dipeptide. Journal of Computational Chemistry, 40(2):297\u2013309, 2019.\nFrank No\u00e9, Alexandre Tkatchenko, Klaus-Robert M\u00fcller, and Cecilia Clementi. Machine learning\nfor molecular simulation. Annual review of physical chemistry, 71:361\u2013390, 2020.\nM. Opper and G. Sanguinetti. Variational inference for markov jump processes. In NIPS, 2007.\nShun Otsubo, Sreekanth K Manikandan, Takahiro Sagawa, and Supriya Krishnamurthy. Estimating\ntime-dependent entropy production from non-equilibrium trajectories. Communications Physics, 5\n(1):11, 2022.\nBrooks Paige and Frank Wood. Inference networks for sequential monte carlo in graphical models.\nIn International Conference on Machine Learning, pages 3040\u20133049. PMLR, 2016.\nVinayak Rao and Yee Whte Teg. Fast mcmc sampling for markov jump processes and extensions.\nJournal of Machine Learning Research, 14(11), 2013.\n\u00c9. Rold\u00e1n and J. M. R. Parrondo. Estimating dissipation from single stationary trajectories. Physical\nreview letters, 105 15:150607, 2010.\n12\nChristof Sch\u00fctte, Frank No\u00e9, Jianfeng Lu, Marco Sarich, and Eric Vanden-Eijnden. Markov state\nmodels based on milestoning. The Journal of chemical physics, 134(20), 2011.\nUdo Seifert. Stochastic thermodynamics, fluctuation theorems and molecular machines. Reports\non Progress in Physics, 75(12):126001, nov 2012. doi: 10.1088/0034-4885/75/12/126001. URL\nhttps://dx.doi.org/10.1088/0034-4885/75/12/126001.\nPatrick Seifner and Rams\u00e9s J S\u00e1nchez. Neural markov jump processes. In International Conference\non Machine Learning, pages 30523\u201330552. PMLR, 2023.\nPatrick Seifner, Kostadin Cvejoski, and Ramses J Sanchez. Foundational inference models for\ndynamical systems. arXiv preprint arXiv:2402.07594, 2024.\nAndreas Stuhlm\u00fcller, Jacob Taylor, and Noah Goodman. Learning stochastic inverses. Advances in\nneural information processing systems, 26, 2013.\nBenjamin Trendelkamp-Schroer and Frank No\u00e9. Efficient estimation of rare-event kinetics. arXiv:\nChemical Physics, 2014.\nYasemin Bozkurt Varolg\u00fcnes, T. Bereau, and Joseph F. Rudzinski. Interpretable embeddings from\nmolecular simulations using gaussian mixture variational autoencoders. Machine Learning: Science\nand Technology, 1, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pages 5998\u20136008, 2017.\nChristian Wildner and Heinz Koeppl. Moment-based variational inference for markov jump processes.\nIn International Conference on Machine Learning, pages 6766\u20136775. PMLR, 2019.\nBoqian Zhang, Jiangwei Pan, and Vinayak A Rao. Collapsed variational bayes for markov jump\nprocesses. Advances in Neural Information Processing Systems, 30, 2017.\n13\nA\nBackground on MJPs\nIn this section we provide some brief background on MJPs and describe how physical quantities such\nas the stationary distributions, relaxation times and mean first passage times can be computed from\nthe intensity matrix. Additionally, we mention how trajectories for MJPs can be sampled using the\nGillespie algorithm.\nA.1\nBackground on Markov Jump Processes in Continuous Time\nMarkov jump processes are stochastic models used to describe systems that transition between states\nat random times. These processes are characterized by the Markov property where the future state\ndepends only on the current state, not on the sequence of events that preceded it.\nA continuous-time MJP X(t) has right-continuous, piecewise-constant paths and takes values in a\ncountable state space X over a time interval [0, T]. The instantaneous probability rate of transitioning\nfrom state x\u2032 to x is defined as\nf(x|x\u2032, t) = lim\n\u2206t\u21920\n1\n\u2206tpMJP(x, t + \u2206t|x\u2032, t),\n(9)\nwhere pMJP(x, t|x\u2032, t\u2032) denotes the transition probability.\nThe evolution of the state probabilities pMJP(x, t) is governed by the master equation\ndpMJP(x, t)\ndt\n=\nX\nx\u2032\u0338=x\n\u0010\nf(x|x\u2032)pMJP(x\u2032, t) \u2212f(x\u2032|x)pMJP(x, t)\n\u0011\n.\n(10)\nFor homogeneous MJPs with time-independent transition rates, the master equation in matrix form is\ndpMJP(x, t)\ndt\n(t) = pMJP(t) \u00b7 F,\n(11)\nwith the solution given by the matrix exponential\npMJP(t) = pMJP(0) \u00b7 exp(Ft).\n(12)\nA.2\nStationary Distribution\nThe stationary distribution p\u2217\nMJP of a homogeneous MJP is a probability distribution over the state\nspace X that satisfies the condition p\u2217\nMJP \u00b7 F = 0. This implies that the stationary distribution is a left\neigenvector of the rate matrix corresponding to the eigenvalue 0.\nA.3\nRelaxation Times\nThe relaxation time of a homogeneous MJP is determined by its non-zero eigenvalues \u03bb2, \u03bb3, . . . , \u03bb|X|.\nThese eigenvalues define the time scales of the process: |Re(\u03bb2)|\u22121, |Re(\u03bb3)|\u22121, . . . , |Re(\u03bb|X|)|\u22121.\nThese time scales are indicative of the exponential rates of decay toward the stationary distribution.\nThe relaxation time, which is the longest of these time scales, dominates the long-term convergence\nbehavior. If the eigenvalue corresponding to the relaxation time has a non-zero imaginary part, then\nthis means that the system does not converge into a fixed stationary distribution but that it instead\nends in a periodic oscillation.\nA.4\nMean First-Passage Times (MFPT)\nFor an MJP starting in a state i \u2208X, the first-passage time to another state j \u2208X is defined as the\nearliest time t at which the MJP reaches state j, given it started in state i. The mean first-passage\ntime (MFPT) \u03c4ij is the expected value of this time. For a finite state, time-homogeneous MJP, the\nMFPTs can be determined by solving a series of linear equations for each state j, distinct from i,\nwith the initial condition that \u03c4ii = 0\n\u001a\u03c4ii = 0\n1 + P\nk Fik\u03c4kj = 0,\nj \u0338= i\n(13)\n14\nA.5\nThe Gillespie Algorithm for Continuous-Time Markov Jump Processes\nThe Gillespie algorithm (Gillespie, 1977) is a stochastic simulation algorithm used to generate\ntrajectories of Markov jump processes in continuous time. The algorithm proceeds as follows:\nAlgorithm 1 Gillespie Algorithm for Markov Jump Processes\n1: INPUT: The intensity matrix F, the initial state distribution \u03c00, the starting time t0 and the end\ntime tend\n2: Initialize the time t to the starting time t0\n3: Initialize the system\u2019s state s to an initial state s0 \u223c\u03c00\n4: While t < tend do\n5:\nCalculate the intensity \u03bb = \u22121/Fss from state s\n6:\nSample the time \u03c4 to the next event from an exponential distribution with rate \u03bb\n7:\nUpdate the time t \u2190t + \u03c4\n8:\nIf t \u2265tend then exit loop\n9:\nCalculate transition probabilities p = \u2212Fsj/Fss for each possible next state j\n10:\nSet ps to zero because we allow for no self jumps\n11:\nSample the next state s\u2032 from the distribution defined by p\n12:\nUpdate the system\u2019s state s \u2190s\u2032\n13:\nRecord the state s and time t\n14: End while\n15: OUTPUT: The trajectory of states and times\nB\nSynthetic Dataset Generation: Statistics and other Details\nThis section is a continuation of section 3.1 and provides more details on the generation of our\nsynthetic training dataset. Additionally, we provide some statistics about the dataset distribution.\nB.1\nPrior Distributions and their Implementation\nIn this subsection we give additional details about our data generation mechanism.\nDistribution over rate matrices. Our data generation procedure starts by sampling the entries fij of\nthe intensity matrix from the following beta distributions\np(fij|\u03c1f) = Beta(\u03c1f = (\u03b1, \u03b2)), with p(\u03b1) = Uniform({1, 2})\nand p(\u03b2) = Uniform({1, 3, 5, 10}).\n(14)\nBoth these discrete uniform distribution define the prior p(\u03c1f) = p(\u03b1)p(\u03b2).\nThe choices for \u03b1 and \u03b2 were made heuristically, to obtain reasonable (i.e. varied) distributions over\nthe number of jumps (see e.g. Figure 5). We remark that we fixed this set of training distributions\nbefore evaluating the model on the evaluation sets, in order to prevent us from introducing unwanted\nbiases into the distribution hyperparameters by optimizing on the evaluation set.\nNext we define the prior over the adjacency matrix as\np(A) = 1\n2\u03b4(A \u2212J) + 1\n2pErd\u00f6s-R\u00e9nyi(A, p = 0.5)\n(15)\nwhere \u03b4(\u00b7) labels the Dirac delta distribution and J denotes the matrix for which all off-diagonal\nentries are 1 and the diagonal ones are 0. Furthermore pErd\u00f6s-R\u00e9nyi labels the Erd\u00f6s-R\u00e9nyi model (Erd\u00f6s\nand R\u00e9nyi, 1959), for which each link is defined via an independent Bernoulli variable, with some\nfixed, global probability p, here set to 1\n2. Equation 15 indicates that (in average) 50 percent of our\nstate networks are fully connected, whether the other 50 percent are not.\nOur motivation for this prior is that it often happens in real world processes that the intensity matrices\nare not fully connected. Let us remark, however, that we only accept the Erd\u00f6s-R\u00e9nyi sample if the\ncorresponding graph is connected \u2014 that is, if the system cannot get stuck into a single state. Both\nthese distributions implicitly define prates(F|A, \u03c1f), for Fij = aijfij.\n15\n0\n5\n10\n15\n20\nNumber of jumps\n0\n20000\n40000\n60000\n80000\nFrequency\n(a) 2D\n0\n10\n20\n30\nNumber of jumps\n0\n10000\n20000\n30000\n40000\nFrequency\n(b) 3D\n0\n10\n20\n30\nNumber of jumps\n0\n5000\n10000\n15000\n20000\n25000\nFrequency\n(c) 4D\n0\n10\n20\n30\nNumber of jumps\n0\n5000\n10000\n15000\n20000\nFrequency\n(d) 5D\n0\n10\n20\n30\nNumber of jumps\n0\n5000\n10000\n15000\nFrequency\n(e) 6D\nFigure 5: Distributions of the number of jumps per trajectory. We used the same distributions as the\ntraining set and sampled up to time 10. The figures are based on 1000 processes with 300 paths per\nprocess.\nRemark on generalization beyond prior rate distribution. We remark that while all entries of the\nintensity matrix seen during training lie on the interval [0, 1], the model can still predict intensities\noutside this interval. We empirically demonstrated that this in indeed the case on the widely different\ntarget sets of the experimental section, in the main text. The reason behind this is that we normalize\nthe maximum time among all input paths to be 1, and rescale the predicted intensities accordingly.\nUltimately, what matters is the difference among the rates (and therefore among the observation\ntimes) within the target time series. Our approach for sampling intensity matrices resulted in a vast\nvariety of different processes.\nThe distribution of the number of jumps per trajectory is shown in figure 5 and that of relaxation\ntimes is shown in figure 6.\nDistribution over initial conditions. We choose half of our initial distributions in our synthetic\nensemble to be the stationary distribution of the MJP p\u2217\nMJP. The motivation for this is that it often\nhappens that real life experiments produce very long observations of a system in equilibrium. The\nsecond half of our initial distributions \u03c00 are randomly sampled from a Dirichlet distribution Dir(\u03c10),\nwhere we heuristically choose \u03c10 = 50. In equations, we write\np(\u03c00) = 1\n2p\u2217\nMJP(F) + 1\n2Dir(\u03c10 = 50).\n(16)\nDistribution over observation grids. In practice, the exact jump (i.e. transition) times are not known.\nWe therefore first generate observations of the state of the system on a regular grid with a maximum\nof L = 100 points. We then randomly mask out some observations from this fixed regular grid, in\norder to make the model grid independent. Half of our (subsampled) observation grids are chosen to\nbe regular, i.e. they are strided with strides \u2208{1, 2, 3, 4}. The other half are chosen to be irregular,\nthrough a Bernoulli filter (or mask) with \u03c1survival \u2208{1/4, 1/2} applied to the base (L = 100) grid.\nDistribution over noise process. Because real world data is often noisy we also add noise to the\nlabels. If a state observation is selected to be mislabeled, the new label is randomly chosen from a\nuniform distribution over all states. We investigate two different configurations in this project, one\nwith 1% label noise (\u03c1x = 0.01) and one with 10% label noise (\u03c1x = 0.1).\nMJP simulation. We sample the jumps between different states with an algorithm due to (Gillespie,\n1977) (see A.5). We sample jumps between times 0 and 10 because almost all of our processes are in\nequilibrium by then (see figure 6).\n16\n0\n10\n20\n30\n40\n50\nRelaxation Time\n0\n50\n100\n150\n200\nFrequency\n(a) 2D - OP: 0% NCP: 2.6%\n0\n10\n20\n30\n40\n50\nRelaxation Time\n0\n50\n100\n150\n200\n250\nFrequency\n(b) 3D - OP: 13.9% NCP: 4.1%\n0\n10\n20\n30\n40\n50\nRelaxation Time\n0\n200\n400\n600\nFrequency\n(c) 4D - OP: 19.9% NCP: 4.2%\n0\n10\n20\n30\n40\n50\nRelaxation Time\n0\n100\n200\n300\n400\n500\nFrequency\n(d) 5D - OP: 19.3% NCP: 4.5%\n0\n10\n20\n30\n40\n50\nRelaxation Time\n0\n100\n200\n300\nFrequency\n(e) 6D - OP: 18.8% NCP: 4.8%\nFigure 6: Distributions of the relaxation times. We also report the percentage of processes that\nconverge into an oscillating distribution (OP) and the percentage of processes that have a relaxation\ntime which is larger than the maximum sampling time (NCP) of our training data (given by tend = 10).\nThe figures are based on 1000 processes.\nTraining Dataset Size The synthetic dataset on which our models were trained consists of 25k\nsix-state processes, and 5k processes of 2-5 states, resulting in a total size of 45k processes. For each\nof these processes we sampled 300 paths.\nDistribution over the number of MJP paths p(K). While we generate the data with 300 paths per\nprocess, we want to ensure that the model is able to handle datasets with less than 300 paths. For this\nreason, we shuffle the training data at the beginning of every epoch and distribute it into batches with\npath counts 1, 11, 21, . . . , 300. We found that such a static selection of the path counts is better than\na random selection, because a random selection leads to oscillating loss functions (because the model\nobviously gets a larger loss for samples with fewer paths), and thus training instabilities. Since we do\nnot always select all paths per process but instead select a random subset of them, the data that the\nmodel processes changes during every epoch, which helps in reducing overfitting.\nC\nHow to use the Model: Inputs, Outputs and Rescalings\nIn this section, we give details about the inputs to and outputs of our pretrained recognition model.\nWe also comment on the internal rescalings done by the model, in order to be able to infer MJPs from\ntime series with observation times of any scale.\nC.1\nInput\nThe model takes as input three parameters:\n1. The observation grids (shape: [num_paths K, grid_size L]): The observation\ntimes {\u03c4k1, . . . , \u03c4kl}K\nk=1, padded to the maximum length L.\n2. The observation values (shape: [num_paths K, grid_size L]): The noisy observa-\ntion values {x\u2032\nk1, . . . , x\u2032\nkl}K\nk=1 padded to the maximum length L. Note that these values are\nintegers lying on the discrete set {0, 1, . . . , C \u22121}.\n3. The dimension c: The (a priori known) dimension of the process as an integer between\n2 and C. If this dimension is unknown, the model returns a C \u00d7 C rate matrix whose\nrank might (approximately) be smaller than C, which indicates a hidden state-space of size\nsmaller than C.\n17\nWe recommend users to use the model only within its training range. That is, with up to a maximum\nof K = 300 paths, and grids up to a maximum of L = 100 points.\nC.2\nInternal Rescaling\nInternally, the model does the following:\n1. It computes the maximum observation time:\n\u03c4max = max{\u03c4k1, . . . , \u03c4kl}K\nk=1.\n(17)\n2. It normalizes the observation times between 0 and 1:\n{\u03c4k1, . . . , \u03c4kl}K\nk=1 \u2190{\u03c4k1, . . . , \u03c4kl}K\nk=1/\u03c4max.\n(18)\n3. It computes the inter-event times \u2206\u03c4ki = \u03c4k,i+1 \u2212\u03c4ki, for k = 1, . . . , K.\n4. It transforms the observation values to one-hot-encodings.\n5. It predicts the (normalized) off-diagonal elements of the intensity matrix and variance\nmatrix as well as the initial distribution (here we are working with the maximum supported\ndimension, that is C).\n6. It rescales back the estimates to the original time scale:\nintensity matrix \u02c6F \u2190intensity matrix \u02c6F/\u03c4max,\n(19)\nVar \u02c6F \u2190Var \u02c6F/\u03c4max.\n(20)\nNote that, as we empirically demonstrated in the paper, this rescaling procedure allows us to work\nwith real-world MJPs of arbitrary time scales. For example, the time scales for the switching ion\nchannel dataset were more than 500 times smaller than the time scales in our training dataset.\nC.3\nSupport for varying State Space Sizes\nWe now elaborate on how the model can deal with processes whose state spaces have sizes c < C.\nWe arranged all the target rate matrices F within our training dataset, for MJPs with state spaces of\nsize c < C, to be the leftmost block diagonal c \u00d7 c matrix within a C \u00d7 C matrix of zeros, so that the\nredundant matrix elements are always zero. As can be read from equation (6) of the main text, we\ntrain FIM to predict zeros for those redundant matrix elements.\nIn practice, however, our trained FIM does not exactly predict zeros for those redundant matrix\nelements. In our experiments, the user knows a priori the number of states c of the hidden process, so\nwe explicitly set the redundant matrix elements to zero, and only then compute the corrected diagonal\n(i.e. the normalization) of the output rate matrix.\nWe afterwards select the c \u00d7 c entries of the predicted intensity matrix and variance matrix as well as\nthe first c entries of the predicted initial distribution.\nWe refer the reader to our library for additional details.\nC.4\nOutput\nThe output of the model consists of three parameters:\n1. The intensity matrix \u02c6F (shape: [c, c]).\n2. The variance matrix Var \u02c6F (shape: [c,c]).\n3. The initial distribution \u03c00 (shape [c]).\n18\nD\nModel Architecture and Experimental Setup\nIn this section we provide more details about the architecture of our models and the hyperparameters.\nD.1\nModel Architecture\nPath encoder \u03c81. We evaluated two different approaches for the path encoder \u03c81. The first approach\nutilizes a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) as \u03c81, while the second approach\nemploys a transformer (Vaswani et al., 2017) for \u03c81. The time series embeddings are denoted by\nhk\u03b8 (see Equation 4). The input to the encoder \u03c81 is (x\u2032\nk1, \u03c4 k1, . . . , x\u2032\nkl, \u03c4 kl), where \u03c4 kl = [\u03c4kl, \u03b4kl],\n\u03b4kl = \u03c4kl \u2212\u03c4(k\u22121)l, and xkl \u2208{0, 1}C is the one-hot encoding of the system\u2019s state.\nPath attention network \u21261. We tested two approaches. The first approach uses classical self-\nattention Vaswani et al. (2017) and selects the last embedding. For the second approach we used\nan approach we denote as learnable query attention which is equivalent to classical multi-head\nattention with the exception that we do not compute the query based on the input, but instead make it\na learnable parameter, i.e.,\nMultiHead(Q, K, V ) = Concat(head1, . . . , headh),\n(21)\nheadi = Attention(Qi, H1:KW K\ni , H1:KW V\ni ),\n(22)\nwhere H1:K \u2208RK\u00d7dmodel denotes a concatenation of h1, . . . , hK, W K\ni , W V\ni\n\u2208Rdmodel\u00d7dk and\nQi \u2208Rq\u00d7dk is the learnable query matrix. The output dimension of the learnable query attention is\ntherefore independent of the number of input tokens.\nD.2\nExperimental Setup\nHyperparameter tuning: Hyperparameters were tuned using a grid search method. The optimizer\nutilized was AdamW (Loshchilov and Hutter, 2017), with a learning rate and weight decay both set\nat 1e\u22124. A batch size of 128 was used. During the grid search, we experimented with the hidden size\nof the path encoder ([64, 128, 256, 512]), the hidden size of the path attention network ([128, 256]),\nand various MLP architectures for \u03d51, \u03d52, and \u03d53 ([[32, 32], [128, 128]]).\nTraining procedure: All models were trained on two A100 80Gb GPUs for approximately 500\nepochs or approximately 2.5 days on average per model. Early stopping was employed as the stopping\ncriterion. The models were trained by maximizing the likelihood.\nFinal model parameters: The final models (FIM-MJP 1% Noise and FIM-MJP 10% Noise) have\nthe following hyperparameters: Path encoder - hidden_size(\u03c81) = 256 (the final models used a\nBiLSTM); Path attention network - \u21261: q = 16, dk = 128 (the final models used the learnable query\napproach); \u03d51, \u03d52, \u03d53 = [128, 128].\nPretrained models: Our pretrained models are also available online5.\nE\nAblation Studies\nIn this section, we study the performance of the models with different architectures. Additionally, we\nstudy the behavior of the performance of the models with respect to varying numbers of states and\nvarying number of paths.\nE.1\nGeneral Remarks about the Error Bars and Context Number\nIf the evaluation set is larger than the optimal context number c(Kmax, lmax), we split the evaluation\nset into batches and give these to the model independently (because the model does not work well to\ngive the model more paths than during training, see table 8). Afterwards, we compute the mean of the\npredictions among the batches and report the mean RMSE of the intensity entries (if the ground-truth\nis available). This makes it easier to compare our model against previous works which have also used\nthe full dataset to make predictions. Interestingly, we find that the RMSE of this averaged prediction\n5https://github.com/cvejoski/OpenFIM\n19\nis often significantly better than the mean RMSE among the batches. For example for the DFR dataset\nthe RMSE of the averaged prediction is 0.0617, while the average RMSE of the batches is 0.122.\nIf the dataset has been split into multiple batches, we report the RMSE together with the standard\ndeviation of the RMSE among the batches. The reported confidence is the mean predicted variance\nof the model (recall that we are using Gaussian log-likelihood during training).\nE.2\nPerformance of the Model by varying its Architecture\nThe ablation study presented in Table 5 evaluates the impact of different model features on the\nperformance by comparing various combinations of architectures and attention mechanisms with\nvarying numbers of paths, and their corresponding RMSE values. The study examines models using\na BiLSTM or Transformer, with and without self-attention and learnable query attention, across 1,\n100, and 300 paths. The results indicate that increasing the number of paths consistently reduces\nRMSE (see section E.4 for more details), demonstrating the benefit of considering more paths\nduring training. Specifically, using a BiLSTM with learnable query attention achieves an RMSE\nof 0.193 \u00b1 0.031 with a single path, significantly improving to 0.048 \u00b1 0.011 with 100 paths, and\nfurther to 0.0457 \u00b1 0.0 with 300 paths. Similarly, a Transformer with learnable query attention shows\nan RMSE of 0.196 \u00b1 0.031 for a single path, 0.049 \u00b1 0.011 for 100 paths, and 0.0458 \u00b1 0.0 for 300\npaths. The inclusion of self-attention in the Transformer models slightly improves performance, with\nthe best RMSE of 0.0459 \u00b1 0.0 achieved when both self-attention and learnable query attention are\nused with 300 paths. In this case since many of the processes contain one path it is beneficial to use\nthe learnable query attention over the standard self-attention mechanism.\n# Paths\nBiLSTM\nTransformer\nSelf Attention\nLearnable Query Attention\nRMSE\n1\n\u2713\n\u2713\n0.193 \u00b1 0.031\n1\n\u2713\n\u2713\n0.196 \u00b1 0.031\n1\n\u2713\n\u2713\n0.197 \u00b1 0.015\n100\n\u2713\n\u2713\n0.048 \u00b1 0.011\n100\n\u2713\n\u2713\n0.049 \u00b1 0.011\n100\n\u2713\n\u2713\n0.054 \u00b1 0.012\n300\n\u2713\n\u2713\n0.0457 \u00b1 0.0\n300\n\u2713\n\u2713\n0.0458 \u00b1 0.0\n300\n\u2713\n\u2713\n0.0459 \u00b1 0.0\nTable 5: Comparison of model features with different number of paths and their RMSE. This table\npresents an ablation study comparing the performance of models using BiLSTM and Transformer\narchitectures, with and without self-attention and learnable query attention, across different numbers\nof paths (1, 100, and 300). The performance is measured by the Root Mean Square Error (RMSE),\nwith lower values indicating better model accuracy. The study highlights that both the architectural\nchoices and the number of paths significantly impact model performance, with the best results\nachieved using a combination of attention mechanisms and a higher number of paths.\nFigure 7 presents a series of line plots illustrating the impact of different hyperparameter settings on\nthe RMSE of the model. The first subplot shows the RMSE as a function of the hidden size of the\n\u03c81 path encoder, with hidden sizes 64, 128, 256, and 512. The RMSE increases as the hidden size\nincreases, with the lowest RMSE observed at a hidden size of 256. The second subplot displays the\nRMSE as a function of the architecture size of \u03d51, comparing two architectures: [2x32] and [2x128].\nThe RMSE decreases as the architecture size increases, indicating better performance with a larger\narchitecture size for \u03d51. The third subplot examines the RMSE based on the architecture size of\n\u03d52, with two architectures tested: [2x32] and [2x128]. There is no significant difference in RMSE\nbetween the two sizes, suggesting that the choice of architecture size for \u03d52 does not markedly affect\nmodel performance. The fourth subplot investigates the RMSE as a function of the hidden size of the\n\u21261 component, with hidden sizes 128 and 256 tested, and results shown for different \u03c81 hidden sizes\n(64, 128, 256, and 512). The RMSE remains relatively stable across different hidden sizes of \u21261,\nwith slight variations observed depending on the hidden size of \u03c81. Overall, the plots highlight that\nsome components, such as \u03c81 and \u03d51, are more sensitive to changes in hyperparameters, emphasizing\nthe importance of selecting appropriate hyperparameters to optimize model performance.\n20\n64 128\n256\n512\n\u03c81\u2019s Hidden Size\n0.046\n0.048\n0.050\n0.052\n0.054\n0.056\nRMSE\n2x128\n2x32\n\u03c61 Architecture Size\n2x128\n2x32\n\u03c62 Architecture Size\n128\n256\n\u21261\u2019s Hidden Size\nsize(\u03c81)=64\nsize(\u03c81)=128\nsize(\u03c81)=256\nsize(\u03c81)=512\nFigure 7: Impact of Hyperparameters on RMSE. The figure shows four line plots illustrating the effect\nof hyperparameters on model RMSE. The first plot shows RMSE increases with larger \u03c81 hidden\nsizes, being lowest at 256. The second plot indicates lower RMSE with a larger \u03d51 architecture size\n([2x128]). The third plot shows minimal RMSE impact from \u03d52 architecture size. The fourth plot\nshows RMSE stability across different \u21261 hidden sizes, with slight variations based on \u03c81. This\nhighlights the importance of tuning \u03c81 and \u03d51 for optimal performance.\n1% Noise Data\n10% Noise Data\nFIM-MJP 1%\n0.046\n0.199\nFIM-MJP 10%\n0.096\n0.087\nTable 6: Performance of FIM-MJP 1% and FIM-MJP 10% on synthetic datasets with different noise\nlevels. We use a weighted average among the datasets with different numbers of states to compute a\nfinal RMSE.\nTable 6 compares the performance of two models, FIM-MJP 1% and FIM-MJP 10%, on synthetic\ndatasets with noise levels of 1% and 10%, measured in terms of RMSE. For datasets with 1% noise,\nthe FIM-MJP 1% model achieves an RMSE of 0.046, indicating good performance, but its RMSE\nincreases significantly to 0.199 on 10% noise data, showing decreased performance with higher\nnoise. Conversely, the FIM-MJP 10% model, trained with 10% noise data, has an RMSE of 0.096 on\n1% noise data, higher than the FIM-MJP 1% model on the same data, but achieves a lower RMSE\nof 0.087 on 10% noise data, demonstrating better performance under high noise conditions. This\nindicates that the FIM-MJP 10% model is more robust to noise, maintaining consistent performance\nacross varying noise levels, while the FIM-MJP 1% model excels in low noise environments but\nstruggles with higher noise. The results highlight the importance of training with appropriate noise\nlevels to ensure robust model performance across different noise conditions.\nE.3\nPerformance of the Model with varying Number of States\nWe compare the performance of our models on processes with varying number of states. Note that\nour model always outputs a 6 \u00d7 6 dimensional intensity matrix. However, in these experiments we\nonly use the rows and columns that correspond to the lower-dimensional process. This improves the\ncomparability between different dimensions as lower-dimensional processes obviously have many\nzero-entries in their intensity matrix which would make it easier for the model to achieve a good\nRMSE score.\nIt can be seen in Table 7 that the multi-state-model performs well among all different dimensions. As\nexpected, lower-dimensional processes seem to be easier for the model. Additionally, Table 7 shows\nthe performance of a model which has only been trained on six-state processes. The performance of\nthis native six-state-model for six number of states is very similar to the multi-state-model which\nshows that having more states during training does not reduce the single-state performance. As\nexpected, the performance of the six-state model on processes with lower numbers of states is\nsignificantly worse, but still better than random.\nE.4\nPerformance of the Model with varying Number of Paths during Evaluation\nOne of the advantages of our model architecture is that it can handle arbitrary number of paths.\nWe therefore use our model that was trained on at maximum 300 paths and assess its performance\n21\n# States\nMulti-State RMSE\nMulti-State Confidence\n6-State RMSE\n6-State Confidence\n2\n0.026\n0.028\n0.129\n0.056\n3\n0.037\n0.030\n0.113\n0.049\n4\n0.046\n0.037\n0.087\n0.046\n5\n0.054\n0.040\n0.066\n0.041\n6\n0.059\n0.044\n0.059\n0.044\nTable 7: Performance of the multi-state and six-state models (which has only been trained on\nprocesses with six states) on synthetic test sets with varying number of states\nwith varying number of paths during evaluation. The results are presented in Table 8. When being\ninside the training range, the performance and the confidence of the model goes down as the model\nis given fewer paths per evaluation, which is to be expected. Interestingly, the performance of the\nlearnable-query (LQ) model peaks at 500 paths instead of at 300, which was the maximum training\nrange. One possible explanation for this might be that we are still close enough to the training range\nwhile being able to use the full data (note that the dataset contains 5000 paths which is not divisible\nby 300, so we have to leave some of the data out). Going too far beyond the training range does\nhowever not work well, for example processing all 5000 paths at once leads to very poor performance,\nalthough the model (falsely) become very confident. Another insight from this experiment is that the\nself-attention (SA) architecture behaves significantly worse when going beyond the maximum number\nof paths that was seen during training. This is another reason why we chose the (LQ) architecture\nover the (SA) architecture for the final version of our model.\n#Paths during Evaluation\nRMSE (LQ)\nConfidence (LQ)\nRMSE (SA)\nConfidence (SA)\n1\n0.548 \u00b1 0.067\n0.838\n0.579 \u00b1 0.074\n0.898\n30\n0.074 \u00b1 0.081\n0.263\n0.075 \u00b1 0.070\n0.264\n100\n0.061 \u00b1 0.039\n0.143\n0.060 \u00b1 0.035\n0.142\n300\n0.056 \u00b1 0.023\n0.089\n0.059 \u00b1 0.024\n0.085\n500\n0.053 \u00b1 0.014\n0.069\n0.074 \u00b1 0.021\n0.061\n1000\n0.067 \u00b1 0.012\n0.037\n0.229 \u00b1 0.025\n0.029\n5000\n0.818 \u00b1 0.000\n0.000\n2.135 \u00b1 0.000\n0.000\nTable 8: Performance of FIM-MJP 1% given varying number of paths during the evaluation on\nthe DFR dataset with regular grid. (LQ) denotes learnable-query-attention (see section D.1), (SA)\ndenotes self-attention.\nF\nAdditional Results\nThis section contains more of our results which did not fit into the main text. We begin this section by\nproviding more details on the Hellinger distance which we used as a metric to assess the performance\nof our models. Afterwards, we provide more results and background on the ADP, ion channel and\nDFR datasets. Additionally, we introduce two two-state MJPs, given by the protein folding datasets\n(F.5) and the two-mode switching system (F.6), which we use to evaluate our models and to compare\nit against previous works.\nF.1\nHellinger Distance\nReal-world empirical datasets of MJPs provide no knowledge of a ground truth solution. For this\nreason we present a new metric that can be used to compare the performance of the inference of\nvarious models based on only the empirical data. Our metric of choice is the Hellinger distance which\nis a measure of the dissimilarity between two probability distributions. Given two discrete probability\ndistributions P = (p1, . . . , pk) and Q = (q1, . . . , qk), the Hellinger distance is defined as\nH(P, Q) =\n1\n\u221a\n2\nv\nu\nu\nt\nk\nX\ni=1\n(\u221api \u2212\u221aqi)2 .\n(23)\n22\nFor our empirical cases, the class probabilities of the discrete probability distributions are not\nknown explicitly. We therefore approximate them by using the empirical distributions, given by the\n(normalized) histograms of the observed states at the observation grids.\nWe test this approach on the DFR process by first sampling a specified number of paths for the\npotential V = 1 using the Gillespie algorithm, which we then consider as the target distribution.\nCounting states among the different paths then yields histograms of the states for every time step. We\nrepeat the same procedure for different choices of V . Afterwards we compute the Hellinger distance\nbetween the newly sampled histogram and the target distribution for every time step. Figure 8 shows\nthat the distance indeed goes down as we approach the target distribution, which provides heuristic\nevidence of the effectiveness of our metric. The Hellinger distances for various models are shown in\nTable 2 and Table 9.\nAs one can see, FIM-MJP performs as well (and sometimes better) as the current state-of-the-art\nmodel NeuralMJP.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nV\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nAverage Hellinger Distance\nMaster eq.\n100 Paths\n1000 Paths\nFigure 8: Time-Average Hellinger distance for varying potentials on the DFR. The plot shows the\nHellinger distance to a target dataset that was sampled from a DFR with V = 1 on a grid of 50 points\nbetween 0 and 2.5. The means and standard deviations were computed by sampling 100 histograms\nper dataset. As expected, the distance decreases as the voltage gets closer to the voltage of the target\ndataset. We also remark that the scale of the distances gets smaller as one takes more paths into\naccount and converge to the distance of the solutions of the master equation.\nDataset\nNeuralMJP\nFIM-MJP 1% Noise\nFIM-MJP 10% Noise\nADP\n1.38 \u00b1 0.52\n1.39 \u00b1 0.47\n1.35 \u00b1 0.42\nIon Channel\n0.48 \u00b1 0.02\n0.41 \u00b1 0.02\n1.78 \u00b1 0.03\nProtein Folding\n0.015 \u00b1 0.015\n0.014 \u00b1 0.014\n0.024 \u00b1 0.026\nDFR\n0.30 \u00b1 0.06\n0.27 \u00b1 0.06\n0.28 \u00b1 0.06\nTable 9: Comparison of the time-average Hellinger distances for various models. We used the same\nlabels as NeuralMJP to make the results comparable. The errors are the standard deviation among\n100 sampled histograms. The target datasets contain 200 paths for ADP, 1500 paths for Ion Channel,\n2000 paths for Protein Folding and 1000 paths for the DFR. The distances are reported in a scale\n1e-2. We remark that the high variance of the distances on the Protein Folding dataset is caused by\nthe models performing basically perfect predictions, which causes the oscillations to be noise. We\nverified this claim by confirming that the distances of the predictions of the models are as small as\nthe distance of the target dataset to additional simulated data.\nF.2\nAlanine Dipeptide\nWe use the dataset of Husic et al. (2020), which models the conformal dynamics of ADP, for evaluating\nour model. This dataset was provided to us via private communication. The dataset consists of 9800\npaths on grids of size 100 and has the sines and cosines of the Ramachandran angles as features:\n23\nsin \u03c8, cos \u03c8, sin \u03d5 and cos \u03d5. We use KMeans to classify the data into states. The reason why we\ndid not choose GMM as for the other datasets is that we could initialize KMeans with hand-selected\nvalues to try to achieve a similar classification like those learned by NeuralMJP (Seifner and S\u00e1nchez,\n2023), see Figure 9. Still, the classification is very different and thus also leads to very different\nresults (see Table 10). We use 9600 paths to evaluate our models. Our results are shown in Table 10.\nTable 11 reports the stationary distributions and compares them to previous works, while Table 12\nreports the ordered time scales.\nFigure 9: Comparison of the classifications between KMeans (left) and NeuralMJP (right).\nModel\nIntensity Matrix\nNeuralMJP\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u221261.32\n53.15\n0.19\n7.89\n0.06\n0.02\n47.29\n\u221259.37\n0.05\n11.97\n0.04\n0.01\n0.28\n0.13\n\u221217.28\n16.81\n0.02\n0.04\n35.48\n26.94\n40.93\n\u2212103.61\n0.25\n0.01\n0.16\n0.22\n0.31\n0.2\n\u22123.86\n2.96\n1.13\n1.73\n0.46\n0.66\n18.78\n\u221222.76\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nFIM-MJP 1% Noise (NeuralMJP Labels)\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u221259.35 \u00b1 2.11\n48.72 \u00b1 1.90\n0.33 \u00b1 0.08\n10.14 \u00b1 1.47\n0.09 \u00b1 0.08\n0.07 \u00b1 0.07\n50.54 \u00b1 3.25\n\u221257.62 \u00b1 2.99\n0.44 \u00b1 0.04\n6.44 \u00b1 1.18\n0.09 \u00b1 0.09\n0.10 \u00b1 0.10\n0.40 \u00b1 0.08\n0.50 \u00b1 0.10\n\u221214.29 \u00b1 1.57\n13.16 \u00b1 1.31\n0.17 \u00b1 0.17\n0.07 \u00b1 0.06\n38.31 \u00b1 4.63\n33.71 \u00b1 4.97\n49.14 \u00b1 4.80\n\u2212121.66 \u00b1 7.36\n0.21 \u00b1 0.19\n0.30 \u00b1 0.32\n0.25 \u00b1 0.27\n0.43 \u00b1 0.60\n0.20 \u00b1 0.24\n0.30 \u00b1 0.33\n\u22122.40 \u00b1 3.06\n1.23 \u00b1 1.69\n0.44 \u00b1 0.45\n1.12 \u00b1 1.55\n0.48 \u00b1 0.42\n0.68 \u00b1 0.99\n4.79 \u00b1 5.91\n\u22127.52 \u00b1 8.64\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nFIM-MJP 10% Noise (NeuralMJP Labels)\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u221249.35 \u00b1 4.58\n40.51 \u00b1 3.82\n0.3 \u00b1 0.1\n7.96 \u00b1 1.69\n0.35 \u00b1 0.15\n0.22 \u00b1 0.11\n39.99 \u00b1 6.65\n\u221246.82 \u00b1 6.37\n0.3 \u00b1 0.1\n5.99 \u00b1 1.14\n0.27 \u00b1 0.07\n0.27 \u00b1 0.08\n0.27 \u00b1 0.04\n0.44 \u00b1 0.1\n\u221213.05 \u00b1 1.66\n11.35 \u00b1 1.81\n0.32 \u00b1 0.07\n0.68 \u00b1 0.27\n39.18 \u00b1 5.42\n28.24 \u00b1 4.14\n58.86 \u00b1 8.72\n\u2212129.02 \u00b1 10.51\n1.1 \u00b1 0.19\n1.64 \u00b1 0.58\n9.61 \u00b1 7.02\n9.32 \u00b1 6.83\n5.53 \u00b1 3.97\n4.36 \u00b1 3.09\n\u221243.11 \u00b1 29.51\n14.3 \u00b1 9.01\n2.49 \u00b1 1.12\n5.8 \u00b1 2.25\n8.82 \u00b1 4.95\n6.72 \u00b1 2.32\n11.5 \u00b1 5.67\n\u221235.32 \u00b1 5.99\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nFIM-MJP 1% Noise (KMeans Labels)\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u2212175.42 \u00b1 8.87\n172.65 \u00b1 8.73\n1.84 \u00b1 0.69\n0.48 \u00b1 0.12\n0.22 \u00b1 0.18\n0.23 \u00b1 0.24\n157.16 \u00b1 13.99\n\u2212165.37 \u00b1 13.64\n6.67 \u00b1 1.78\n1.17 \u00b1 0.24\n0.22 \u00b1 0.16\n0.14 \u00b1 0.14\n22.26 \u00b1 3.88\n9.84 \u00b1 3.10\n\u2212375.78 \u00b1 20.96\n342.13 \u00b1 19.80\n0.71 \u00b1 0.67\n0.84 \u00b1 0.65\n0.93 \u00b1 0.15\n1.37 \u00b1 0.16\n305.86 \u00b1 20.47\n\u2212308.48 \u00b1 20.30\n0.25 \u00b1 0.19\n0.07 \u00b1 0.09\n0.81 \u00b1 1.34\n0.35 \u00b1 0.39\n0.28 \u00b1 0.29\n0.25 \u00b1 0.27\n\u22122.30 \u00b1 2.52\n0.61 \u00b1 0.82\n0.28 \u00b1 0.33\n0.89 \u00b1 1.14\n0.28 \u00b1 0.38\n0.18 \u00b1 0.23\n4.81 \u00b1 7.13\n\u22126.44 \u00b1 9.08\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nFIM-MJP 10% Noise (KMeans Labels)\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u221294.75 \u00b1 15.46\n91.38 \u00b1 16.21\n1.91 \u00b1 0.76\n0.84 \u00b1 0.15\n0.32 \u00b1 0.09\n0.29 \u00b1 0.10\n184.85 \u00b1 20.63\n\u2212190.00 \u00b1 19.41\n1.98 \u00b1 0.49\n0.49 \u00b1 0.23\n0.84 \u00b1 0.32\n1.83 \u00b1 0.93\n5.93 \u00b1 1.57\n13.71 \u00b1 2.48\n\u2212266.49 \u00b1 18.43\n241.54 \u00b1 17.99\n0.85 \u00b1 0.18\n4.48 \u00b1 0.52\n1.44 \u00b1 0.74\n0.91 \u00b1 0.35\n188.88 \u00b1 31.10\n\u2212193.76 \u00b1 29.77\n1.29 \u00b1 0.30\n1.22 \u00b1 0.31\n3.45 \u00b1 1.82\n17.28 \u00b1 11.78\n7.08 \u00b1 4.79\n3.01 \u00b1 2.02\n\u221242.3 \u00b1 26.94\n11.48 \u00b1 6.83\n2.43 \u00b1 0.89\n7.14 \u00b1 3.09\n6.11 \u00b1 2.37\n6.62 \u00b1 2.24\n16.39 \u00b1 7.84\n\u221238.69 \u00b1 5.39\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nTable 10: Comparison of intensity matrices for the ADP dataset. The time scales are in nanoseconds.\n24\nPROBABILITY PER STATE\nI\nII\nIII\nIV\nV\nVI\nVAMPNETS\n0.30\n0.24\n0.20\n0.15\n0.11\n0.01\nNEURALMJP\n0.30\n0.31\n0.23\n0.10\n0.05\n0.01\nFIM-MJP 1% NOISE\n0.28\n0.28\n0.24\n0.07\n0.10\n0.03\nFIM-MJP 10% NOISE\n0.30\n0.30\n0.31\n0.06\n0.01\n0.02\nTable 11: Comparison of the stationary distribution on the ADP dataset of FIM-MJP, VAMPnets\nMardt et al. (2017) and NeuralMJP (Seifner and S\u00e1nchez, 2023). The states are ordered such that the\nprotein conformations associated to a given state are comparable in both models. We use the labels of\nNeuralMJP to evaluate FIM-MJP.\nRELAXATION TIME SCALES (IN ns)\nVAMPNETS\n0.008\n0.009\n0.055\n0.065\n1.920\nGMVAE\n0.003\n0.003\n0.033\n0.065\n1.430\nMSM\n-\n-\n-\n-\n1.490\nNEURALMJP\n0.009\n0.009\n0.043\n0.069\n0.774\nFIM-MJP 1% NOISE (NEURALMJP LABELS)\n0.008\n0.009\n0.079\n0.118\n0.611\nFIM-MJP 10% NOISE (NEURALMJP LABELS)\n0.007\n0.011\n0.019\n0.038\n0.091\nFIM-MJP 1% NOISE (KMEANS LABELS)\n0.001\n0.003\n0.046\n0.142\n0.455\nFIM-MJP 10% NOISE (KMEANS LABELS)\n0.002\n0.004\n0.018\n0.034\n0.070\nTable 12: Relaxation time scales for six-state Markov models of ADP. The time scales are ordered\nby size and reported in nanoseconds. VAMPnet results are taken from Mardt et al. (2017), GMVAE\nfrom Varolg\u00fcnes et al. (2019), MSM from Trendelkamp-Schroer and No\u00e9 (2014) and NeuralMJP\nfrom (Seifner and S\u00e1nchez, 2023).\nF.3\nIon Channel\nWe consider the 1s observation window that has been used in (K\u00f6hs et al., 2021) and (Seifner and\nS\u00e1nchez, 2023) and split it into 50 paths of 100 points. This dataset was provided to us via private\ncommunication. We then apply a Gaussian Mixture Model (GMM) to classify the experimental data\ninto discrete states as shown in figure 10.\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nTime [s]\n0\n2\n4\n6\nCurrent [pA]\nObservations\nGMM Classi\ufb01cation\nFigure 10: Classification of the ion channel dataset into states.\nThe predictions of our models and NeuralMJP are shown in table 13. Table 14 reports the stationary\ndistributions and Table 15 reports the mean first-passage times.\n25\nModel\nIntensity Matrix\nNeuralMJP\n\uf8ee\n\uf8f0\n\u221257.73\n55.81\n1.93\n102.13\n\u2212306.93\n204.81\n0.70\n26.05\n\u221226.75\n\uf8f9\n\uf8fb\nFIM-MJP 1% Noise (NeuralMJP Labels)\n\uf8ee\n\uf8f0\n\u221264.65\n62.25\n2.40\n110.55\n\u2212334.05\n223.50\n0.78\n31.53\n\u221232.30\n\uf8f9\n\uf8fb\nFIM-MJP 10% Noise (NeuralMJP Labels)\n\uf8ee\n\uf8f0\n\u221292.63\n85.83\n6.79\n49.31\n\u2212141.72\n92.40\n2.86\n32.72\n\u221235.58\n\uf8f9\n\uf8fb\nFIM-MJP 1% Noise (GMM Labels)\n\uf8ee\n\uf8f0\n\u2212116.37\n114.65\n1.73\n271.88\n\u2212716.52\n444.64\n0.56\n49.69\n\u221250.25\n\uf8f9\n\uf8fb\nFIM-MJP 10% Noise (GMM Labels)\n\uf8ee\n\uf8f0\n\u2212104.01\n97.30\n6.71\n82.72\n\u2212215.58\n132.86\n2.89\n40.29\n\u221243.18\n\uf8f9\n\uf8fb\nTable 13: Comparison of intensity matrices for the ion channel dataset. We cannot report error bars\nhere because the dataset is so small that it gets processed in a single batch.\nBOTTOM\nMIDDLE\nTOP\nK\u00d6HS ET AL. (2021)\n0.17961\n0.14987\n0.67052\nNEURALMJP (1 SEC)\n0.17672\n0.09472\n0.72856\nFIM-MJP 1% NOISE (NEURALMJP LABELS)\n0.18224\n0.10156\n0.71621\nFIM-MJP 10% NOISE (NEURALMJP LABELS)\n0.14229\n0.23090\n0.62682\nFIM-MJP 1% NOISE (GMM LABELS)\n0.19330\n0.08124\n0.72546\nFIM-MJP 10% NOISE (GMM LABELS)\n0.17348\n0.19610\n0.63042\nTable 14: Stationary distribution for the switching ion channel process when trained on the one-second\nwindow.\nK\u00d6HS ET AL. (2021)\nNEURALMJP\nFIM-MJP 1% NOISE\n(NEURALMJP LABELS)\n\u03c4ij/s\nBOTTOM\nMIDDLE\nTOP\nBOTTOM\nMIDDLE\nTOP\nBOTTOM\nMIDDLE\nTOP\nBOTTOM\n0.\n0.068\n0.054\n0.\n0.019\n0.031\n0\n0.017\n0.027\nMIDDLE\n0.133\n0.\n0.033\n0.083\n0.\n0.014\n0.068\n0\n0.012\nTOP\n0.181\n0.092\n0.\n0.119\n0.038\n0.\n0.098\n0.031\n0\nFIM-MJP 10% NOISE\nFIM-MJP 1% NOISE\nFIM-MJP 10% NOISE\n(NEURALMJP LABELS)\n(GMM LABELS)\n(GMM LABELS)\n\u03c4ij/s\nBOTTOM\nMIDDLE\nTOP\nBOTTOM\nMIDDLE\nTOP\nBOTTOM\nMIDDLE\nTOP\nBOTTOM\n0\n0.013\n0.026\n0\n0.009\n0.016\n0\n0.011\n0.022\nMIDDLE\n0.063\n0\n0.016\n0.036\n0\n0.007\n0.045\n0\n0.013\nTOP\n0.086\n0.029\n0\n0.055\n0.02\n0\n0.065\n0.024\n0\nTable 15: Mean first-passage times of the predictions of various models on the Switching Ion Channel\ndataset. We compare against (K\u00f6hs et al., 2021) and NeuralMJP (Seifner and S\u00e1nchez, 2023). Entry\nj in row i is mean first-passage time of transition i \u2192j of the corresponding model.\nF.4\nDiscrete Flashing Ratchet\nWe use the same datasets that were used in Seifner and S\u00e1nchez (2023), which contains 5000 paths\non grids of size 50 that lie between times 0 and 2.5. This dataset was provided to us via private\ncommunication. We used 4500 paths to evaluate our model. The predicted intensity matrices for the\nDFR and the ground truth are shown in table 16.\n26\nModel\nIntensity Matrix\nGround Truth\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u22121.97\n0.61\n0.37\n1\n0\n0\n1.65\n\u22123.26\n0.61\n0\n1\n0\n2.72\n1.65\n\u22125.37\n0\n0\n1\n1\n0\n0\n\u22123\n1\n1\n0\n1\n0\n1\n\u22123\n1\n0\n0\n1\n1\n1\n\u22123\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nFIM-MJP 1% Noise\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u22121.88 \u00b1 0.09\n0.52 \u00b1 0.06\n0.31 \u00b1 0.05\n0.99 \u00b1 0.09\n0.03 \u00b1 0.00\n0.03 \u00b1 0.01\n1.62 \u00b1 0.12\n\u22123.34 \u00b1 0.13\n0.57 \u00b1 0.10\n0.06 \u00b1 0.01\n1.04 \u00b1 0.14\n0.05 \u00b1 0.01\n2.73 \u00b1 0.31\n1.66 \u00b1 0.19\n\u22125.60 \u00b1 0.55\n0.12 \u00b1 0.03\n0.10 \u00b1 0.01\n1.00 \u00b1 0.27\n0.97 \u00b1 0.10\n0.05 \u00b1 0.01\n0.04 \u00b1 0.01\n\u22123.02 \u00b1 0.17\n0.99 \u00b1 0.10\n0.97 \u00b1 0.09\n0.05 \u00b1 0.01\n0.98 \u00b1 0.12\n0.05 \u00b1 0.01\n0.95 \u00b1 0.15\n\u22123.05 \u00b1 0.18\n1.01 \u00b1 0.11\n0.07 \u00b1 0.02\n0.05 \u00b1 0.01\n0.96 \u00b1 0.11\n0.94 \u00b1 0.10\n1.03 \u00b1 0.11\n\u22123.05 \u00b1 0.19\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nFIM-MJP 10% Noise\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u22121.61 \u00b1 0.10\n0.46 \u00b1 0.07\n0.23 \u00b1 0.05\n0.88 \u00b1 0.10\n0.02 \u00b1 0.00\n0.02 \u00b1 0.00\n1.42 \u00b1 0.11\n\u22122.78 \u00b1 0.13\n0.48 \u00b1 0.09\n0.04 \u00b1 0.01\n0.81 \u00b1 0.12\n0.04 \u00b1 0.01\n2.68 \u00b1 0.34\n1.47 \u00b1 0.17\n\u22124.93 \u00b1 0.49\n0.06 \u00b1 0.01\n0.06 \u00b1 0.01\n0.65 \u00b1 0.25\n0.87 \u00b1 0.12\n0.03 \u00b1 0.01\n0.03 \u00b1 0.00\n\u22122.53 \u00b1 0.20\n0.80 \u00b1 0.09\n0.80 \u00b1 0.10\n0.04 \u00b1 0.01\n0.84 \u00b1 0.12\n0.03 \u00b1 0.00\n0.84 \u00b1 0.17\n\u22122.61 \u00b1 0.19\n0.87 \u00b1 0.10\n0.05 \u00b1 0.01\n0.03 \u00b1 0.01\n0.78 \u00b1 0.09\n0.86 \u00b1 0.09\n0.93 \u00b1 0.12\n\u22122.65 \u00b1 0.15\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nTable 16: Comparison of intensity matrices for the DFR dataset on the irregular grid.\nF.5\nModeling Protein Folding through Bistable Dynamics\nThe work of Mardt et al. (2017) introduces a simple protein folding model via a 105 step trajectory\nsimulation in a 5-dimensional Brownian dynamics framework, governed by:\ndx(t) = \u2212\u2207U(x(t)) +\n\u221a\n2dW(t)\n,\nwith the potential U(x) being dependent solely on the norm r(x) = |x| as follows:\nU(x) =\n\u001a\u22122.5[r(x) \u22123]2\n, if r(x) < 3\n0.5[r(x) \u22123]3 \u2212[r(x) \u22123]2\n, if r(x) \u22653\nThis model exhibits bistability in the norm r(x), encapsulating two states akin to the folded and\nunfolded conformations of a protein.\nWe use the dataset of Seifner and S\u00e1nchez (2023) and apply a Gaussian-Mixture-Model to classify\nthe dataset into two states. The decision boundary of the classifier seems to be based on the absolute\nabsolute value of the radius, namely the classifier seems to classify all states with a radius smaller\nthan approximately 2 into the lower state (see figure 11).\nSeifner and S\u00e1nchez (2023) generated 1000 trajectories, each with 100 steps after a 1000-step burn-in\nperiod. We used 900 paths to evaluate our model. The results are shown in table 17. Table 18\ncompares the stationary distributions obtained from our models to the ones from Mardt et al. (2017)\nand Seifner and S\u00e1nchez (2023).\n27\nLOW STD\nHIGH STD\nMARDT ET AL. (2017)\n0.73\n0.27\nNEURALMJP\n0.74\n0.26\nFIM-MJP 1% NOISE (NEURALMJP LABELS)\n0.73\n0.27\nFIM-MJP 10% NOISE (NEURALMJP LABELS)\n0.62\n0.38\nFIM-MJP 1% NOISE (GMM LABELS)\n0.70\n0.30\nFIM-MJP 10% NOISE (GMM LABELS)\n0.65\n0.35\nTable 18: Stationary distribution of the model predictions on the protein folding dataset\n0\n100\n200\n300\n400\n500\nTime\n0\n1\n2\n3\n4\n5\n6\nRadius\nFigure 11: Classification of the protein folding dataset into a Low and a High state. The GMM-\nClassifier has learned a decision boundary close to the radius 2.\nLOW STD \u2192HIGH STD\nHIGH STD \u2192LOW STD\nNEURALMJP\n0.028\n0.085\nFIM-MJP 1% NOISE (NEURALMJP LABELS)\n0.019 \u00b1 0.003\n0.054 \u00b1 0.011\nFIM-MJP 10% NOISE (NEURALMJP LABELS)\n0.034 \u00b1 0.005\n0.055 \u00b1 0.008\nFIM-MJP 1% NOISE (GMM LABELS)\n0.054 \u00b1 0.005\n0.154 \u00b1 0.018\nFIM-MJP 10% NOISE (GMM LABELS)\n0.050 \u00b1 0.006\n0.093 \u00b1 0.011\nTable 17: Predicted transition rates on the protein folding dataset\nF.6\nA Toy Two-Mode Switching System\nIn their study, K\u00f6hs et al. (2021) produced a time series derived from the trajectory of a switching\nstochastic differential equation\ndy(t) = \u03b1z(t)(\u03b2z(t) \u2212y(t)) + 0.5dW(t) ,\nwith parameters \u03b11 = \u03b12 = 1.5, \u03b21 = \u22121, and \u03b22 = 1. For a concise overview of the generation\nprocess, the reader is directed to (K\u00f6hs et al., 2021) for comprehensive details. We use the same\ndataset that was generated in (Seifner and S\u00e1nchez, 2023) using the code of (K\u00f6hs et al., 2021) which\ncontains 256 paths of length 67 to evaluate our model. Our results are shown in table 20.\n28\nBOTTOM \u2192TOP\nTOP \u2192BOTTOM\nGROUND TRUTH\n0.2\n0.2\nK\u00d6HS ET AL. (2021)\n0.64\n0.63\nNEURALMJP\n0.19\n0.36\nFIM-MJP 1% NOISE\n0.43\n0.25\nFIM-MJP 10% NOISE\n0.23\n0.15\nTable 19: Two-Mode Switching System transition rates. We do not report error bars here because the\ndataset is so small that it runs in a single batch.\nF.7\nInitial Distributions\nFor completeness, we report in this section the initial distributions predicted by FIM-MJP on various\ndatasets as well as the heuristic initial distribution (which is computed simply by counting the number\nof state occurrences at the first observation). We observe that FIM-MJP typically captures the initial\ndistribution quite well. An exception is the Two-Mode Switching System for which FIM-MJP falsely\npredicts a non-zero probability of the first state. This might happen because we did not capture this\ncase in our training distribution which could be an improvement for future work.\nDATASET\nPREDICTED \u03c00\nHEURISTIC \u03c00\nDFR V = 1\n[0.22, 0.15, 0.11, 0.19, 0.16, 0.17]\n[0.3, 0.14, 0.06, 0.2, 0.17, 0.14]\nIONCH\n[0.14, 0.11, 0.75]\n[0.24, 0.08, 0.68]\nADP\n[0.34, 0.3, 0.23, 0.11, 0.02, 0.0]\n[0.33, 0.29, 0.25, 0.11, 0.02, 0.0]\nTWO-MODE SYSTEM\n[0.32, 0.68]\n[0.0, 1.0]\nPROTEIN FOLDING\n[0.75, 0.25]\n[0.73, 0.27]\nTable 20: Comparison of the predicted initial distribution of the model versus the heuristic initial\ndistribution of various datasets.\n29\nNeurIPS Paper Checklist\n1. Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper\u2019s contributions and scope?\nAnswer: [Yes]\nJustification: Yes, the claims of the abstract and in the introduction are shown in the\ncontributions of sections 3 and 4.\nGuidelines:\n\u2022 The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n\u2022 The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n\u2022 The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n\u2022 It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2. Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes]\nJustification: Yes, Section 6 is devoted to the limitations of our approach.\nGuidelines:\n\u2022 The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022 The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n\u2022 The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n\u2022 The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n\u2022 The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n\u2022 If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n\u2022 While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren\u2019t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3. Theory Assumptions and Proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA]\n30\nJustification: We do not present any theoretical results in this work.\nGuidelines:\n\u2022 The answer NA means that the paper does not include theoretical results.\n\u2022 All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n\u2022 All assumptions should be clearly stated or referenced in the statement of any theorems.\n\u2022 The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n\u2022 Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\n4. Experimental Result Reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes]\nJustification: We share our code and trained models, as well as the synthetic data used to\nevaluate our models6. The synthetic training data is however too large to be published, but\ncan be regenerated with our code. The relevant hyperparameters are stated in Appendix B.\nLastly, we cannot share all evaluation data because we do not own it. We provide references\nin the Acknowledgments to the data owners.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n\u2022 If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n\u2022 Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n\u2022 While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a) If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b) If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c) If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d) We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n6https://github.com/cvejoski/OpenFIM\n31\n5. Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes]\nJustification: Our code and models are openly available. For the availability of the data,\nplease refer to the above point.\nGuidelines:\n\u2022 The answer NA means that paper does not include experiments requiring code.\n\u2022 Please see the NeurIPS code and data submission guidelines (https://nips.cc/\npublic/guides/CodeSubmissionPolicy) for more details.\n\u2022 While we encourage the release of code and data, we understand that this might not be\npossible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n\u2022 The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines (https:\n//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n\u2022 The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n\u2022 The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n\u2022 At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n\u2022 Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6. Experimental Setting/Details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes]\nJustification: All the training details are described in section D.2 in the Appendix.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n\u2022 The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7. Experiment Statistical Significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes]\nJustification: Our results are reported with error bars if possible.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n\u2022 The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n32\n\u2022 The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n\u2022 The assumptions made should be given (e.g., Normally distributed errors).\n\u2022 It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n\u2022 It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n\u2022 For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n\u2022 If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8. Experiments Compute Resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes]\nJustification: The resources that are used for computation are described in section D.2 of the\nAppendix.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022 The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n\u2022 The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n\u2022 The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn\u2019t make it into the paper).\n9. Code Of Ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\nAnswer: [Yes]\nJustification: Our conducted research does not clash with the NeurIPS Code of Ethics.\nGuidelines:\n\u2022 The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\u2022 If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n\u2022 The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10. Broader Impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [NA]\nJustification: Our work is fundamental research that has no impact on society.\nGuidelines:\n\u2022 The answer NA means that there is no societal impact of the work performed.\n\u2022 If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n33\n\u2022 Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n\u2022 The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n\u2022 The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n\u2022 If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11. Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA]\nJustification: Our paper poses no such risks.\nGuidelines:\n\u2022 The answer NA means that the paper poses no such risks.\n\u2022 Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n\u2022 Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n\u2022 We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12. Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer: [Yes]\nJustification: We have credited the owners of the evaluation data and referenced the related\nwork on which this project has been built on.\nGuidelines:\n\u2022 The answer NA means that the paper does not use existing assets.\n\u2022 The authors should cite the original paper that produced the code package or dataset.\n\u2022 The authors should state which version of the asset is used and, if possible, include a\nURL.\n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n\u2022 For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n34\n\u2022 If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n\u2022 For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n\u2022 If this information is not available online, the authors are encouraged to reach out to\nthe asset\u2019s creators.\n13. New Assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes]\nJustification: The new asset of this paper are the code and the models which are well\ndocumented.\nGuidelines:\n\u2022 The answer NA means that the paper does not release new assets.\n\u2022 Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n\u2022 The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n\u2022 At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14. Crowdsourcing and Research with Human Subjects\nQuestion: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA]\nJustification: This paper did not involve crowdsourcing or reasearch with human subjects.\nGuidelines:\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022 Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n\u2022 According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\nSubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer: [NA]\nJustification: This paper did not involve crowdsourcing or reasearch with human subjects.\nGuidelines:\n\u2022 The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022 Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n35\n\u2022 We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n\u2022 For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n36"
    }
]