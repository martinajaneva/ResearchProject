[
    {
        "paper": "Tri-Level Navigator LLM-Empowered Tri-Level Learning for Time Series OOD Generalization",
        "input": "Out-of-Distribution (OOD) generalization in machine learning is a burgeoning area of study. Its primary goal is to enhance the adaptability and resilience of machine learning models when faced with new, unseen, and potentially adversarial data that significantly diverges from their original training datasets. In this paper, we investigate time series OOD generalization via pre-trained Large Language Models (LLMs). We first propose a novel \\textbf{T}ri-level learning framework for \\textbf{T}ime \\textbf{S}eries \\textbf{O}OD generalization, termed TTSO, which considers both sample-level and group-level uncertainties. This formula offers a fresh theoretic perspective for formulating and analyzing OOD generalization problem. In addition, we provide a theoretical analysis to justify this method is well motivated. We then develop a stratified localization algorithm tailored for this tri-level optimization problem, theoretically demonstrating the guaranteed convergence of the proposed algorithm. Our analysis also reveals that the iteration complexity to obtain an $\\epsilon$-stationary point is bounded by O($\\frac{1}{\\epsilon^{2}}$). Extensive experiments on real-world datasets have been conducted to elucidate the effectiveness of the proposed method.\n\nExisting OOD generalization methods mainly focus on sample-level uncertainties or group-level\nuncertainties, often overlooking the interplay between these two aspects. In light of this, we propose\nthe TTSO framework to integrate both sample-level and group-level uncertainties within a unified\ntri-level learning approach, thereby enhancing the model\u2019s robustness and adaptability in facing\ndiverse and unforeseen distribution shifts. In addition, this innovative framework introduces a\nfresh perspective for the development and analysis of the Out-of-Distribution (OOD) generalization\nproblem. Based on this formulation, we develop a stratified localization algorithm for the tri-level\noptimization problem and provide theoretical analysis regarding the iteration complexity of the\nproposed algorithm. Comprehensive studies have been carried out to assess the performance of\nthe proposed algorithm and substantiate the theoretic claims. It is seen that TTSO with LLM can\nconsiderably improves the performance of time series OOD generalization.\n7"
    },
    {
        "paper": "FinBen A Holistic Financial Benchmark for Large Language Models",
        "input": "LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including 42 datasets spanning 24 financial tasks, covering eight critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, decision-making, and bilingual (English and Spanish). FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and two novel datasets for regulations and stock trading. Our evaluation of 21 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel solutions outperformed GPT-4, showcasing FinBen's potential to drive innovations in financial LLMs. All datasets and code are publicly available for the research community, with results shared and updated regularly on the Open Financial LLM Leaderboard.\n\nIn this work, we present FinBen, a comprehensive benchmark specifically designed to evaluate\nLLMs in the financial domain. FinBen includes 42 diverse datasets spanning 24 tasks, meticulously\norganized to assess LLMs across eight critical aspects: information extraction, textual analysis,\nquestion answering, text generation, risk management, forecasting, decision-making, and Spanish.\nThis breadth of coverage sets FinBen apart from existing financial benchmarks, enabling a more\nrobust and nuanced evaluation of LLM capabilities. Our evaluation of 21 LLMs, including GPT-4,\nChatGPT, and Gemini, reveals their key advantages and limitations, highlighting directions for future\nwork. Looking ahead, FinBen continuously evolves into an open FinLLM leaderboard (Lin et al.,\n2024). We will incorporat additional languages and multimodal financial tasks (Yanglet and Deng,\n2024) and expand the range of financial tasks to further enhance its applicability and impact.\nOpenness: Our FinBen project follows the model openness framework (White et al., 2024) by\nproviding a comprehensive set of financial datasets and evalution codes under OSI-approved licenses.\nLimitations: We acknowledge several limitations that could impact FinBen\u2019s effectiveness and\napplicability. The restricted size of available datasets may affect the models\u2019 financial understanding\nand generalization across various contexts. Computational constraints limited our evaluation to the\nLLaMA 70B model, potentially overlooking the capabilities of larger models. Additionally, the tasks\nare based on American market data and English texts, which may limit the benchmark\u2019s applicability\nto global financial markets. Responsible usage and safeguards are essential to prevent potential\nmisuse, such as financial misinformation or unethical market influence10.\nEthical Statement: The authors take full responsibility for any potential legal issues arising from\nFinBen\u2019s development and dissemination. All data used are publicly available, non-personal, and\nshared under the MIT license, adhering to privacy and ethical guidelines. This manuscript and\nassociated materials are for academic and educational use only and do not provide financial, legal, or\ninvestment advice. The authors disclaim any liability for losses or damages from using the material,\nand users agree to seek professional consultation and indemnify the authors against any claims arising\nfrom its use11.\n10For a detailed limitation concerning this work, please see Appendix.\n11For a detailed ethical and legal statement concerning this work, please see Appendix.\n10"
    },
    {
        "paper": "Magnet We Never Know How Text-to-Image Diffusion Models Work, Until We Learn How Vision-Language Models Function",
        "input": "Text-to-image diffusion models particularly Stable Diffusion, have revolutionized the field of computer vision. However, the synthesis quality often deteriorates when asked to generate images that faithfully represent complex prompts involving multiple attributes and objects. While previous studies suggest that blended text embeddings lead to improper attribute binding, few have explored this in depth. In this work, we critically examine the limitations of the CLIP text encoder in understanding attributes and investigate how this affects diffusion models. We discern a phenomenon of attribute bias in the text space and highlight a contextual issue in padding embeddings that entangle different concepts. We propose Magnet, a novel training-free approach to tackle the attribute binding problem. We introduce positive and negative binding vectors to enhance disentanglement, further with a neighbor strategy to increase accuracy. Extensive experiments show that Magnet significantly improves synthesis quality and binding accuracy with negligible computational cost, enabling the generation of unconventional and unnatural concepts.\n\nIn this work, we propose a novel training-free method, Magnet, to tackle the attribute binding issue.\nFirst, we conduct a fine-grained analysis of the CLIP text encoder. We observe the phenomenon of\nattribute bias and point out the context issue of padding embeddings, where the representations of\ndifferent concepts are entangled, and hence provide potential explanations for existing T2I issues.\nSecond, we introduce the positive and negative binding vectors to enhance the binding within the\nconcept and strengthen the distinction between concepts. Further with the neighbor strategy, the\nvector estimation can be more accurate. Evaluated in various ways, Magnet shows the ability to\ndisentangle different attributes and generate anti-prior concepts. Performed in the textual space,\nMagnet improves the synthesis quality and text alignment, with an impressively low increase in\ncomputational cost. We sincerely hope that this work will motivate the exploration of generative\ndiffusion models and the discovery of other interesting phenomena."
    },
    {
        "paper": "Deep Graph Mating",
        "input": "In this paper, we introduce the first learning-free model reuse task within the non-Euclidean domain, termed as Deep Graph Mating (Grama). We strive to create a child Graph Neural Network (GNN) that integrates knowledge from pre-trained parent models without requiring re-training, fine-tuning, or annotated labels. To this end, we begin by investigating the permutation invariance property of GNNs, which leads us to develop two vanilla approaches for Grama: Vanilla Parameter Interpolation (VPI) and Vanilla Alignment Prior to Interpolation (VAPI), both employing topology-independent interpolation in the parameter space. However, neither approach has achieved the anticipated results. Through theoretical analysis of VPI and VAPI, we identify critical challenges unique to Grama, including increased sensitivity to parameter misalignment and further the inherent topology-dependent complexities. Motivated by these findings, we propose the Dual-Message Coordination and Calibration (DuMCC) methodology, comprising the Parent Message Coordination (PMC) scheme to optimise the permutation matrices for parameter interpolation by coordinating aggregated messages, and the Child Message Calibration (CMC) scheme to mitigate over-smoothing identified in PMC by calibrating the message statistics within child GNNs. Experiments across diverse domains, including node and graph property prediction, 3D object recognition, and large-scale semantic parsing, demonstrate that the proposed DuMCC effectively enables training-free knowledge transfer, yielding results on par with those of pre-trained models.\n\nIn this paper, we explore a novel GRAMA task for learning-free GNN reuse. The child model from\nGRAMA is expected to functionally merge knowledge from pre-trained parent models. Uniquely,\nGRAMA establishes the \ufb01rst paradigm in GNN reuse that operates entirely without re-training or \ufb01ne-\ntuning, while also eliminating the need for ground-truth labels. To this end, we start by developing two\nvanilla GRAMA approaches, which reveal speci\ufb01c challenges inherent to GRAMA. These challenges\nmotivate us to develop a DuMCC framework for topology-aware model reuse, leveraging a parent\nmessage coordination scheme followed by child message calibration. Experiments on node- and\ngraph-level tasks across various domains demonstrate the effectiveness of the proposed approach for\nannotation-free knowledge transfer without additional learning.\nDespite its strengths, the proposed DuMCC is primarily designed for homogeneous GRAMA, as\ndiscussed in Sect. 3. Currently, the framework does not support cross-architecture heterogeneous\nGRAMA, where parent models have different architectures, such as a combination of GCN and\nGraphSAGE. Additionally, it does not handle scenarios where parent models address tasks at differ-\nent levels, such as node-level versus graph-level tasks\u2014another aspect of heterogeneous GRAMA.\nThese limitations primarily arise from the absence of direct correspondence between the differing\narchitectural layers of the parent models, an issue we plan to explore in our future work. We will\nalso explore the possibility of a fully data-independent GRAMA scheme and investigate broader\napplications beyond training-free model reuse, such as its use as a pre-processing step to facilitate\ngraph-based knowledge amalgamation. Further discussions on limitations and potential solutions are\nprovided in Sect. H of the appendix.\n10"
    },
    {
        "paper": "Foundation Inference Models for Markov Jump Processes",
        "input": "Markov jump processes are continuous-time stochastic processes which describe dynamical systems evolving in discrete state spaces. These processes find wide application in the natural sciences and machine learning, but their inference is known to be far from trivial. In this work we introduce a methodology for zero-shot inference of Markov jump processes (MJPs), on bounded state spaces, from noisy and sparse observations, which consists of two components. First, a broad probability distribution over families of MJPs, as well as over possible observation times and noise mechanisms, with which we simulate a synthetic dataset of hidden MJPs and their noisy observations. Second, a neural recognition model that processes subsets of the simulated observations, and that is trained to output the initial condition and rate matrix of the target MJP in a supervised way. We empirically demonstrate that one and the same (pretrained) recognition model can infer, in a zero-shot fashion, hidden MJPs evolving in state spaces of different dimensionalities. Specifically, we infer MJPs which describe (i) discrete flashing ratchet systems, which are a type of Brownian motors, and the conformational dynamics in (ii) molecular simulations, (iii) experimental ion channel data and (iv) simple protein folding models. What is more, we show that our model performs on par with state-of-the-art models which are trained on the target datasets.Our pretrained model is available online.\n\nIn this work we introduced a novel methodology for zero-shot inference of Markov jump processes\nand its Foundation Inference Model (FIM). We empirically demonstrated that one and the same\nFIM can be used to estimate stationary distributions, relaxation times, mean first-passage times,\ntime-dependent moments and thermodynamic quantities (i.e. the entropy production) from noisy and\ndiscretely observed MJPs, taking values in state spaces of different dimensionalities, all in zero-shot\nmode. To the best of our knowledge, FIM is also the first zero-shot generative model for MJPs.\nFuture work shall involve extending our methodology to Birth and Death processes, as well as\nconsidering more complex (prior) transition rate distributions. See our discussion on Limitations in\nthe next section, for details.\n6"
    }
]