{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ae645-f378-4cf6-a44f-e18b3053280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai\n",
    "# !pip install --upgrade PyMuPDF\n",
    "from API_KEY import API_KEY\n",
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "output_folder = \"limitations_2024\"\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "inline_limitations = re.compile(\n",
    "    r\"\\b(Limitations?|Challenges?)[:\\s]+(.{100,1500}?)\",\n",
    "    flags=re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "\n",
    "find_limitations = re.compile(\n",
    "    r\"(?:^|\\n)(?:\\d{0,2}[\\.]\\s*)?\"\n",
    "    r\"(limitations|limitation|conclusions and limitations|future work|conclusion (?:and|&) future work|limitations (?:and|&) future work|conclusion|conclusions|discussion|conclusion (?:and|&) discussion|conclusions|research limitations|study limitations|challenges)\"\n",
    "    r\"(?::)?\\s*\\n+(.*?)(?=\\n\\s*(?:\\d{1,2}[\\.]+\\s*)?[A-Z][A-Za-z0-9, \\-]{3,60}\\n|\\Z)\",\n",
    "    flags=re.IGNORECASE | re.DOTALL\n",
    ")\n",
    "\n",
    "def text_from_pdf(path):\n",
    "    text = \"\"\n",
    "    with fitz.open(path) as doc:\n",
    "        for page in doc:\n",
    "            text += \" \".join([block[4] for block in page.get_text(\"blocks\")]) + \"\\n\"\n",
    "    return text\n",
    "\n",
    "keywords = [\"limitations\", \"conclusions and limitations\",\"future work\", \"challenges\", \"limitation\", \"study limitations\", \"research limitations\", \"limitations and future work\", \"conclusion and future work\", \"conclusion & future work\"]\n",
    "def find_section(txt):\n",
    "    paper_sections = {}\n",
    "    for f in find_limitations.finditer(txt):\n",
    "        title = f.group(1).strip().lower()\n",
    "        text = f.group(2).strip()\n",
    "        if not text.lower().startswith(\"question: does the paper discuss the limitations\"):\n",
    "            paper_sections[title] = text\n",
    "    has_limitations = any(k in paper_sections for k in keywords)\n",
    "    if not has_limitations:\n",
    "        for section_title in [\"conclusion\", \"conclusions and limitations\" \"conclusions\", \"discussion\", \"conclusion and discussion\", \"conclusion & discussion\"]:\n",
    "            section_text = paper_sections.get(section_title)\n",
    "            if section_text:\n",
    "                match = inline_limitations.search(section_text)\n",
    "                if match:\n",
    "                    paper_sections[\"inline\"] = match.group(1).strip()\n",
    "                break\n",
    "    return paper_sections\n",
    "    \n",
    "def extract_limitations(path):\n",
    "    doc = fitz.open(path)\n",
    "    txt = \"\"\n",
    "    for page_num in range(doc.page_count):\n",
    "        page = doc.load_page(page_num)\n",
    "        txt += page.get_text(\"text\") + \"\\n\"\n",
    "    title_paper = os.path.basename(path)\n",
    "    paper_sections = find_section(txt)\n",
    "    return title_paper, paper_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ac0f3d-a9e8-48ef-bb11-815fa8c11df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bullet point list, use LLMs to summarize\n",
    "client = OpenAI(api_key = API_KEY)\n",
    "def bullet_list(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model = \"gpt-4.1-nano\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\":f\"You are a research assistant that summarizes academic limitation sections.\"},\n",
    "            {\"role\": \"user\", \"content\":f\"Convert the following text into 3-6 clear bullet points:\\n {text}\"}\n",
    "        ],\n",
    "        max_tokens=300\n",
    "    )\n",
    "    return (response.choices[0].message.content).strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4685ae-a5cb-47c3-8284-2dc01a114ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2024\n",
    "full_papers_folder = f\"full_papers/papers/{year}\"\n",
    "abstracts_folder = f\"retrieved_abstracts/data_{year}/papers_data.jsonl\"\n",
    "tokenized_papers_folder = f\"tokenized_data/tokenized_{year}.json\"\n",
    "\n",
    "papers = [paper for paper in os.listdir(full_papers_folder) if paper.endswith(\".pdf\")]\n",
    "limitations_input_conclusion = []\n",
    "limitations_input_tokenized_text = []\n",
    "limitations_input_full_paper = []\n",
    "limitations_only = []\n",
    "\n",
    "def clean_title(title):\n",
    "    title = title.lower().replace(\"-\", \" \").replace(\":\", \" \").strip()\n",
    "    title = re.sub(r\"\\s+\", \" \", title)\n",
    "    title = re.sub(r\"[^a-z0-9 ]\", \" \", title)\n",
    "    return title\n",
    "\n",
    "def load_json_title(title_n, folder):\n",
    "    title_n = clean_title(title_n)\n",
    "    try:\n",
    "        with open(folder, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    except UnicodeDecodeError:\n",
    "        with open(folder, \"r\", encoding=\"latin-1\") as f:\n",
    "            data = json.load(f)\n",
    "    for i in data:\n",
    "        if isinstance(i, dict):\n",
    "            t = clean_title(i.get(\"title\", \"\"))\n",
    "            if t == title_n:\n",
    "                return i\n",
    "    return None\n",
    "\n",
    "def load_jsonl_title(title_n, folder):\n",
    "    title_n = clean_title(title_n)\n",
    "    with open(folder, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if clean_title(data.get(\"title\", \"\")) == title_n:\n",
    "                    return data\n",
    "            except Exception:\n",
    "                print(f\"Failed to read {path}\")\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "for paper in papers:\n",
    "    path = os.path.join(full_papers_folder, paper)\n",
    "    name, sections = extract_limitations(path)\n",
    "    keywords = [\"limitations\", \"future work\", \"challenges\"]\n",
    "    for l in keywords:\n",
    "        section_text = sections.get(l)\n",
    "        if section_text and section_text.strip().lower() != \"question: does the paper discuss the limitations\" and (\"conclusions and limitations\" in sections or \"conclusions\" in sections or \"conclusion\" in sections or \"discussion\" in sections or \"conclusion and discussion\" in sections or \"conclusion & discussion\" in sections):\n",
    "            bullets = bullet_list(section_text)\n",
    "            print(name + \"\\n\" + bullets)\n",
    "\n",
    "            abstract_data = load_jsonl_title(name, abstracts_folder)\n",
    "            tokenized_data = load_json_title(name, tokenized_papers_folder)\n",
    "            full_papers_data = text_from_pdf(path)\n",
    "\n",
    "            # First dataset, abstracts and conclusion\n",
    "            if abstract_data:\n",
    "                conclusion_text = sections.get(\"conclusion\") or sections.get(\"discussion\") or sections.get(\"conclusion and discussion\") or sections.get(\"conclusion & discussion\")\n",
    "                abstract = abstract_data.get(\"abstract\", \"\")\n",
    "                given_input = abstract + \"\\n\" + conclusion_text\n",
    "                limitations_input_conclusion.append({\"paper\": name, \"input\": given_input, \"target\": section_text, \"target_bullets\": bullets})\n",
    "\n",
    "            # Second dataset, tokenized papers\n",
    "            if tokenized_data:\n",
    "                tokens = \" \".join(tokenized_data.get(\"tokens\", []))\n",
    "                limitations_input_tokenized_text.append({\"paper\": name, \"input\": tokens, \"target\": section_text, \"target_bullets\": bullets})\n",
    "\n",
    "            # Third dataset, full papers\n",
    "             if full_papers_data:\n",
    "                limitations_input_full_paper.append({\"paper\": name, \"input\": full_papers_data, \"target\": section_text, \"target_bullets\": bullets})\n",
    "\n",
    "            limitations_only.append({\"paper\": name, \"target\": section_text, \"target_bullets\": bullets})\n",
    "            break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb84114e-b5b6-470f-b321-13c1d2bb8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file1 = os.path.join(output_folder, f\"limitations_training_1_{year}.json\")\n",
    "output_file2 = os.path.join(output_folder, f\"limitations_training_2_{year}.json\")\n",
    "output_file3 = os.path.join(output_folder, f\"limitations_only_{year}.json\")\n",
    "output_file4 = os.path.join(output_folder, f\"limitations_training_3_{year}.json\")\n",
    "with open(output_file1, 'w') as out_file:\n",
    "    json.dump(limitations_input_conclusion, out_file, indent=4)\n",
    "with open(output_file2, 'w') as out_file:\n",
    "    json.dump(limitations_input_tokenized_text, out_file, indent=4)\n",
    "with open(output_file3, 'w') as out_file:\n",
    "    json.dump(limitations_only, out_file, indent=4)\n",
    "with open(output_file4, 'w') as out_file:\n",
    "    json.dump(limitations_input_full_paper, out_file, indent=4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d59a09-a4a7-4498-9180-3025f3292ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract + conclusion working\n",
    "# !pip install unidecode\n",
    "import os\n",
    "import json\n",
    "import unidecode\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from difflib import get_close_matches\n",
    "\n",
    "year = 2024\n",
    "limitations_file = f\"limitations_2024/limitations_only_{year}.json\"\n",
    "abstracts_file = f\"retrieved_abstracts/data_{year}/papers_data.jsonl\"\n",
    "pdf_folder = f\"full_papers/papers/{year}\"\n",
    "output_full = f\"limitations_2024/limitations_training_1_{year}.json\"\n",
    "output_training_only = f\"limitations_2024/limitations_training_bart_1_{year}.json\"\n",
    "\n",
    "def clean_title(title):\n",
    "    title = unidecode.unidecode(title)\n",
    "    title = title.lower()\n",
    "    title = re.sub(r\"[^a-z0-9 ]\", \" \", title)\n",
    "    title = re.sub(r\"\\s+\", \" \", title).strip()\n",
    "    return title\n",
    "\n",
    "with open(limitations_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    limitation_entries = json.load(f)\n",
    "\n",
    "abstract_map = {}\n",
    "with open(abstracts_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            paper = json.loads(line)\n",
    "            if \"title\" in paper and \"abstract\" in paper:\n",
    "                abstract_map[clean_title(paper[\"title\"])] = paper[\"abstract\"]\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "abstract_conclusion_dataset = []\n",
    "training_only_dataset = []\n",
    "\n",
    "for entry in tqdm(limitation_entries):\n",
    "    raw_title = entry[\"paper\"].replace(\".pdf\", \"\")\n",
    "    cleaned = clean_title(raw_title)\n",
    "\n",
    "    # Find the abstract of the paper\n",
    "    abstract = abstract_map.get(cleaned)\n",
    "    if not abstract:\n",
    "        matches = get_close_matches(cleaned, list(abstract_map.keys()), n=1, cutoff=0.7)\n",
    "        if matches:\n",
    "            abstract = abstract_map[matches[0]]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    # Load the conclusion or discussion\n",
    "    pdf_path = os.path.join(pdf_folder, entry[\"paper\"])\n",
    "    if not os.path.exists(pdf_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        _, sections = extract_limitations(pdf_path)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "    conclusion = (\n",
    "        sections.get(\"conclusion\")\n",
    "        or sections.get(\"conclusions\")\n",
    "        or sections.get(\"discussion\")\n",
    "        or sections.get(\"conclusion and discussion\")\n",
    "        or sections.get(\"conclusion & discussion\")\n",
    "        or sections.get(\"conclusions and limitations\")\n",
    "    )\n",
    "\n",
    "    if not conclusion:\n",
    "        continue\n",
    "    combined_input = abstract.strip() + \"\\n\\n\" + conclusion.strip()\n",
    "\n",
    "    # Full dataset\n",
    "    abstract_conclusion_dataset.append({\n",
    "        \"paper\": raw_title,\n",
    "        \"input\": combined_input,\n",
    "        \"target\": entry[\"target\"],\n",
    "        \"target_bullets\": entry[\"target_bullets\"]\n",
    "    })\n",
    "    # Only for training dataset\n",
    "    training_only_dataset.append({\n",
    "        \"input\": combined_input,\n",
    "        \"target\": entry[\"target_bullets\"]\n",
    "    })\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_full), exist_ok=True)\n",
    "\n",
    "    with open(output_full, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(abstract_conclusion_dataset, f, indent=4)\n",
    "    \n",
    "    with open(output_training_only, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(training_only_dataset, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911403c2-416e-4c67-b7a5-67d01a42b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenized working\n",
    "\n",
    "with open(limitations_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    limitation_entries = json.load(f)\n",
    "\n",
    "with open(tokenized_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    tokenized_data = json.load(f)\n",
    "\n",
    "tokenized_map = {\n",
    "    clean_title(entry[\"title\"]): entry[\"tokens\"]\n",
    "    for entry in tokenized_data.values()\n",
    "    if isinstance(entry, dict) and \"title\" in entry and \"tokens\" in entry\n",
    "}\n",
    "\n",
    "full_dataset = []\n",
    "training_dataset = []\n",
    "\n",
    "for entry in tqdm(limitation_entries):\n",
    "    raw_title = entry[\"paper\"].replace(\".pdf\", \"\")\n",
    "    cleaned = clean_title(raw_title)\n",
    "\n",
    "    tokens = tokenized_map.get(cleaned)\n",
    "\n",
    "    if not tokens:\n",
    "        matches = get_close_matches(cleaned, list(tokenized_map.keys()), n=1, cutoff=0.7)\n",
    "        if matches:\n",
    "            match = matches[0]\n",
    "            tokens = tokenized_map[match]\n",
    "        else:\n",
    "            continue\n",
    "    if isinstance(tokens, list):\n",
    "        tokens = \" \".join(tokens)\n",
    "\n",
    "    # Full dataset\n",
    "    full_dataset.append({\n",
    "        \"paper\": raw_title,\n",
    "        \"input\": tokens,\n",
    "        \"target\": entry[\"target\"],\n",
    "        \"target_bullets\": entry[\"target_bullets\"]\n",
    "    })\n",
    "\n",
    "    # Version for training\n",
    "    training_dataset.append({\n",
    "        \"input\": tokens,\n",
    "        \"target\": entry[\"target_bullets\"]\n",
    "    })\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_full), exist_ok=True)\n",
    "\n",
    "    with open(output_full, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(full_dataset, f, indent=4)\n",
    "    \n",
    "    with open(output_training, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(training_dataset, f, indent=4)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c1ad02-c1be-4e1a-9e92-39b2625879b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data for the full papers\n",
    "year = 2024\n",
    "input_file = f\"limitations_2024/limitations_training_3_{year}.json\"\n",
    "output_file = f\"limitations_2024/limitations_training_bart_3_{year}.json\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    full_data = json.load(f)\n",
    "\n",
    "training_data = [\n",
    "    {\"input\": entry[\"input\"], \"target\": entry[\"target_bullets\"]}\n",
    "    for entry in full_data\n",
    "    if \"input\" in entry and \"target_bullets\" in entry\n",
    "]\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(training_data, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
