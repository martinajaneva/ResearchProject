{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe15311-ec9c-4f5c-905f-31ad4064306b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Attention is All you Need from year 2017: 2200 citations\n",
      "2. Generative Adversarial Nets from year 2014: 1193 citations\n",
      "3. PyTorch  An Imperative Style  High Performance Deep Learning Library from year 2019: 1151 citations\n",
      "4. Language Models are Few Shot Learners from year 2020: 1141 citations\n",
      "5. Denoising Diffusion Probabilistic Models from year 2020: 849 citations\n",
      "6. GANs Trained by a Two Time Scale Update Rule Converge to a Local Nash Equilibrium from year 2017: 530 citations\n",
      "7. A  Sampling from year 2014: 458 citations\n",
      "8. Inductive Representation Learning on Large Graphs from year 2017: 401 citations\n",
      "9. Faster R CNN  Towards Real Time Object Detection with Region Proposal Networks from year 2015: 400 citations\n",
      "10. Diffusion Models Beat GANs on Image Synthesis from year 2021: 387 citations\n"
     ]
    }
   ],
   "source": [
    "# Find the top cited papers\n",
    "\n",
    "import pickle\n",
    "with open('graph.gpickle', 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "    \n",
    "papers_top_10 = sorted(graph.in_degree(), key=lambda y: y[1], reverse=True)[:10]\n",
    "i = 1\n",
    "for n, citation in papers_top_10:\n",
    "    paper_year = graph.nodes[n].get(\"year\", \"unknown\")\n",
    "    paper_name = graph.nodes[n].get(\"title\", n)\n",
    "    print(f\"{i}. {paper_name} from year {paper_year}: {citation} citations\")\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "deaa8652-8f15-436f-a4b5-f709ccd3bc15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the citation chain is 5\n",
      "Paper 1 How to Characterize The Landscape of Overparameterized Convolutional Neural Networks (2020)\n",
      "  - The loss landscape of the overparameterized convolutional neural network (CNN) is not fully understood, especially when the network is trained on a single layer.\n",
      "- The model is not suitable for training with multiple layers, which may not be suitable for deep neural networks (DNNs).\n",
      "- Although the model can be trained on multiple layers (e.g., ReLU), it may not fully capture the feature distributions of multiple layers.\n",
      "– The model's performance is not optimized for deep learning, which is a limitation of the current work.\n",
      "\n",
      "\n",
      "Paper 2 On the Global Convergence of Gradient Descent for Over parameterized Models using Optimal Transport (2018)\n",
      "  - The study focuses on the Wasserstein gradient flow, which is a by-product of optimal transport theory.  \n",
      "- It does not address the generalizability of the gradient flow in the model, which may not be fully understood in real-world scenarios. \n",
      "‐ The study does not explore the generalization of the Wassers gradient flow to other methods, such as the gradient gradient descent.\n",
      "\n",
      "\n",
      "Paper 3 Implicit Regularization in Matrix Factorization (2017)\n",
      "  - The study focuses on the implicit regularization of non-convex optimization methods.  \n",
      "- It does not address the underlying assumptions underlying the implicit generalization of the implicit bias, which may not be fully understood by the generalizability of the assumptions.\n",
      "\n",
      "\n",
      "Paper 4 Matrix Completion has No Spurious Local Minimum (2016)\n",
      "  - The method is based on a non-convex objective function, which may not always converge to a good initial point.\n",
      "- It may not be suitable for all optimization algorithms, such as those used in the current work.\n",
      "\n",
      "\n",
      "Paper 5 Regularized M estimators with nonconvexity  Statistical and algorithmic theory for local optima (2013)\n",
      "  - The theoretical results are limited to nonconvex objective functions.\n",
      "- There is a need for further research to understand the generalizability of the theoretical results.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pickle\n",
    "import networkx as nx\n",
    "\n",
    "# Find a random citation chain to show example\n",
    "\n",
    "begin = [i for i in graph.nodes if 0 <= graph.out_degree(i)]\n",
    "first_node = random.choice(begin)\n",
    "\n",
    "for i in graph.nodes:\n",
    "    if first_node == i:\n",
    "        continue\n",
    "    try:\n",
    "        path = list(nx.all_simple_paths(graph, source=first_node, target=i, cutoff=4))\n",
    "        if path:\n",
    "            y = path[0]\n",
    "            break\n",
    "    except nx.NetworkXNoPath:\n",
    "        continue\n",
    "\n",
    "print(f\"The length of the citation chain is {len(y)}\")\n",
    "\n",
    "\n",
    "for k, n in enumerate(y):\n",
    "    paper_year = graph.nodes[n].get(\"year\", \"unknown\")\n",
    "    gaps = graph.nodes[n].get(\"limitations\", [])\n",
    "    paper_name = graph.nodes[n].get(\"title\", n)\n",
    "    \n",
    "    print(f\"Paper {1+k} {paper_name} ({paper_year})\")\n",
    "    if isinstance(gaps, list):\n",
    "        for b in gaps:\n",
    "            print(f\"  - {b}\")\n",
    "    elif isinstance(gaps, str):\n",
    "        print(f\"  {gaps}\")\n",
    "    else:\n",
    "        print(\"No limitations found\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20bcb45-259e-4364-b66b-522703b43ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find 1000 citation chains for evaluation\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "chain_number = 1000\n",
    "chains = []\n",
    "min_len = 3\n",
    "max_len = 5\n",
    "\n",
    "begin = [i for i in graph.nodes if 0 <= graph.out_degree(i)]\n",
    "\n",
    "# Extract 1000 chains from graph\n",
    "for _ in tqdm(range(chain_number)):\n",
    "    first_node = random.choice(begin)\n",
    "    for y in graph.nodes:\n",
    "        if y == first_node:\n",
    "            continue\n",
    "        try: \n",
    "            path = list(nx.all_simple_paths(graph, source=first_node, target= y, cutoff=max_len))\n",
    "            for j in path:\n",
    "                if min_len <= len(j):\n",
    "                    chains.append(j)\n",
    "                    break\n",
    "            if chain_number <= len(chains) :\n",
    "                break\n",
    "        except nx.NetworkXNoPath:\n",
    "            continue\n",
    "print(f\"{len(chains) chains were found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d249bc61-edd3-4f89-8efb-25ea067057ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the chains\n",
    "import json\n",
    "from tqdm from tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "res = []\n",
    "\n",
    "def find_lims(child):\n",
    "    lims = graph.nodes[child].get(\"limitations\", [])\n",
    "    if isinstance(lims, list):\n",
    "        return \" \".join(lims)\n",
    "    elif isinstance(lims, str):\n",
    "        return lims\n",
    "    return \"\"\n",
    "\n",
    "for c in tqdm(chains):\n",
    "    res_chain = {\"chain\": [], \"similarities\": []}\n",
    "    txt = []\n",
    "    for n in c:\n",
    "        paper_year = graph.nodes[n].get(\"year\", \"unknown\")\n",
    "        gaps = find_lims(n)\n",
    "        paper_name = graph.nodes[n].get(\"title\", n)\n",
    "        res_chain[\"chain\"].append({\"node\": n, \"title\": paper_name, \"year\": paper_year, \"limitations\":gaps})\n",
    "        txt.append(gaps)\n",
    "    if len(txt) > 1:\n",
    "        tfidf = vectorizer.fit_transform(txt)\n",
    "        similarities = cosine_similarity(tfidf)\n",
    "        pw_sims = [similarities[x, x+1] for x in range(c)-1]\n",
    "        res_chain[\"similarities\"] = pw_sims\n",
    "    res.append(res_chain)\n",
    "\n",
    "with open(\"citation_chains_eval.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(res, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996fa0c-bd84-4c81-881e-93882463dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a histogram figure out of the results from the cosime similarity\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "with open(\"citation_chains_eval.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    res = json.load(f)\n",
    "\n",
    "similarities = [s for r in res for s in r[\"similarities\"] if s is not None]\n",
    "avg_per_chain = [np.mean(r[\"similarities\"]) for r in res if r[\"similarities\"]]\n",
    "\n",
    "avg_sim = np.mean(similarities)\n",
    "inc_trend_count = sum(1 for r in res if r[\"similarities\"] and r[\"similarities\"][-1] > r[\"similarities\"][0])\n",
    "low_s_score = sum(1 for r in res if r[\"similarities\"] and np.mean(r[\"similarities\"]) < 0.3)\n",
    "all_chains = len(r for r in res if r[\"similarities\"])\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.hist(similarities, bins=30, edgecolor='black', color=sns.color_pallete(\"Blues\")[2])\n",
    "plt.title(\"Distribution of Cosine Similarity Across Citation Chains\", fontsize=12, fontweight=\"bold\")\n",
    "plt.xlabel(\"Cosine Similarity\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "{\n",
    "    \"Mean Cosine Similarity\": avg_sim,\n",
    "    \"% of Chains with Increasing Similarity\": inc_trend_count/all_chains * 100,\n",
    "    \"% of Chains with Low Similarity (< 0.3)\": low_s_score/all_chains * 100,\n",
    "    \"Total Chains Evaluated\":  all_chains\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e7da9e-5a1d-44f4-842b-44105e3adfea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
