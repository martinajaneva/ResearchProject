{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1b7c572-b8fa-43c6-9b40-1e062a25cdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install unidecode --quiet\n",
    "# !pip install networkx\n",
    "import networkx as nx\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from unidecode import unidecode\n",
    "from tqdm import tqdm\n",
    "\n",
    "def clean_title(title):\n",
    "    title = unidecode(title.lower())\n",
    "    title = re.sub(r'\\W', '', title)\n",
    "    return title.strip()\n",
    "\n",
    "def match_references(ref, find_paper_db):\n",
    "    clean_ref = clean_title(ref)\n",
    "    for t, _ in find_paper_db.items():\n",
    "        title = t.split()\n",
    "        index = 0\n",
    "        for w in title:\n",
    "            h = clean_ref.find(w, index)\n",
    "            if -1 == h:\n",
    "                break\n",
    "            index = h + len(w)\n",
    "        else:\n",
    "            return t\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1d05d88-9f01-4bc9-8a71-3ede80dc5bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_paper_db = {}\n",
    "LIMITATIONS_FOLDER = \"../Generate_limitations\"\n",
    "for year in range(2013,2025):\n",
    "    path = os.path.join(LIMITATIONS_FOLDER, f\"test_outputs/output_{year}.jsonl\")\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            e = json.loads(line)\n",
    "            title = e.get(\"paper\")\n",
    "            cleaned = clean_title(title)\n",
    "            find_paper_db[cleaned] = {\"title\": title, \"year\":year, \"limitations\": e.get(\"generated\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df1e26f4-088c-4abe-8b94-f83561f45dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 360/360 [03:54<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 411/411 [05:01<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 403/403 [05:14<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 569/569 [06:31<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 679/679 [09:22<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1008/1008 [21:53<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1417/1417 [36:45<00:00,  1.56s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1898/1898 [1:17:30<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2329/2329 [5:18:02<00:00,  8.19s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2831/2831 [5:34:58<00:00,  7.10s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3532/3532 [4:54:25<00:00,  5.00s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4488/4488 [2:33:54<00:00,  2.06s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph nodes: 19356, Graph edges: 91585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "graph = nx.DiGraph()\n",
    "\n",
    "for year in range(2013,2025):\n",
    "    print(f\"Processing {year}\")\n",
    "    path = f\"references_by_year/references_{year}.json\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        references_by_year = json.load(f)\n",
    "    for e in tqdm(references_by_year):\n",
    "        pdf = e[\"paper\"]\n",
    "        title = pdf.replace(\".pdf\", \"\")\n",
    "        clean = clean_title(title)\n",
    "        if clean not in find_paper_db:\n",
    "            continue\n",
    "\n",
    "        graph.add_node(clean, **find_paper_db[clean])\n",
    "\n",
    "        for r in e[\"references\"]:\n",
    "            target = match_references(r, find_paper_db)\n",
    "            if target and target != clean:\n",
    "                graph.add_node(target, **find_paper_db[target])\n",
    "                graph.add_edge(clean, target)\n",
    "print(f\"Graph nodes: {graph.number_of_nodes()}, Graph edges: {graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308436d0-f730-480a-bbcb-963f8ed61e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"graph.gpickle\", 'wb') as f:\n",
    "    pickle.dump(graph, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cb25c460-646b-4ba8-9cd3-0f48e87b3cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most cited papers:\n",
      "Attention is All you Need (attentionisallyouneed) 2200 citations\n",
      "Generative Adversarial Nets (generativeadversarialnets) 1193 citations\n",
      "PyTorch  An Imperative Style  High Performance Deep Learning Library (pytorchanimperativestylehighperformancedeeplearninglibrary) 1151 citations\n",
      "Language Models are Few Shot Learners (languagemodelsarefewshotlearners) 1141 citations\n",
      "Denoising Diffusion Probabilistic Models (denoisingdiffusionprobabilisticmodels) 849 citations\n",
      "GANs Trained by a Two Time Scale Update Rule Converge to a Local Nash Equilibrium (ganstrainedbyatwotimescaleupdateruleconvergetoalocalnashequilibrium) 530 citations\n",
      "A  Sampling (asampling) 458 citations\n",
      "Inductive Representation Learning on Large Graphs (inductiverepresentationlearningonlargegraphs) 401 citations\n",
      "Faster R CNN  Towards Real Time Object Detection with Region Proposal Networks (fasterrcnntowardsrealtimeobjectdetectionwithregionproposalnetworks) 400 citations\n",
      "Diffusion Models Beat GANs on Image Synthesis (diffusionmodelsbeatgansonimagesynthesis) 387 citations\n"
     ]
    }
   ],
   "source": [
    "with open('graph.gpickle', 'rb') as f:\n",
    "    graph = pickle.load(f)\n",
    "top_papers = sorted(graph.in_degree(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 most cited papers:\")\n",
    "for n, c in top_papers:\n",
    "    title = graph.nodes[n].get(\"title\", node)\n",
    "    print(f\"{title} ({n}) {c} citations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134aad87-ef5f-4ad5-b4b8-1b35276a8356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
