{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd415b1-9aec-41a5-a47a-553c2096397e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "^C\n",
      "Fatal Python error: init_import_site: Failed to import the site module\n",
      "Python runtime state: initialized\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 980, in exec_module\n",
      "  File \"<frozen site>\", line 626, in <module>\n",
      "  File \"<frozen site>\", line 613, in main\n",
      "  File \"<frozen site>\", line 394, in addsitepackages\n",
      "  File \"<frozen site>\", line 236, in addsitedir\n",
      "  File \"<frozen site>\", line 195, in addpackage\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 980, in exec_module\n",
      "  File \"<frozen importlib.util>\", line 18, in <module>\n",
      "  File \"/opt/conda/lib/python3.11/threading.py\", line 9, in <module>\n",
      "    from _weakrefset import WeakSet\n",
      "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1032, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1130, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk\n",
    "!pip install pymupdf\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "import fitz\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from helper.preprocess_text import text_from_pdf\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text) \n",
    "    text = re.sub(r'http[s]?://\\S+', ' ', text) \n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text) \n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    return tokens\n",
    "\n",
    "directory = 'full_papers/papers'\n",
    "processed_data = {}\n",
    "for year_folder in os.listdir(directory):\n",
    "    year_path = os.path.join(directory, year_folder)\n",
    "    print(\"Processing year:\", year_folder) \n",
    "    if os.path.isdir(year_path):\n",
    "        processed_data[year_folder] = {}\n",
    "    print(\"Check 1: Before For Loop\")\n",
    "    data_year = {}\n",
    "    i = 0\n",
    "    for filename in os.listdir(year_path):\n",
    "        file_path = os.path.join(year_path, filename)\n",
    "        if os.path.isdir(file_path):  \n",
    "            continue\n",
    "        text = text_from_pdf(file_path)    \n",
    "        tokens = tokenize(text)\n",
    "        entry = {\n",
    "            \"title\": filename.replace(\".pdf\", \"\"),\n",
    "            \"tokens\": tokens\n",
    "        }\n",
    "        data_year[str(i)] = entry\n",
    "        i += 1\n",
    "    processed_data[year_folder] = data_year\n",
    "    print(\"Check 2: After For Loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "271fbb9c-77b3-4dc3-abeb-884d6efdc697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved tokenized data for 2013 to tokenized_data_with_title/tokenized_2013.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "output_dir = 'tokenized_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for year, tokens_list in processed_data.items():\n",
    "    file_path = os.path.join(output_dir, f\"tokenized_{year}.json\")\n",
    "    with open(file_path, 'w', encoding='utf-8', errors='ignore') as f:\n",
    "        json.dump(tokens_list, f, indent=2)\n",
    "    print(f\"Saved tokenized data for {year} to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "252965ce-b40f-482b-bdde-008791a41921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.1\n",
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/s0242735/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/s0242735/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/s0242735/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/s0242735/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ttso', 'general', 'framework', 'learning', 'invariant', 'representation', 'across', 'diverse', 'domain', 'distri', 'butions', 'currently', 'discussed', 'time', 'series', 'classification', 'framework', 'could', 'enhanced', 'extending', 'time', 'series', 'ood', 'task', 'time', 'series', 'forecasting', 'anomaly', 'detection', 'additionally', 'distribution', 'shift', 'occur', 'time', 'series', 'also', 'machine', 'learning', 'domain', 'like', 'image', 'deecke', 'et', 'al', 'text', 'tan', 'et', 'al', 'applying', 'approach', 'domain', 'could', 'improve', 'performance']\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk\n",
    "!pip install pymupdf\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "import fitz\n",
    "!pip install unidecode --quiet\n",
    "from unidecode import unidecode \n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from helper.preprocess_text import text_from_pdf\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text) \n",
    "    text = re.sub(r'http[s]?://\\S+', ' ', text) \n",
    "    text = re.sub(r'\\b[a-zA-Z]\\b', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text) \n",
    "    text = re.sub(r'\\b\\d+\\b', ' ', text)  \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    return tokens\n",
    "\n",
    "text = \"TTSO is a general framework for learning invariant representations across diverse domain distri- butions, currently discussed only for time series classification. This framework could be further enhanced by extending it to more time series OOD tasks, such as time series forecasting and anomaly detection. Additionally, distribution shifts occur not only in time series but also in other machine learning domains like images [Deecke et al., 2021] and text [Tan et al., 2022]. Applying our approach to these domains could further improve performance.\" \n",
    "tokens = tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
